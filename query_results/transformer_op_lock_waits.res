train size: 668
test size: 223
[ 9.73479223 14.68693671 -5.23716441 -0.68317547 -0.1807337  -0.74149625
 -2.78108404  3.18298059 -1.45110767 -3.19411609]
1
before training
after training
XXMULT(
  (trans): Transformer(
    (encoder): TransformerEncoder(
      (layers): ModuleList(
        (0): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
          )
          (linear1): Linear(in_features=12, out_features=2048, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=2048, out_features=12, bias=True)
          (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.1, inplace=False)
          (dropout2): Dropout(p=0.1, inplace=False)
        )
        (1): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
          )
          (linear1): Linear(in_features=12, out_features=2048, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=2048, out_features=12, bias=True)
          (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.1, inplace=False)
          (dropout2): Dropout(p=0.1, inplace=False)
        )
        (2): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
          )
          (linear1): Linear(in_features=12, out_features=2048, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=2048, out_features=12, bias=True)
          (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.1, inplace=False)
          (dropout2): Dropout(p=0.1, inplace=False)
        )
        (3): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
          )
          (linear1): Linear(in_features=12, out_features=2048, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=2048, out_features=12, bias=True)
          (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.1, inplace=False)
          (dropout2): Dropout(p=0.1, inplace=False)
        )
        (4): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
          )
          (linear1): Linear(in_features=12, out_features=2048, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=2048, out_features=12, bias=True)
          (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.1, inplace=False)
          (dropout2): Dropout(p=0.1, inplace=False)
        )
        (5): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
          )
          (linear1): Linear(in_features=12, out_features=2048, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=2048, out_features=12, bias=True)
          (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.1, inplace=False)
          (dropout2): Dropout(p=0.1, inplace=False)
        )
      )
      (norm): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
    )
    (decoder): TransformerDecoder(
      (layers): ModuleList(
        (0): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
          )
          (linear1): Linear(in_features=12, out_features=2048, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=2048, out_features=12, bias=True)
          (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.1, inplace=False)
          (dropout2): Dropout(p=0.1, inplace=False)
          (dropout3): Dropout(p=0.1, inplace=False)
        )
        (1): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
          )
          (linear1): Linear(in_features=12, out_features=2048, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=2048, out_features=12, bias=True)
          (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.1, inplace=False)
          (dropout2): Dropout(p=0.1, inplace=False)
          (dropout3): Dropout(p=0.1, inplace=False)
        )
        (2): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
          )
          (linear1): Linear(in_features=12, out_features=2048, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=2048, out_features=12, bias=True)
          (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.1, inplace=False)
          (dropout2): Dropout(p=0.1, inplace=False)
          (dropout3): Dropout(p=0.1, inplace=False)
        )
        (3): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
          )
          (linear1): Linear(in_features=12, out_features=2048, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=2048, out_features=12, bias=True)
          (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.1, inplace=False)
          (dropout2): Dropout(p=0.1, inplace=False)
          (dropout3): Dropout(p=0.1, inplace=False)
        )
        (4): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
          )
          (linear1): Linear(in_features=12, out_features=2048, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=2048, out_features=12, bias=True)
          (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.1, inplace=False)
          (dropout2): Dropout(p=0.1, inplace=False)
          (dropout3): Dropout(p=0.1, inplace=False)
        )
        (5): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
          )
          (linear1): Linear(in_features=12, out_features=2048, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=2048, out_features=12, bias=True)
          (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.1, inplace=False)
          (dropout2): Dropout(p=0.1, inplace=False)
          (dropout3): Dropout(p=0.1, inplace=False)
        )
      )
      (norm): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
    )
  )
  (transfc): Linear(in_features=48, out_features=64, bias=True)
  (fc1): Linear(in_features=64, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=10, bias=True)
)
loaded
======Top3 metric
total cpu usage_sys: 0.39011495298884025
dsk/total_read: 0.26545495960211574
total cpu usage_usr: 0.1259747719655154
igs:
[0, 289.3509521484375, 289.31585693359375, 289.3645935058594, 289.3779296875, 289.3837585449219, 289.37542724609375, 289.3658142089844, 289.35211181640625, 289.3841857910156, 289.3509521484375, 289.3302917480469, 289.37542724609375, 289.3769226074219, 289.3788146972656, 289.3725891113281, 289.3836364746094, 289.3818664550781, 289.35302734375, 289.3816223144531, 289.35986328125, 289.3929138183594, 289.3887023925781, 289.3775329589844, 289.3653259277344, 289.3936767578125, 289.375, 289.3918762207031, 289.3674621582031, 289.3914489746094, 289.390380859375, 289.388427734375, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Transformer(
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (2): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (3): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (4): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (5): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
    )
    (norm): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
  )
  (decoder): TransformerDecoder(
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (1): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (2): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (3): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (4): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (5): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
    )
    (norm): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
  )
)
Transformer(
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (2): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (3): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (4): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (5): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
    )
    (norm): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
  )
  (decoder): TransformerDecoder(
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (1): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (2): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (3): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (4): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (5): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
    )
    (norm): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
  )
)
[[[0.0, 0.0, 0.06167038902640343, 0.06192544475197792], [0.0, 0.0, 0.06167038902640343, 0.06192544475197792], [0.0, 0.0, 0.06167038902640343, 0.06192544475197792], [0.0, 0.0, 0.06167038902640343, 0.06192544475197792], [0.0, 0.0, 0.06167038902640343, 0.06192544475197792]], [[0.0, 0.0, 0.0614020861685276, 0.06189192086458206], [0.0, 0.0, 0.0614020861685276, 0.06189192086458206], [0.0, 0.0, 0.0614020861685276, 0.06189192086458206], [0.0, 0.0, 0.0614020861685276, 0.06189192086458206], [0.0, 0.0, 0.0614020861685276, 0.06189192086458206]]]
# 1
UPDATE CUSTOMER   SET C_BALANCE = $1,        C_YTD_PAYMENT = $2,        C_PAYMENT_CNT = $3,        C_DATA = $4  WHERE C_W_ID = $5    AND C_D_ID = $6    AND C_ID = $7
* 1
{'Node Type': 'ModifyTable', 'Operation': 'Update', 'Parallel Aware': False, 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 690, 'Plans': [{'Node Type': 'Index Scan', 'Parent Relationship': 'Member', 'Parallel Aware': False, 'Scan Direction': 'Forward', 'Index Name': 'customer_pkey', 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 690, 'Index Cond': '((c_w_id = 15) AND (c_d_id = 6) AND (c_id = 723))'}]}
[{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 15) AND (c_d_id = 6) AND (c_id = 723))'}, {'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}]
! 3 0.06192544475197792
{'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}
! 2 0.06167038902640343
{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 15) AND (c_d_id = 6) AND (c_id = 723))'}
! 0 0.0
empty
! 1 0.0
empty
* 2
{'Node Type': 'ModifyTable', 'Operation': 'Update', 'Parallel Aware': False, 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 690, 'Plans': [{'Node Type': 'Index Scan', 'Parent Relationship': 'Member', 'Parallel Aware': False, 'Scan Direction': 'Forward', 'Index Name': 'customer_pkey', 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 690, 'Index Cond': '((c_w_id = 25) AND (c_d_id = 1) AND (c_id = 299))'}]}
[{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 25) AND (c_d_id = 1) AND (c_id = 299))'}, {'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}]
! 3 0.06192544475197792
{'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}
! 2 0.06167038902640343
{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 25) AND (c_d_id = 1) AND (c_id = 299))'}
! 0 0.0
empty
! 1 0.0
empty
* 3
{'Node Type': 'ModifyTable', 'Operation': 'Update', 'Parallel Aware': False, 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 690, 'Plans': [{'Node Type': 'Index Scan', 'Parent Relationship': 'Member', 'Parallel Aware': False, 'Scan Direction': 'Forward', 'Index Name': 'customer_pkey', 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 690, 'Index Cond': '((c_w_id = 25) AND (c_d_id = 2) AND (c_id = 238))'}]}
[{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 25) AND (c_d_id = 2) AND (c_id = 238))'}, {'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}]
! 3 0.06192544475197792
{'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}
! 2 0.06167038902640343
{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 25) AND (c_d_id = 2) AND (c_id = 238))'}
! 0 0.0
empty
! 1 0.0
empty
* 4
{'Node Type': 'ModifyTable', 'Operation': 'Update', 'Parallel Aware': False, 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 690, 'Plans': [{'Node Type': 'Index Scan', 'Parent Relationship': 'Member', 'Parallel Aware': False, 'Scan Direction': 'Forward', 'Index Name': 'customer_pkey', 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 690, 'Index Cond': '((c_w_id = 30) AND (c_d_id = 6) AND (c_id = 231))'}]}
[{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 30) AND (c_d_id = 6) AND (c_id = 231))'}, {'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}]
! 3 0.06192544475197792
{'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}
! 2 0.06167038902640343
{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 30) AND (c_d_id = 6) AND (c_id = 231))'}
! 0 0.0
empty
! 1 0.0
empty
* 5
[{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 30) AND (c_d_id = 6) AND (c_id = 231))'}, {'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}]
! 3 0.06192544475197792
{'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}
! 2 0.06167038902640343
{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 30) AND (c_d_id = 6) AND (c_id = 231))'}
! 0 0.0
empty
! 1 0.0
empty
# 2
UPDATE CUSTOMER   SET C_BALANCE = $1,        C_YTD_PAYMENT = $2,        C_PAYMENT_CNT = $3  WHERE C_W_ID = $4    AND C_D_ID = $5    AND C_ID = $6
* 1
{'Node Type': 'ModifyTable', 'Operation': 'Update', 'Parallel Aware': False, 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 577, 'Plans': [{'Node Type': 'Index Scan', 'Parent Relationship': 'Member', 'Parallel Aware': False, 'Scan Direction': 'Forward', 'Index Name': 'customer_pkey', 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 577, 'Index Cond': '((c_w_id = 23) AND (c_d_id = 8) AND (c_id = 1019))'}]}
[{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 23) AND (c_d_id = 8) AND (c_id = 1019))'}, {'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}]
! 3 0.06189192086458206
{'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}
! 2 0.0614020861685276
{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 23) AND (c_d_id = 8) AND (c_id = 1019))'}
! 0 0.0
empty
! 1 0.0
empty
* 2
{'Node Type': 'ModifyTable', 'Operation': 'Update', 'Parallel Aware': False, 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 577, 'Plans': [{'Node Type': 'Index Scan', 'Parent Relationship': 'Member', 'Parallel Aware': False, 'Scan Direction': 'Forward', 'Index Name': 'customer_pkey', 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 577, 'Index Cond': '((c_w_id = 25) AND (c_d_id = 8) AND (c_id = 618))'}]}
[{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 25) AND (c_d_id = 8) AND (c_id = 618))'}, {'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}]
! 3 0.06189192086458206
{'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}
! 2 0.0614020861685276
{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 25) AND (c_d_id = 8) AND (c_id = 618))'}
! 0 0.0
empty
! 1 0.0
empty
* 3
{'Node Type': 'ModifyTable', 'Operation': 'Update', 'Parallel Aware': False, 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 577, 'Plans': [{'Node Type': 'Index Scan', 'Parent Relationship': 'Member', 'Parallel Aware': False, 'Scan Direction': 'Forward', 'Index Name': 'customer_pkey', 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 577, 'Index Cond': '((c_w_id = 30) AND (c_d_id = 4) AND (c_id = 950))'}]}
[{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 30) AND (c_d_id = 4) AND (c_id = 950))'}, {'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}]
! 3 0.06189192086458206
{'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}
! 2 0.0614020861685276
{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 30) AND (c_d_id = 4) AND (c_id = 950))'}
! 0 0.0
empty
! 1 0.0
empty
* 4
{'Node Type': 'ModifyTable', 'Operation': 'Update', 'Parallel Aware': False, 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 577, 'Plans': [{'Node Type': 'Index Scan', 'Parent Relationship': 'Member', 'Parallel Aware': False, 'Scan Direction': 'Forward', 'Index Name': 'customer_pkey', 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 577, 'Index Cond': '((c_w_id = 45) AND (c_d_id = 6) AND (c_id = 747))'}]}
[{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 45) AND (c_d_id = 6) AND (c_id = 747))'}, {'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}]
! 3 0.06189192086458206
{'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}
! 2 0.0614020861685276
{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 45) AND (c_d_id = 6) AND (c_id = 747))'}
! 0 0.0
empty
! 1 0.0
empty
* 5
[{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 45) AND (c_d_id = 6) AND (c_id = 747))'}, {'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}]
! 3 0.06189192086458206
{'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}
! 2 0.0614020861685276
{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 45) AND (c_d_id = 6) AND (c_id = 747))'}
! 0 0.0
empty
! 1 0.0
empty
======Top3 metric
total cpu usage_sys: 0.31491451630094935
dsk/total_read: 0.26583006579657364
total cpu usage_usr: 0.1246023474932716
igs:
[0, 19.983213424682617, 0, 19.984155654907227, 19.985074996948242, 19.9854793548584, 19.98490333557129, 0, 19.983291625976562, 19.98550796508789, 0, 19.98178482055664, 19.98490333557129, 19.98500633239746, 0, 0, 19.985469818115234, 19.985347747802734, 0, 19.98533058166504, 0, 19.98611068725586, 19.98581886291504, 0, 19.98420524597168, 19.986164093017578, 19.984874725341797, 0, 19.984352111816406, 19.98600959777832, 19.98593521118164, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Transformer(
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (2): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (3): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (4): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (5): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
    )
    (norm): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
  )
  (decoder): TransformerDecoder(
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (1): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (2): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (3): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (4): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (5): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
    )
    (norm): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
  )
)
Transformer(
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (2): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (3): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (4): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (5): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
    )
    (norm): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
  )
  (decoder): TransformerDecoder(
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (1): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (2): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (3): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (4): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (5): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
    )
    (norm): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
  )
)
[[[0.0, 0.0, 0.06167038902640343, 0.06192544475197792], [0.0, 0.0, 0.06167038902640343, 0.06192544475197792], [0.0, 0.0, 0.06167038902640343, 0.06192544475197792], [0.0, 0.0, 0.06167038902640343, 0.06192544475197792], [0.0, 0.0, 0.06167038902640343, 0.06192544475197792]], [[0.0, 0.0, 0.0614020861685276, 0.06189192086458206], [0.0, 0.0, 0.0614020861685276, 0.06189192086458206], [0.0, 0.0, 0.0614020861685276, 0.06189192086458206], [0.0, 0.0, 0.0614020861685276, 0.06189192086458206], [0.0, 0.0, 0.0614020861685276, 0.06189192086458206]]]
# 1
UPDATE CUSTOMER   SET C_BALANCE = $1,        C_YTD_PAYMENT = $2,        C_PAYMENT_CNT = $3,        C_DATA = $4  WHERE C_W_ID = $5    AND C_D_ID = $6    AND C_ID = $7
* 1
{'Node Type': 'ModifyTable', 'Operation': 'Update', 'Parallel Aware': False, 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 690, 'Plans': [{'Node Type': 'Index Scan', 'Parent Relationship': 'Member', 'Parallel Aware': False, 'Scan Direction': 'Forward', 'Index Name': 'customer_pkey', 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 690, 'Index Cond': '((c_w_id = 15) AND (c_d_id = 6) AND (c_id = 723))'}]}
[{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 15) AND (c_d_id = 6) AND (c_id = 723))'}, {'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}]
! 3 0.06192544475197792
{'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}
! 2 0.06167038902640343
{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 15) AND (c_d_id = 6) AND (c_id = 723))'}
! 0 0.0
empty
! 1 0.0
empty
* 2
{'Node Type': 'ModifyTable', 'Operation': 'Update', 'Parallel Aware': False, 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 690, 'Plans': [{'Node Type': 'Index Scan', 'Parent Relationship': 'Member', 'Parallel Aware': False, 'Scan Direction': 'Forward', 'Index Name': 'customer_pkey', 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 690, 'Index Cond': '((c_w_id = 25) AND (c_d_id = 1) AND (c_id = 299))'}]}
[{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 25) AND (c_d_id = 1) AND (c_id = 299))'}, {'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}]
! 3 0.06192544475197792
{'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}
! 2 0.06167038902640343
{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 25) AND (c_d_id = 1) AND (c_id = 299))'}
! 0 0.0
empty
! 1 0.0
empty
* 3
{'Node Type': 'ModifyTable', 'Operation': 'Update', 'Parallel Aware': False, 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 690, 'Plans': [{'Node Type': 'Index Scan', 'Parent Relationship': 'Member', 'Parallel Aware': False, 'Scan Direction': 'Forward', 'Index Name': 'customer_pkey', 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 690, 'Index Cond': '((c_w_id = 25) AND (c_d_id = 2) AND (c_id = 238))'}]}
[{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 25) AND (c_d_id = 2) AND (c_id = 238))'}, {'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}]
! 3 0.06192544475197792
{'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}
! 2 0.06167038902640343
{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 25) AND (c_d_id = 2) AND (c_id = 238))'}
! 0 0.0
empty
! 1 0.0
empty
* 4
{'Node Type': 'ModifyTable', 'Operation': 'Update', 'Parallel Aware': False, 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 690, 'Plans': [{'Node Type': 'Index Scan', 'Parent Relationship': 'Member', 'Parallel Aware': False, 'Scan Direction': 'Forward', 'Index Name': 'customer_pkey', 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 690, 'Index Cond': '((c_w_id = 30) AND (c_d_id = 6) AND (c_id = 231))'}]}
[{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 30) AND (c_d_id = 6) AND (c_id = 231))'}, {'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}]
! 3 0.06192544475197792
{'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}
! 2 0.06167038902640343
{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 30) AND (c_d_id = 6) AND (c_id = 231))'}
! 0 0.0
empty
! 1 0.0
empty
* 5
[{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 30) AND (c_d_id = 6) AND (c_id = 231))'}, {'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}]
! 3 0.06192544475197792
{'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}
! 2 0.06167038902640343
{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 30) AND (c_d_id = 6) AND (c_id = 231))'}
! 0 0.0
empty
! 1 0.0
empty
# 2
UPDATE CUSTOMER   SET C_BALANCE = $1,        C_YTD_PAYMENT = $2,        C_PAYMENT_CNT = $3  WHERE C_W_ID = $4    AND C_D_ID = $5    AND C_ID = $6
* 1
{'Node Type': 'ModifyTable', 'Operation': 'Update', 'Parallel Aware': False, 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 577, 'Plans': [{'Node Type': 'Index Scan', 'Parent Relationship': 'Member', 'Parallel Aware': False, 'Scan Direction': 'Forward', 'Index Name': 'customer_pkey', 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 577, 'Index Cond': '((c_w_id = 23) AND (c_d_id = 8) AND (c_id = 1019))'}]}
[{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 23) AND (c_d_id = 8) AND (c_id = 1019))'}, {'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}]
! 3 0.06189192086458206
{'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}
! 2 0.0614020861685276
{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 23) AND (c_d_id = 8) AND (c_id = 1019))'}
! 0 0.0
empty
! 1 0.0
empty
* 2
{'Node Type': 'ModifyTable', 'Operation': 'Update', 'Parallel Aware': False, 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 577, 'Plans': [{'Node Type': 'Index Scan', 'Parent Relationship': 'Member', 'Parallel Aware': False, 'Scan Direction': 'Forward', 'Index Name': 'customer_pkey', 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 577, 'Index Cond': '((c_w_id = 25) AND (c_d_id = 8) AND (c_id = 618))'}]}
[{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 25) AND (c_d_id = 8) AND (c_id = 618))'}, {'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}]
! 3 0.06189192086458206
{'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}
! 2 0.0614020861685276
{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 25) AND (c_d_id = 8) AND (c_id = 618))'}
! 0 0.0
empty
! 1 0.0
empty
* 3
{'Node Type': 'ModifyTable', 'Operation': 'Update', 'Parallel Aware': False, 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 577, 'Plans': [{'Node Type': 'Index Scan', 'Parent Relationship': 'Member', 'Parallel Aware': False, 'Scan Direction': 'Forward', 'Index Name': 'customer_pkey', 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 577, 'Index Cond': '((c_w_id = 30) AND (c_d_id = 4) AND (c_id = 950))'}]}
[{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 30) AND (c_d_id = 4) AND (c_id = 950))'}, {'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}]
! 3 0.06189192086458206
{'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}
! 2 0.0614020861685276
{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 30) AND (c_d_id = 4) AND (c_id = 950))'}
! 0 0.0
empty
! 1 0.0
empty
* 4
{'Node Type': 'ModifyTable', 'Operation': 'Update', 'Parallel Aware': False, 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 577, 'Plans': [{'Node Type': 'Index Scan', 'Parent Relationship': 'Member', 'Parallel Aware': False, 'Scan Direction': 'Forward', 'Index Name': 'customer_pkey', 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 577, 'Index Cond': '((c_w_id = 45) AND (c_d_id = 6) AND (c_id = 747))'}]}
[{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 45) AND (c_d_id = 6) AND (c_id = 747))'}, {'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}]
! 3 0.06189192086458206
{'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}
! 2 0.0614020861685276
{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 45) AND (c_d_id = 6) AND (c_id = 747))'}
! 0 0.0
empty
! 1 0.0
empty
* 5
[{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 45) AND (c_d_id = 6) AND (c_id = 747))'}, {'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}]
! 3 0.06189192086458206
{'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}
! 2 0.0614020861685276
{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 45) AND (c_d_id = 6) AND (c_id = 747))'}
! 0 0.0
empty
! 1 0.0
empty
======Top3 metric
total cpu usage_sys: 0.3937213527333413
dsk/total_read: 0.26954924972598954
total cpu usage_usr: 0.12623400732318446
igs:
[0, 233.789794921875, 233.7614288330078, 233.8008270263672, 233.81158447265625, 233.81631469726562, 233.8095703125, 233.80178833007812, 233.7907257080078, 233.816650390625, 233.789794921875, 233.77310180664062, 233.8095703125, 233.81077575683594, 233.8123016357422, 233.80728149414062, 233.8162078857422, 233.81475830078125, 233.7914581298828, 233.81455993652344, 233.7969970703125, 233.82371520996094, 233.82029724121094, 233.811279296875, 233.80142211914062, 233.82432556152344, 233.80923461914062, 233.82286071777344, 233.80311584472656, 233.82252502441406, 233.82164001464844, 233.82008361816406, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Transformer(
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (2): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (3): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (4): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (5): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
    )
    (norm): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
  )
  (decoder): TransformerDecoder(
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (1): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (2): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (3): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (4): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (5): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
    )
    (norm): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
  )
)
Transformer(
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (2): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (3): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (4): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (5): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
    )
    (norm): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
  )
  (decoder): TransformerDecoder(
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (1): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (2): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (3): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (4): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (5): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
    )
    (norm): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
  )
)
[[[0.0, 0.0, 0.06167038902640343, 0.06192544475197792], [0.0, 0.0, 0.06167038902640343, 0.06192544475197792], [0.0, 0.0, 0.06167038902640343, 0.06192544475197792], [0.0, 0.0, 0.06167038902640343, 0.06192544475197792], [0.0, 0.0, 0.06167038902640343, 0.06192544475197792]], [[0.0, 0.0, 0.0614020861685276, 0.06189192086458206], [0.0, 0.0, 0.0614020861685276, 0.06189192086458206], [0.0, 0.0, 0.0614020861685276, 0.06189192086458206], [0.0, 0.0, 0.0614020861685276, 0.06189192086458206], [0.0, 0.0, 0.0614020861685276, 0.06189192086458206]]]
# 1
UPDATE CUSTOMER   SET C_BALANCE = $1,        C_YTD_PAYMENT = $2,        C_PAYMENT_CNT = $3,        C_DATA = $4  WHERE C_W_ID = $5    AND C_D_ID = $6    AND C_ID = $7
* 1
{'Node Type': 'ModifyTable', 'Operation': 'Update', 'Parallel Aware': False, 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 690, 'Plans': [{'Node Type': 'Index Scan', 'Parent Relationship': 'Member', 'Parallel Aware': False, 'Scan Direction': 'Forward', 'Index Name': 'customer_pkey', 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 690, 'Index Cond': '((c_w_id = 15) AND (c_d_id = 6) AND (c_id = 723))'}]}
[{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 15) AND (c_d_id = 6) AND (c_id = 723))'}, {'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}]
! 3 0.06192544475197792
{'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}
! 2 0.06167038902640343
{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 15) AND (c_d_id = 6) AND (c_id = 723))'}
! 0 0.0
empty
! 1 0.0
empty
* 2
{'Node Type': 'ModifyTable', 'Operation': 'Update', 'Parallel Aware': False, 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 690, 'Plans': [{'Node Type': 'Index Scan', 'Parent Relationship': 'Member', 'Parallel Aware': False, 'Scan Direction': 'Forward', 'Index Name': 'customer_pkey', 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 690, 'Index Cond': '((c_w_id = 25) AND (c_d_id = 1) AND (c_id = 299))'}]}
[{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 25) AND (c_d_id = 1) AND (c_id = 299))'}, {'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}]
! 3 0.06192544475197792
{'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}
! 2 0.06167038902640343
{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 25) AND (c_d_id = 1) AND (c_id = 299))'}
! 0 0.0
empty
! 1 0.0
empty
* 3
{'Node Type': 'ModifyTable', 'Operation': 'Update', 'Parallel Aware': False, 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 690, 'Plans': [{'Node Type': 'Index Scan', 'Parent Relationship': 'Member', 'Parallel Aware': False, 'Scan Direction': 'Forward', 'Index Name': 'customer_pkey', 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 690, 'Index Cond': '((c_w_id = 25) AND (c_d_id = 2) AND (c_id = 238))'}]}
[{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 25) AND (c_d_id = 2) AND (c_id = 238))'}, {'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}]
! 3 0.06192544475197792
{'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}
! 2 0.06167038902640343
{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 25) AND (c_d_id = 2) AND (c_id = 238))'}
! 0 0.0
empty
! 1 0.0
empty
* 4
{'Node Type': 'ModifyTable', 'Operation': 'Update', 'Parallel Aware': False, 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 690, 'Plans': [{'Node Type': 'Index Scan', 'Parent Relationship': 'Member', 'Parallel Aware': False, 'Scan Direction': 'Forward', 'Index Name': 'customer_pkey', 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 690, 'Index Cond': '((c_w_id = 30) AND (c_d_id = 6) AND (c_id = 231))'}]}
[{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 30) AND (c_d_id = 6) AND (c_id = 231))'}, {'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}]
! 3 0.06192544475197792
{'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}
! 2 0.06167038902640343
{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 30) AND (c_d_id = 6) AND (c_id = 231))'}
! 0 0.0
empty
! 1 0.0
empty
* 5
[{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 30) AND (c_d_id = 6) AND (c_id = 231))'}, {'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}]
! 3 0.06192544475197792
{'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}
! 2 0.06167038902640343
{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 30) AND (c_d_id = 6) AND (c_id = 231))'}
! 0 0.0
empty
! 1 0.0
empty
# 2
UPDATE CUSTOMER   SET C_BALANCE = $1,        C_YTD_PAYMENT = $2,        C_PAYMENT_CNT = $3  WHERE C_W_ID = $4    AND C_D_ID = $5    AND C_ID = $6
* 1
{'Node Type': 'ModifyTable', 'Operation': 'Update', 'Parallel Aware': False, 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 577, 'Plans': [{'Node Type': 'Index Scan', 'Parent Relationship': 'Member', 'Parallel Aware': False, 'Scan Direction': 'Forward', 'Index Name': 'customer_pkey', 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 577, 'Index Cond': '((c_w_id = 23) AND (c_d_id = 8) AND (c_id = 1019))'}]}
[{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 23) AND (c_d_id = 8) AND (c_id = 1019))'}, {'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}]
! 3 0.06189192086458206
{'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}
! 2 0.0614020861685276
{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 23) AND (c_d_id = 8) AND (c_id = 1019))'}
! 0 0.0
empty
! 1 0.0
empty
* 2
{'Node Type': 'ModifyTable', 'Operation': 'Update', 'Parallel Aware': False, 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 577, 'Plans': [{'Node Type': 'Index Scan', 'Parent Relationship': 'Member', 'Parallel Aware': False, 'Scan Direction': 'Forward', 'Index Name': 'customer_pkey', 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 577, 'Index Cond': '((c_w_id = 25) AND (c_d_id = 8) AND (c_id = 618))'}]}
[{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 25) AND (c_d_id = 8) AND (c_id = 618))'}, {'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}]
! 3 0.06189192086458206
{'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}
! 2 0.0614020861685276
{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 25) AND (c_d_id = 8) AND (c_id = 618))'}
! 0 0.0
empty
! 1 0.0
empty
* 3
{'Node Type': 'ModifyTable', 'Operation': 'Update', 'Parallel Aware': False, 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 577, 'Plans': [{'Node Type': 'Index Scan', 'Parent Relationship': 'Member', 'Parallel Aware': False, 'Scan Direction': 'Forward', 'Index Name': 'customer_pkey', 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 577, 'Index Cond': '((c_w_id = 30) AND (c_d_id = 4) AND (c_id = 950))'}]}
[{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 30) AND (c_d_id = 4) AND (c_id = 950))'}, {'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}]
! 3 0.06189192086458206
{'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}
! 2 0.0614020861685276
{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 30) AND (c_d_id = 4) AND (c_id = 950))'}
! 0 0.0
empty
! 1 0.0
empty
* 4
{'Node Type': 'ModifyTable', 'Operation': 'Update', 'Parallel Aware': False, 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 577, 'Plans': [{'Node Type': 'Index Scan', 'Parent Relationship': 'Member', 'Parallel Aware': False, 'Scan Direction': 'Forward', 'Index Name': 'customer_pkey', 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 577, 'Index Cond': '((c_w_id = 45) AND (c_d_id = 6) AND (c_id = 747))'}]}
[{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 45) AND (c_d_id = 6) AND (c_id = 747))'}, {'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}]
! 3 0.06189192086458206
{'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}
! 2 0.0614020861685276
{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 45) AND (c_d_id = 6) AND (c_id = 747))'}
! 0 0.0
empty
! 1 0.0
empty
* 5
[{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 45) AND (c_d_id = 6) AND (c_id = 747))'}, {'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}]
! 3 0.06189192086458206
{'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}
! 2 0.0614020861685276
{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 45) AND (c_d_id = 6) AND (c_id = 747))'}
! 0 0.0
empty
! 1 0.0
empty
======Top3 metric
total cpu usage_sys: 0.38772744143469734
dsk/total_read: 0.2568920636063575
total cpu usage_usr: 0.12230007232445873
igs:
[0, 203.7664031982422, 203.74168395996094, 203.7760009765625, 203.78538513183594, 203.7895050048828, 203.78363037109375, 203.77685546875, 203.7672119140625, 203.789794921875, 203.7664031982422, 203.75184631347656, 203.78363037109375, 203.78468322753906, 203.7860107421875, 203.78164672851562, 203.78941345214844, 203.7881622314453, 203.76785278320312, 203.78797912597656, 203.77267456054688, 203.79595947265625, 203.79298400878906, 203.7851104736328, 203.77651977539062, 203.79649353027344, 203.78334045410156, 203.7952117919922, 203.77801513671875, 203.794921875, 203.79415893554688, 203.79278564453125, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Transformer(
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (2): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (3): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (4): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (5): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
    )
    (norm): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
  )
  (decoder): TransformerDecoder(
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (1): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (2): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (3): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (4): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (5): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
    )
    (norm): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
  )
)
Transformer(
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (2): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (3): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (4): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (5): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
    )
    (norm): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
  )
  (decoder): TransformerDecoder(
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (1): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (2): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (3): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (4): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (5): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
    )
    (norm): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
  )
)
[[[0.0, 0.0, 0.06167038902640343, 0.06192544475197792], [0.0, 0.0, 0.06167038902640343, 0.06192544475197792], [0.0, 0.0, 0.06167038902640343, 0.06192544475197792], [0.0, 0.0, 0.06167038902640343, 0.06192544475197792], [0.0, 0.0, 0.06167038902640343, 0.06192544475197792]], [[0.0, 0.0, 0.0614020861685276, 0.06189192086458206], [0.0, 0.0, 0.0614020861685276, 0.06189192086458206], [0.0, 0.0, 0.0614020861685276, 0.06189192086458206], [0.0, 0.0, 0.0614020861685276, 0.06189192086458206], [0.0, 0.0, 0.0614020861685276, 0.06189192086458206]]]
# 1
UPDATE CUSTOMER   SET C_BALANCE = $1,        C_YTD_PAYMENT = $2,        C_PAYMENT_CNT = $3,        C_DATA = $4  WHERE C_W_ID = $5    AND C_D_ID = $6    AND C_ID = $7
* 1
{'Node Type': 'ModifyTable', 'Operation': 'Update', 'Parallel Aware': False, 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 690, 'Plans': [{'Node Type': 'Index Scan', 'Parent Relationship': 'Member', 'Parallel Aware': False, 'Scan Direction': 'Forward', 'Index Name': 'customer_pkey', 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 690, 'Index Cond': '((c_w_id = 15) AND (c_d_id = 6) AND (c_id = 723))'}]}
[{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 15) AND (c_d_id = 6) AND (c_id = 723))'}, {'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}]
! 3 0.06192544475197792
{'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}
! 2 0.06167038902640343
{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 15) AND (c_d_id = 6) AND (c_id = 723))'}
! 0 0.0
empty
! 1 0.0
empty
* 2
{'Node Type': 'ModifyTable', 'Operation': 'Update', 'Parallel Aware': False, 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 690, 'Plans': [{'Node Type': 'Index Scan', 'Parent Relationship': 'Member', 'Parallel Aware': False, 'Scan Direction': 'Forward', 'Index Name': 'customer_pkey', 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 690, 'Index Cond': '((c_w_id = 25) AND (c_d_id = 1) AND (c_id = 299))'}]}
[{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 25) AND (c_d_id = 1) AND (c_id = 299))'}, {'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}]
! 3 0.06192544475197792
{'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}
! 2 0.06167038902640343
{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 25) AND (c_d_id = 1) AND (c_id = 299))'}
! 0 0.0
empty
! 1 0.0
empty
* 3
{'Node Type': 'ModifyTable', 'Operation': 'Update', 'Parallel Aware': False, 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 690, 'Plans': [{'Node Type': 'Index Scan', 'Parent Relationship': 'Member', 'Parallel Aware': False, 'Scan Direction': 'Forward', 'Index Name': 'customer_pkey', 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 690, 'Index Cond': '((c_w_id = 25) AND (c_d_id = 2) AND (c_id = 238))'}]}
[{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 25) AND (c_d_id = 2) AND (c_id = 238))'}, {'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}]
! 3 0.06192544475197792
{'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}
! 2 0.06167038902640343
{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 25) AND (c_d_id = 2) AND (c_id = 238))'}
! 0 0.0
empty
! 1 0.0
empty
* 4
{'Node Type': 'ModifyTable', 'Operation': 'Update', 'Parallel Aware': False, 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 690, 'Plans': [{'Node Type': 'Index Scan', 'Parent Relationship': 'Member', 'Parallel Aware': False, 'Scan Direction': 'Forward', 'Index Name': 'customer_pkey', 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 690, 'Index Cond': '((c_w_id = 30) AND (c_d_id = 6) AND (c_id = 231))'}]}
[{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 30) AND (c_d_id = 6) AND (c_id = 231))'}, {'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}]
! 3 0.06192544475197792
{'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}
! 2 0.06167038902640343
{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 30) AND (c_d_id = 6) AND (c_id = 231))'}
! 0 0.0
empty
! 1 0.0
empty
* 5
[{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 30) AND (c_d_id = 6) AND (c_id = 231))'}, {'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}]
! 3 0.06192544475197792
{'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}
! 2 0.06167038902640343
{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 30) AND (c_d_id = 6) AND (c_id = 231))'}
! 0 0.0
empty
! 1 0.0
empty
# 2
UPDATE CUSTOMER   SET C_BALANCE = $1,        C_YTD_PAYMENT = $2,        C_PAYMENT_CNT = $3  WHERE C_W_ID = $4    AND C_D_ID = $5    AND C_ID = $6
* 1
{'Node Type': 'ModifyTable', 'Operation': 'Update', 'Parallel Aware': False, 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 577, 'Plans': [{'Node Type': 'Index Scan', 'Parent Relationship': 'Member', 'Parallel Aware': False, 'Scan Direction': 'Forward', 'Index Name': 'customer_pkey', 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 577, 'Index Cond': '((c_w_id = 23) AND (c_d_id = 8) AND (c_id = 1019))'}]}
[{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 23) AND (c_d_id = 8) AND (c_id = 1019))'}, {'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}]
! 3 0.06189192086458206
{'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}
! 2 0.0614020861685276
{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 23) AND (c_d_id = 8) AND (c_id = 1019))'}
! 0 0.0
empty
! 1 0.0
empty
* 2
{'Node Type': 'ModifyTable', 'Operation': 'Update', 'Parallel Aware': False, 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 577, 'Plans': [{'Node Type': 'Index Scan', 'Parent Relationship': 'Member', 'Parallel Aware': False, 'Scan Direction': 'Forward', 'Index Name': 'customer_pkey', 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 577, 'Index Cond': '((c_w_id = 25) AND (c_d_id = 8) AND (c_id = 618))'}]}
[{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 25) AND (c_d_id = 8) AND (c_id = 618))'}, {'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}]
! 3 0.06189192086458206
{'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}
! 2 0.0614020861685276
{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 25) AND (c_d_id = 8) AND (c_id = 618))'}
! 0 0.0
empty
! 1 0.0
empty
* 3
{'Node Type': 'ModifyTable', 'Operation': 'Update', 'Parallel Aware': False, 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 577, 'Plans': [{'Node Type': 'Index Scan', 'Parent Relationship': 'Member', 'Parallel Aware': False, 'Scan Direction': 'Forward', 'Index Name': 'customer_pkey', 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 577, 'Index Cond': '((c_w_id = 30) AND (c_d_id = 4) AND (c_id = 950))'}]}
[{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 30) AND (c_d_id = 4) AND (c_id = 950))'}, {'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}]
! 3 0.06189192086458206
{'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}
! 2 0.0614020861685276
{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 30) AND (c_d_id = 4) AND (c_id = 950))'}
! 0 0.0
empty
! 1 0.0
empty
* 4
{'Node Type': 'ModifyTable', 'Operation': 'Update', 'Parallel Aware': False, 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 577, 'Plans': [{'Node Type': 'Index Scan', 'Parent Relationship': 'Member', 'Parallel Aware': False, 'Scan Direction': 'Forward', 'Index Name': 'customer_pkey', 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 577, 'Index Cond': '((c_w_id = 45) AND (c_d_id = 6) AND (c_id = 747))'}]}
[{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 45) AND (c_d_id = 6) AND (c_id = 747))'}, {'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}]
! 3 0.06189192086458206
{'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}
! 2 0.0614020861685276
{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 45) AND (c_d_id = 6) AND (c_id = 747))'}
! 0 0.0
empty
! 1 0.0
empty
* 5
[{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 45) AND (c_d_id = 6) AND (c_id = 747))'}, {'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}]
! 3 0.06189192086458206
{'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}
! 2 0.0614020861685276
{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 45) AND (c_d_id = 6) AND (c_id = 747))'}
! 0 0.0
empty
! 1 0.0
empty
======Top3 metric
total cpu usage_sys: 0.39541073329229837
dsk/total_read: 0.26875588888922686
total cpu usage_idl: 0.12509217623712224
igs:
[0, 316.0240478515625, 315.9857177734375, 316.0389709472656, 316.05352783203125, 316.0599060058594, 316.05078125, 316.040283203125, 316.02532958984375, 316.06036376953125, 316.0240478515625, 316.0014953613281, 316.05078125, 316.05242919921875, 316.05450439453125, 316.0476989746094, 316.0597839355469, 316.05780029296875, 316.02630615234375, 316.05755615234375, 316.0337829589844, 316.0699157714844, 316.0653076171875, 316.0531005859375, 316.0397644042969, 316.07073974609375, 316.0503234863281, 316.0687561035156, 316.0420837402344, 316.06829833984375, 316.0671081542969, 316.06500244140625, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Transformer(
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (2): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (3): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (4): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (5): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
    )
    (norm): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
  )
  (decoder): TransformerDecoder(
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (1): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (2): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (3): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (4): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (5): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
    )
    (norm): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
  )
)
Transformer(
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (2): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (3): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (4): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (5): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
    )
    (norm): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
  )
  (decoder): TransformerDecoder(
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (1): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (2): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (3): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (4): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (5): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
    )
    (norm): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
  )
)
[[[0.0, 0.0, 0.06167038902640343, 0.06192544475197792], [0.0, 0.0, 0.06167038902640343, 0.06192544475197792], [0.0, 0.0, 0.06167038902640343, 0.06192544475197792], [0.0, 0.0, 0.06167038902640343, 0.06192544475197792], [0.0, 0.0, 0.06167038902640343, 0.06192544475197792]], [[0.0, 0.0, 0.0614020861685276, 0.06189192086458206], [0.0, 0.0, 0.0614020861685276, 0.06189192086458206], [0.0, 0.0, 0.0614020861685276, 0.06189192086458206], [0.0, 0.0, 0.0614020861685276, 0.06189192086458206], [0.0, 0.0, 0.0614020861685276, 0.06189192086458206]]]
# 1
UPDATE CUSTOMER   SET C_BALANCE = $1,        C_YTD_PAYMENT = $2,        C_PAYMENT_CNT = $3,        C_DATA = $4  WHERE C_W_ID = $5    AND C_D_ID = $6    AND C_ID = $7
* 1
{'Node Type': 'ModifyTable', 'Operation': 'Update', 'Parallel Aware': False, 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 690, 'Plans': [{'Node Type': 'Index Scan', 'Parent Relationship': 'Member', 'Parallel Aware': False, 'Scan Direction': 'Forward', 'Index Name': 'customer_pkey', 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 690, 'Index Cond': '((c_w_id = 15) AND (c_d_id = 6) AND (c_id = 723))'}]}
[{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 15) AND (c_d_id = 6) AND (c_id = 723))'}, {'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}]
! 3 0.06192544475197792
{'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}
! 2 0.06167038902640343
{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 15) AND (c_d_id = 6) AND (c_id = 723))'}
! 0 0.0
empty
! 1 0.0
empty
* 2
{'Node Type': 'ModifyTable', 'Operation': 'Update', 'Parallel Aware': False, 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 690, 'Plans': [{'Node Type': 'Index Scan', 'Parent Relationship': 'Member', 'Parallel Aware': False, 'Scan Direction': 'Forward', 'Index Name': 'customer_pkey', 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 690, 'Index Cond': '((c_w_id = 25) AND (c_d_id = 1) AND (c_id = 299))'}]}
[{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 25) AND (c_d_id = 1) AND (c_id = 299))'}, {'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}]
! 3 0.06192544475197792
{'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}
! 2 0.06167038902640343
{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 25) AND (c_d_id = 1) AND (c_id = 299))'}
! 0 0.0
empty
! 1 0.0
empty
* 3
{'Node Type': 'ModifyTable', 'Operation': 'Update', 'Parallel Aware': False, 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 690, 'Plans': [{'Node Type': 'Index Scan', 'Parent Relationship': 'Member', 'Parallel Aware': False, 'Scan Direction': 'Forward', 'Index Name': 'customer_pkey', 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 690, 'Index Cond': '((c_w_id = 25) AND (c_d_id = 2) AND (c_id = 238))'}]}
[{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 25) AND (c_d_id = 2) AND (c_id = 238))'}, {'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}]
! 3 0.06192544475197792
{'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}
! 2 0.06167038902640343
{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 25) AND (c_d_id = 2) AND (c_id = 238))'}
! 0 0.0
empty
! 1 0.0
empty
* 4
{'Node Type': 'ModifyTable', 'Operation': 'Update', 'Parallel Aware': False, 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 690, 'Plans': [{'Node Type': 'Index Scan', 'Parent Relationship': 'Member', 'Parallel Aware': False, 'Scan Direction': 'Forward', 'Index Name': 'customer_pkey', 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 690, 'Index Cond': '((c_w_id = 30) AND (c_d_id = 6) AND (c_id = 231))'}]}
[{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 30) AND (c_d_id = 6) AND (c_id = 231))'}, {'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}]
! 3 0.06192544475197792
{'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}
! 2 0.06167038902640343
{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 30) AND (c_d_id = 6) AND (c_id = 231))'}
! 0 0.0
empty
! 1 0.0
empty
* 5
[{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 30) AND (c_d_id = 6) AND (c_id = 231))'}, {'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}]
! 3 0.06192544475197792
{'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}
! 2 0.06167038902640343
{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 30) AND (c_d_id = 6) AND (c_id = 231))'}
! 0 0.0
empty
! 1 0.0
empty
# 2
UPDATE CUSTOMER   SET C_BALANCE = $1,        C_YTD_PAYMENT = $2,        C_PAYMENT_CNT = $3  WHERE C_W_ID = $4    AND C_D_ID = $5    AND C_ID = $6
* 1
{'Node Type': 'ModifyTable', 'Operation': 'Update', 'Parallel Aware': False, 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 577, 'Plans': [{'Node Type': 'Index Scan', 'Parent Relationship': 'Member', 'Parallel Aware': False, 'Scan Direction': 'Forward', 'Index Name': 'customer_pkey', 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 577, 'Index Cond': '((c_w_id = 23) AND (c_d_id = 8) AND (c_id = 1019))'}]}
[{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 23) AND (c_d_id = 8) AND (c_id = 1019))'}, {'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}]
! 3 0.06189192086458206
{'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}
! 2 0.0614020861685276
{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 23) AND (c_d_id = 8) AND (c_id = 1019))'}
! 0 0.0
empty
! 1 0.0
empty
* 2
{'Node Type': 'ModifyTable', 'Operation': 'Update', 'Parallel Aware': False, 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 577, 'Plans': [{'Node Type': 'Index Scan', 'Parent Relationship': 'Member', 'Parallel Aware': False, 'Scan Direction': 'Forward', 'Index Name': 'customer_pkey', 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 577, 'Index Cond': '((c_w_id = 25) AND (c_d_id = 8) AND (c_id = 618))'}]}
[{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 25) AND (c_d_id = 8) AND (c_id = 618))'}, {'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}]
! 3 0.06189192086458206
{'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}
! 2 0.0614020861685276
{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 25) AND (c_d_id = 8) AND (c_id = 618))'}
! 0 0.0
empty
! 1 0.0
empty
* 3
{'Node Type': 'ModifyTable', 'Operation': 'Update', 'Parallel Aware': False, 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 577, 'Plans': [{'Node Type': 'Index Scan', 'Parent Relationship': 'Member', 'Parallel Aware': False, 'Scan Direction': 'Forward', 'Index Name': 'customer_pkey', 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 577, 'Index Cond': '((c_w_id = 30) AND (c_d_id = 4) AND (c_id = 950))'}]}
[{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 30) AND (c_d_id = 4) AND (c_id = 950))'}, {'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}]
! 3 0.06189192086458206
{'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}
! 2 0.0614020861685276
{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 30) AND (c_d_id = 4) AND (c_id = 950))'}
! 0 0.0
empty
! 1 0.0
empty
* 4
{'Node Type': 'ModifyTable', 'Operation': 'Update', 'Parallel Aware': False, 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 577, 'Plans': [{'Node Type': 'Index Scan', 'Parent Relationship': 'Member', 'Parallel Aware': False, 'Scan Direction': 'Forward', 'Index Name': 'customer_pkey', 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 577, 'Index Cond': '((c_w_id = 45) AND (c_d_id = 6) AND (c_id = 747))'}]}
[{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 45) AND (c_d_id = 6) AND (c_id = 747))'}, {'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}]
! 3 0.06189192086458206
{'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}
! 2 0.0614020861685276
{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 45) AND (c_d_id = 6) AND (c_id = 747))'}
! 0 0.0
empty
! 1 0.0
empty
* 5
[{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 45) AND (c_d_id = 6) AND (c_id = 747))'}, {'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}]
! 3 0.06189192086458206
{'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}
! 2 0.0614020861685276
{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 45) AND (c_d_id = 6) AND (c_id = 747))'}
! 0 0.0
empty
! 1 0.0
empty
======Top3 metric
total cpu usage_sys: 0.46833335926067715
dsk/total_read: 0.29877914092838664
total cpu usage_usr: 0.10986065877282064
igs:
[0, 63.53432083129883, 63.526611328125, 63.537315368652344, 63.54024124145508, 63.54152297973633, 63.53969192504883, 63.53757858276367, 63.53457260131836, 63.5416145324707, 63.53432083129883, 63.529781341552734, 63.53969192504883, 63.54001998901367, 63.540435791015625, 63.5390739440918, 63.54149627685547, 63.54110336303711, 63.53477096557617, 63.54104995727539, 63.53627395629883, 63.54353332519531, 63.542606353759766, 63.540157318115234, 63.5374755859375, 63.543701171875, 63.53960037231445, 63.543304443359375, 63.537940979003906, 63.543212890625, 63.54297637939453, 63.54254913330078, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Transformer(
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (2): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (3): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (4): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (5): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
    )
    (norm): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
  )
  (decoder): TransformerDecoder(
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (1): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (2): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (3): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (4): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (5): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
    )
    (norm): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
  )
)
Transformer(
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (2): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (3): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (4): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (5): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
    )
    (norm): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
  )
  (decoder): TransformerDecoder(
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (1): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (2): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (3): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (4): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (5): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
    )
    (norm): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
  )
)
[[[0.0, 0.0, 0.06167038902640343, 0.06192544475197792], [0.0, 0.0, 0.06167038902640343, 0.06192544475197792], [0.0, 0.0, 0.06167038902640343, 0.06192544475197792], [0.0, 0.0, 0.06167038902640343, 0.06192544475197792], [0.0, 0.0, 0.06167038902640343, 0.06192544475197792]], [[0.0, 0.0, 0.0614020861685276, 0.06189192086458206], [0.0, 0.0, 0.0614020861685276, 0.06189192086458206], [0.0, 0.0, 0.0614020861685276, 0.06189192086458206], [0.0, 0.0, 0.0614020861685276, 0.06189192086458206], [0.0, 0.0, 0.0614020861685276, 0.06189192086458206]]]
# 1
UPDATE CUSTOMER   SET C_BALANCE = $1,        C_YTD_PAYMENT = $2,        C_PAYMENT_CNT = $3,        C_DATA = $4  WHERE C_W_ID = $5    AND C_D_ID = $6    AND C_ID = $7
* 1
{'Node Type': 'ModifyTable', 'Operation': 'Update', 'Parallel Aware': False, 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 690, 'Plans': [{'Node Type': 'Index Scan', 'Parent Relationship': 'Member', 'Parallel Aware': False, 'Scan Direction': 'Forward', 'Index Name': 'customer_pkey', 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 690, 'Index Cond': '((c_w_id = 15) AND (c_d_id = 6) AND (c_id = 723))'}]}
[{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 15) AND (c_d_id = 6) AND (c_id = 723))'}, {'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}]
! 3 0.06192544475197792
{'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}
! 2 0.06167038902640343
{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 15) AND (c_d_id = 6) AND (c_id = 723))'}
! 0 0.0
empty
! 1 0.0
empty
* 2
{'Node Type': 'ModifyTable', 'Operation': 'Update', 'Parallel Aware': False, 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 690, 'Plans': [{'Node Type': 'Index Scan', 'Parent Relationship': 'Member', 'Parallel Aware': False, 'Scan Direction': 'Forward', 'Index Name': 'customer_pkey', 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 690, 'Index Cond': '((c_w_id = 25) AND (c_d_id = 1) AND (c_id = 299))'}]}
[{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 25) AND (c_d_id = 1) AND (c_id = 299))'}, {'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}]
! 3 0.06192544475197792
{'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}
! 2 0.06167038902640343
{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 25) AND (c_d_id = 1) AND (c_id = 299))'}
! 0 0.0
empty
! 1 0.0
empty
* 3
{'Node Type': 'ModifyTable', 'Operation': 'Update', 'Parallel Aware': False, 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 690, 'Plans': [{'Node Type': 'Index Scan', 'Parent Relationship': 'Member', 'Parallel Aware': False, 'Scan Direction': 'Forward', 'Index Name': 'customer_pkey', 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 690, 'Index Cond': '((c_w_id = 25) AND (c_d_id = 2) AND (c_id = 238))'}]}
[{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 25) AND (c_d_id = 2) AND (c_id = 238))'}, {'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}]
! 3 0.06192544475197792
{'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}
! 2 0.06167038902640343
{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 25) AND (c_d_id = 2) AND (c_id = 238))'}
! 0 0.0
empty
! 1 0.0
empty
* 4
{'Node Type': 'ModifyTable', 'Operation': 'Update', 'Parallel Aware': False, 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 690, 'Plans': [{'Node Type': 'Index Scan', 'Parent Relationship': 'Member', 'Parallel Aware': False, 'Scan Direction': 'Forward', 'Index Name': 'customer_pkey', 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 690, 'Index Cond': '((c_w_id = 30) AND (c_d_id = 6) AND (c_id = 231))'}]}
[{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 30) AND (c_d_id = 6) AND (c_id = 231))'}, {'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}]
! 3 0.06192544475197792
{'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}
! 2 0.06167038902640343
{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 30) AND (c_d_id = 6) AND (c_id = 231))'}
! 0 0.0
empty
! 1 0.0
empty
* 5
[{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 30) AND (c_d_id = 6) AND (c_id = 231))'}, {'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}]
! 3 0.06192544475197792
{'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}
! 2 0.06167038902640343
{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 30) AND (c_d_id = 6) AND (c_id = 231))'}
! 0 0.0
empty
! 1 0.0
empty
# 2
UPDATE CUSTOMER   SET C_BALANCE = $1,        C_YTD_PAYMENT = $2,        C_PAYMENT_CNT = $3  WHERE C_W_ID = $4    AND C_D_ID = $5    AND C_ID = $6
* 1
{'Node Type': 'ModifyTable', 'Operation': 'Update', 'Parallel Aware': False, 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 577, 'Plans': [{'Node Type': 'Index Scan', 'Parent Relationship': 'Member', 'Parallel Aware': False, 'Scan Direction': 'Forward', 'Index Name': 'customer_pkey', 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 577, 'Index Cond': '((c_w_id = 23) AND (c_d_id = 8) AND (c_id = 1019))'}]}
[{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 23) AND (c_d_id = 8) AND (c_id = 1019))'}, {'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}]
! 3 0.06189192086458206
{'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}
! 2 0.0614020861685276
{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 23) AND (c_d_id = 8) AND (c_id = 1019))'}
! 0 0.0
empty
! 1 0.0
empty
* 2
{'Node Type': 'ModifyTable', 'Operation': 'Update', 'Parallel Aware': False, 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 577, 'Plans': [{'Node Type': 'Index Scan', 'Parent Relationship': 'Member', 'Parallel Aware': False, 'Scan Direction': 'Forward', 'Index Name': 'customer_pkey', 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 577, 'Index Cond': '((c_w_id = 25) AND (c_d_id = 8) AND (c_id = 618))'}]}
[{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 25) AND (c_d_id = 8) AND (c_id = 618))'}, {'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}]
! 3 0.06189192086458206
{'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}
! 2 0.0614020861685276
{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 25) AND (c_d_id = 8) AND (c_id = 618))'}
! 0 0.0
empty
! 1 0.0
empty
* 3
{'Node Type': 'ModifyTable', 'Operation': 'Update', 'Parallel Aware': False, 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 577, 'Plans': [{'Node Type': 'Index Scan', 'Parent Relationship': 'Member', 'Parallel Aware': False, 'Scan Direction': 'Forward', 'Index Name': 'customer_pkey', 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 577, 'Index Cond': '((c_w_id = 30) AND (c_d_id = 4) AND (c_id = 950))'}]}
[{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 30) AND (c_d_id = 4) AND (c_id = 950))'}, {'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}]
! 3 0.06189192086458206
{'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}
! 2 0.0614020861685276
{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 30) AND (c_d_id = 4) AND (c_id = 950))'}
! 0 0.0
empty
! 1 0.0
empty
* 4
{'Node Type': 'ModifyTable', 'Operation': 'Update', 'Parallel Aware': False, 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 577, 'Plans': [{'Node Type': 'Index Scan', 'Parent Relationship': 'Member', 'Parallel Aware': False, 'Scan Direction': 'Forward', 'Index Name': 'customer_pkey', 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 577, 'Index Cond': '((c_w_id = 45) AND (c_d_id = 6) AND (c_id = 747))'}]}
[{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 45) AND (c_d_id = 6) AND (c_id = 747))'}, {'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}]
! 3 0.06189192086458206
{'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}
! 2 0.0614020861685276
{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 45) AND (c_d_id = 6) AND (c_id = 747))'}
! 0 0.0
empty
! 1 0.0
empty
* 5
[{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 45) AND (c_d_id = 6) AND (c_id = 747))'}, {'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}]
! 3 0.06189192086458206
{'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}
! 2 0.0614020861685276
{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 45) AND (c_d_id = 6) AND (c_id = 747))'}
! 0 0.0
empty
! 1 0.0
empty
======Top3 metric
total cpu usage_sys: 0.4453019221018032
dsk/total_read: 0.278649070722627
total cpu usage_idl: 0.09640192850136045
igs:
[0, 87.20330047607422, 87.19271850585938, 87.20741271972656, 87.21142578125, 87.21318817138672, 87.2106704711914, 87.20777130126953, 87.20364379882812, 87.21331787109375, 87.20330047607422, 87.19706726074219, 87.2106704711914, 87.21112823486328, 87.2116928100586, 87.20982360839844, 87.21315002441406, 87.21260833740234, 87.20391845703125, 87.21253967285156, 87.20598602294922, 87.21595001220703, 87.21467590332031, 87.21131134033203, 87.20763397216797, 87.21617889404297, 87.2105484008789, 87.21562957763672, 87.20826721191406, 87.21550750732422, 87.21517944335938, 87.21459197998047, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Transformer(
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (2): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (3): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (4): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (5): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
    )
    (norm): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
  )
  (decoder): TransformerDecoder(
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (1): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (2): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (3): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (4): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (5): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
    )
    (norm): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
  )
)
Transformer(
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (2): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (3): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (4): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (5): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
    )
    (norm): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
  )
  (decoder): TransformerDecoder(
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (1): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (2): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (3): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (4): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (5): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
    )
    (norm): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
  )
)
[[[0.0, 0.0, 0.06167038902640343, 0.06192544475197792], [0.0, 0.0, 0.06167038902640343, 0.06192544475197792], [0.0, 0.0, 0.06167038902640343, 0.06192544475197792], [0.0, 0.0, 0.06167038902640343, 0.06192544475197792], [0.0, 0.0, 0.06167038902640343, 0.06192544475197792]], [[0.0, 0.0, 0.0614020861685276, 0.06189192086458206], [0.0, 0.0, 0.0614020861685276, 0.06189192086458206], [0.0, 0.0, 0.0614020861685276, 0.06189192086458206], [0.0, 0.0, 0.0614020861685276, 0.06189192086458206], [0.0, 0.0, 0.0614020861685276, 0.06189192086458206]]]
# 1
UPDATE CUSTOMER   SET C_BALANCE = $1,        C_YTD_PAYMENT = $2,        C_PAYMENT_CNT = $3,        C_DATA = $4  WHERE C_W_ID = $5    AND C_D_ID = $6    AND C_ID = $7
* 1
{'Node Type': 'ModifyTable', 'Operation': 'Update', 'Parallel Aware': False, 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 690, 'Plans': [{'Node Type': 'Index Scan', 'Parent Relationship': 'Member', 'Parallel Aware': False, 'Scan Direction': 'Forward', 'Index Name': 'customer_pkey', 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 690, 'Index Cond': '((c_w_id = 15) AND (c_d_id = 6) AND (c_id = 723))'}]}
[{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 15) AND (c_d_id = 6) AND (c_id = 723))'}, {'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}]
! 3 0.06192544475197792
{'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}
! 2 0.06167038902640343
{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 15) AND (c_d_id = 6) AND (c_id = 723))'}
! 0 0.0
empty
! 1 0.0
empty
* 2
{'Node Type': 'ModifyTable', 'Operation': 'Update', 'Parallel Aware': False, 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 690, 'Plans': [{'Node Type': 'Index Scan', 'Parent Relationship': 'Member', 'Parallel Aware': False, 'Scan Direction': 'Forward', 'Index Name': 'customer_pkey', 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 690, 'Index Cond': '((c_w_id = 25) AND (c_d_id = 1) AND (c_id = 299))'}]}
[{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 25) AND (c_d_id = 1) AND (c_id = 299))'}, {'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}]
! 3 0.06192544475197792
{'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}
! 2 0.06167038902640343
{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 25) AND (c_d_id = 1) AND (c_id = 299))'}
! 0 0.0
empty
! 1 0.0
empty
* 3
{'Node Type': 'ModifyTable', 'Operation': 'Update', 'Parallel Aware': False, 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 690, 'Plans': [{'Node Type': 'Index Scan', 'Parent Relationship': 'Member', 'Parallel Aware': False, 'Scan Direction': 'Forward', 'Index Name': 'customer_pkey', 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 690, 'Index Cond': '((c_w_id = 25) AND (c_d_id = 2) AND (c_id = 238))'}]}
[{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 25) AND (c_d_id = 2) AND (c_id = 238))'}, {'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}]
! 3 0.06192544475197792
{'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}
! 2 0.06167038902640343
{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 25) AND (c_d_id = 2) AND (c_id = 238))'}
! 0 0.0
empty
! 1 0.0
empty
* 4
{'Node Type': 'ModifyTable', 'Operation': 'Update', 'Parallel Aware': False, 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 690, 'Plans': [{'Node Type': 'Index Scan', 'Parent Relationship': 'Member', 'Parallel Aware': False, 'Scan Direction': 'Forward', 'Index Name': 'customer_pkey', 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 690, 'Index Cond': '((c_w_id = 30) AND (c_d_id = 6) AND (c_id = 231))'}]}
[{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 30) AND (c_d_id = 6) AND (c_id = 231))'}, {'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}]
! 3 0.06192544475197792
{'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}
! 2 0.06167038902640343
{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 30) AND (c_d_id = 6) AND (c_id = 231))'}
! 0 0.0
empty
! 1 0.0
empty
* 5
[{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 30) AND (c_d_id = 6) AND (c_id = 231))'}, {'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}]
! 3 0.06192544475197792
{'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}
! 2 0.06167038902640343
{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 30) AND (c_d_id = 6) AND (c_id = 231))'}
! 0 0.0
empty
! 1 0.0
empty
# 2
UPDATE CUSTOMER   SET C_BALANCE = $1,        C_YTD_PAYMENT = $2,        C_PAYMENT_CNT = $3  WHERE C_W_ID = $4    AND C_D_ID = $5    AND C_ID = $6
* 1
{'Node Type': 'ModifyTable', 'Operation': 'Update', 'Parallel Aware': False, 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 577, 'Plans': [{'Node Type': 'Index Scan', 'Parent Relationship': 'Member', 'Parallel Aware': False, 'Scan Direction': 'Forward', 'Index Name': 'customer_pkey', 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 577, 'Index Cond': '((c_w_id = 23) AND (c_d_id = 8) AND (c_id = 1019))'}]}
[{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 23) AND (c_d_id = 8) AND (c_id = 1019))'}, {'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}]
! 3 0.06189192086458206
{'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}
! 2 0.0614020861685276
{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 23) AND (c_d_id = 8) AND (c_id = 1019))'}
! 0 0.0
empty
! 1 0.0
empty
* 2
{'Node Type': 'ModifyTable', 'Operation': 'Update', 'Parallel Aware': False, 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 577, 'Plans': [{'Node Type': 'Index Scan', 'Parent Relationship': 'Member', 'Parallel Aware': False, 'Scan Direction': 'Forward', 'Index Name': 'customer_pkey', 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 577, 'Index Cond': '((c_w_id = 25) AND (c_d_id = 8) AND (c_id = 618))'}]}
[{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 25) AND (c_d_id = 8) AND (c_id = 618))'}, {'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}]
! 3 0.06189192086458206
{'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}
! 2 0.0614020861685276
{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 25) AND (c_d_id = 8) AND (c_id = 618))'}
! 0 0.0
empty
! 1 0.0
empty
* 3
{'Node Type': 'ModifyTable', 'Operation': 'Update', 'Parallel Aware': False, 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 577, 'Plans': [{'Node Type': 'Index Scan', 'Parent Relationship': 'Member', 'Parallel Aware': False, 'Scan Direction': 'Forward', 'Index Name': 'customer_pkey', 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 577, 'Index Cond': '((c_w_id = 30) AND (c_d_id = 4) AND (c_id = 950))'}]}
[{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 30) AND (c_d_id = 4) AND (c_id = 950))'}, {'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}]
! 3 0.06189192086458206
{'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}
! 2 0.0614020861685276
{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 30) AND (c_d_id = 4) AND (c_id = 950))'}
! 0 0.0
empty
! 1 0.0
empty
* 4
{'Node Type': 'ModifyTable', 'Operation': 'Update', 'Parallel Aware': False, 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 577, 'Plans': [{'Node Type': 'Index Scan', 'Parent Relationship': 'Member', 'Parallel Aware': False, 'Scan Direction': 'Forward', 'Index Name': 'customer_pkey', 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 577, 'Index Cond': '((c_w_id = 45) AND (c_d_id = 6) AND (c_id = 747))'}]}
[{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 45) AND (c_d_id = 6) AND (c_id = 747))'}, {'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}]
! 3 0.06189192086458206
{'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}
! 2 0.0614020861685276
{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 45) AND (c_d_id = 6) AND (c_id = 747))'}
! 0 0.0
empty
! 1 0.0
empty
* 5
[{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 45) AND (c_d_id = 6) AND (c_id = 747))'}, {'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}]
! 3 0.06189192086458206
{'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}
! 2 0.0614020861685276
{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 45) AND (c_d_id = 6) AND (c_id = 747))'}
! 0 0.0
empty
! 1 0.0
empty
======Top3 metric
total cpu usage_sys: 0.49278379683875373
dsk/total_read: 0.3075720842956179
dsk/total_writ: 0.06311529778301933
igs:
[0, 8.729568481445312, 8.728508949279785, 8.72998046875, 8.730381965637207, 8.730558395385742, 8.730306625366211, 8.730016708374023, 8.729602813720703, 8.730570793151855, 8.729568481445312, 8.728944778442383, 8.730306625366211, 8.730351448059082, 8.730408668518066, 8.73022174835205, 8.730554580688477, 8.730500221252441, 8.729630470275879, 8.730493545532227, 8.729837417602539, 8.7308349609375, 8.730707168579102, 8.73037052154541, 8.730002403259277, 8.730857849121094, 8.730294227600098, 8.730802536010742, 8.730066299438477, 8.730790138244629, 8.730757713317871, 8.73069953918457, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Transformer(
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (2): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (3): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (4): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (5): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
    )
    (norm): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
  )
  (decoder): TransformerDecoder(
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (1): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (2): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (3): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (4): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (5): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
    )
    (norm): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
  )
)
Transformer(
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (2): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (3): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (4): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (5): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
    )
    (norm): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
  )
  (decoder): TransformerDecoder(
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (1): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (2): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (3): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (4): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (5): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
    )
    (norm): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
  )
)
[[[0.0, 0.0, 0.06167038902640343, 0.06192544475197792], [0.0, 0.0, 0.06167038902640343, 0.06192544475197792], [0.0, 0.0, 0.06167038902640343, 0.06192544475197792], [0.0, 0.0, 0.06167038902640343, 0.06192544475197792], [0.0, 0.0, 0.06167038902640343, 0.06192544475197792]], [[0.0, 0.0, 0.0614020861685276, 0.06189192086458206], [0.0, 0.0, 0.0614020861685276, 0.06189192086458206], [0.0, 0.0, 0.0614020861685276, 0.06189192086458206], [0.0, 0.0, 0.0614020861685276, 0.06189192086458206], [0.0, 0.0, 0.0614020861685276, 0.06189192086458206]]]
# 1
UPDATE CUSTOMER   SET C_BALANCE = $1,        C_YTD_PAYMENT = $2,        C_PAYMENT_CNT = $3,        C_DATA = $4  WHERE C_W_ID = $5    AND C_D_ID = $6    AND C_ID = $7
* 1
{'Node Type': 'ModifyTable', 'Operation': 'Update', 'Parallel Aware': False, 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 690, 'Plans': [{'Node Type': 'Index Scan', 'Parent Relationship': 'Member', 'Parallel Aware': False, 'Scan Direction': 'Forward', 'Index Name': 'customer_pkey', 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 690, 'Index Cond': '((c_w_id = 15) AND (c_d_id = 6) AND (c_id = 723))'}]}
[{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 15) AND (c_d_id = 6) AND (c_id = 723))'}, {'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}]
! 3 0.06192544475197792
{'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}
! 2 0.06167038902640343
{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 15) AND (c_d_id = 6) AND (c_id = 723))'}
! 0 0.0
empty
! 1 0.0
empty
* 2
{'Node Type': 'ModifyTable', 'Operation': 'Update', 'Parallel Aware': False, 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 690, 'Plans': [{'Node Type': 'Index Scan', 'Parent Relationship': 'Member', 'Parallel Aware': False, 'Scan Direction': 'Forward', 'Index Name': 'customer_pkey', 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 690, 'Index Cond': '((c_w_id = 25) AND (c_d_id = 1) AND (c_id = 299))'}]}
[{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 25) AND (c_d_id = 1) AND (c_id = 299))'}, {'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}]
! 3 0.06192544475197792
{'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}
! 2 0.06167038902640343
{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 25) AND (c_d_id = 1) AND (c_id = 299))'}
! 0 0.0
empty
! 1 0.0
empty
* 3
{'Node Type': 'ModifyTable', 'Operation': 'Update', 'Parallel Aware': False, 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 690, 'Plans': [{'Node Type': 'Index Scan', 'Parent Relationship': 'Member', 'Parallel Aware': False, 'Scan Direction': 'Forward', 'Index Name': 'customer_pkey', 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 690, 'Index Cond': '((c_w_id = 25) AND (c_d_id = 2) AND (c_id = 238))'}]}
[{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 25) AND (c_d_id = 2) AND (c_id = 238))'}, {'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}]
! 3 0.06192544475197792
{'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}
! 2 0.06167038902640343
{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 25) AND (c_d_id = 2) AND (c_id = 238))'}
! 0 0.0
empty
! 1 0.0
empty
* 4
{'Node Type': 'ModifyTable', 'Operation': 'Update', 'Parallel Aware': False, 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 690, 'Plans': [{'Node Type': 'Index Scan', 'Parent Relationship': 'Member', 'Parallel Aware': False, 'Scan Direction': 'Forward', 'Index Name': 'customer_pkey', 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 690, 'Index Cond': '((c_w_id = 30) AND (c_d_id = 6) AND (c_id = 231))'}]}
[{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 30) AND (c_d_id = 6) AND (c_id = 231))'}, {'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}]
! 3 0.06192544475197792
{'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}
! 2 0.06167038902640343
{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 30) AND (c_d_id = 6) AND (c_id = 231))'}
! 0 0.0
empty
! 1 0.0
empty
* 5
[{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 30) AND (c_d_id = 6) AND (c_id = 231))'}, {'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}]
! 3 0.06192544475197792
{'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}
! 2 0.06167038902640343
{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 30) AND (c_d_id = 6) AND (c_id = 231))'}
! 0 0.0
empty
! 1 0.0
empty
# 2
UPDATE CUSTOMER   SET C_BALANCE = $1,        C_YTD_PAYMENT = $2,        C_PAYMENT_CNT = $3  WHERE C_W_ID = $4    AND C_D_ID = $5    AND C_ID = $6
* 1
{'Node Type': 'ModifyTable', 'Operation': 'Update', 'Parallel Aware': False, 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 577, 'Plans': [{'Node Type': 'Index Scan', 'Parent Relationship': 'Member', 'Parallel Aware': False, 'Scan Direction': 'Forward', 'Index Name': 'customer_pkey', 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 577, 'Index Cond': '((c_w_id = 23) AND (c_d_id = 8) AND (c_id = 1019))'}]}
[{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 23) AND (c_d_id = 8) AND (c_id = 1019))'}, {'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}]
! 3 0.06189192086458206
{'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}
! 2 0.0614020861685276
{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 23) AND (c_d_id = 8) AND (c_id = 1019))'}
! 0 0.0
empty
! 1 0.0
empty
* 2
{'Node Type': 'ModifyTable', 'Operation': 'Update', 'Parallel Aware': False, 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 577, 'Plans': [{'Node Type': 'Index Scan', 'Parent Relationship': 'Member', 'Parallel Aware': False, 'Scan Direction': 'Forward', 'Index Name': 'customer_pkey', 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 577, 'Index Cond': '((c_w_id = 25) AND (c_d_id = 8) AND (c_id = 618))'}]}
[{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 25) AND (c_d_id = 8) AND (c_id = 618))'}, {'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}]
! 3 0.06189192086458206
{'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}
! 2 0.0614020861685276
{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 25) AND (c_d_id = 8) AND (c_id = 618))'}
! 0 0.0
empty
! 1 0.0
empty
* 3
{'Node Type': 'ModifyTable', 'Operation': 'Update', 'Parallel Aware': False, 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 577, 'Plans': [{'Node Type': 'Index Scan', 'Parent Relationship': 'Member', 'Parallel Aware': False, 'Scan Direction': 'Forward', 'Index Name': 'customer_pkey', 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 577, 'Index Cond': '((c_w_id = 30) AND (c_d_id = 4) AND (c_id = 950))'}]}
[{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 30) AND (c_d_id = 4) AND (c_id = 950))'}, {'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}]
! 3 0.06189192086458206
{'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}
! 2 0.0614020861685276
{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 30) AND (c_d_id = 4) AND (c_id = 950))'}
! 0 0.0
empty
! 1 0.0
empty
* 4
{'Node Type': 'ModifyTable', 'Operation': 'Update', 'Parallel Aware': False, 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 577, 'Plans': [{'Node Type': 'Index Scan', 'Parent Relationship': 'Member', 'Parallel Aware': False, 'Scan Direction': 'Forward', 'Index Name': 'customer_pkey', 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 577, 'Index Cond': '((c_w_id = 45) AND (c_d_id = 6) AND (c_id = 747))'}]}
[{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 45) AND (c_d_id = 6) AND (c_id = 747))'}, {'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}]
! 3 0.06189192086458206
{'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}
! 2 0.0614020861685276
{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 45) AND (c_d_id = 6) AND (c_id = 747))'}
! 0 0.0
empty
! 1 0.0
empty
* 5
[{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 45) AND (c_d_id = 6) AND (c_id = 747))'}, {'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}]
! 3 0.06189192086458206
{'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}
! 2 0.0614020861685276
{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 45) AND (c_d_id = 6) AND (c_id = 747))'}
! 0 0.0
empty
! 1 0.0
empty
======Top3 metric
dsk/total_read: 0.4663741155291149
memory usage_cach: 0.11074839323491198
memory usage_free: 0.08556887636418825
igs:
[0, 5.264186382293701, 5.263547897338867, 5.264434814453125, 5.26467752456665, 5.26478385925293, 5.264631748199463, 5.264456748962402, 5.26420783996582, 5.264791011810303, 5.264186382293701, 5.263810634613037, 5.264631748199463, 5.2646589279174805, 5.264693260192871, 5.264580249786377, 5.264781475067139, 5.264749050140381, 5.264224052429199, 5.264744281768799, 5.26434850692749, 5.264950275421143, 5.264873504638672, 5.264670372009277, 5.264448165893555, 5.2649641036987305, 5.264624118804932, 5.2649312019348145, 5.264486789703369, 5.264923572540283, 0, 5.26486873626709, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Transformer(
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (2): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (3): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (4): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (5): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
    )
    (norm): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
  )
  (decoder): TransformerDecoder(
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (1): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (2): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (3): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (4): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (5): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
    )
    (norm): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
  )
)
Transformer(
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (2): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (3): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (4): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (5): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
    )
    (norm): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
  )
  (decoder): TransformerDecoder(
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (1): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (2): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (3): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (4): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (5): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
    )
    (norm): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
  )
)
[[[0.0, 0.0, 0.06167038902640343, 0.06192544475197792], [0.0, 0.0, 0.06167038902640343, 0.06192544475197792], [0.0, 0.0, 0.06167038902640343, 0.06192544475197792], [0.0, 0.0, 0.06167038902640343, 0.06192544475197792], [0.0, 0.0, 0.06167038902640343, 0.06192544475197792]], [[0.0, 0.0, 0.0614020861685276, 0.06189192086458206], [0.0, 0.0, 0.0614020861685276, 0.06189192086458206], [0.0, 0.0, 0.0614020861685276, 0.06189192086458206], [0.0, 0.0, 0.0614020861685276, 0.06189192086458206], [0.0, 0.0, 0.0614020861685276, 0.06189192086458206]]]
# 1
UPDATE CUSTOMER   SET C_BALANCE = $1,        C_YTD_PAYMENT = $2,        C_PAYMENT_CNT = $3,        C_DATA = $4  WHERE C_W_ID = $5    AND C_D_ID = $6    AND C_ID = $7
* 1
{'Node Type': 'ModifyTable', 'Operation': 'Update', 'Parallel Aware': False, 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 690, 'Plans': [{'Node Type': 'Index Scan', 'Parent Relationship': 'Member', 'Parallel Aware': False, 'Scan Direction': 'Forward', 'Index Name': 'customer_pkey', 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 690, 'Index Cond': '((c_w_id = 15) AND (c_d_id = 6) AND (c_id = 723))'}]}
[{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 15) AND (c_d_id = 6) AND (c_id = 723))'}, {'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}]
! 3 0.06192544475197792
{'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}
! 2 0.06167038902640343
{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 15) AND (c_d_id = 6) AND (c_id = 723))'}
! 0 0.0
empty
! 1 0.0
empty
* 2
{'Node Type': 'ModifyTable', 'Operation': 'Update', 'Parallel Aware': False, 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 690, 'Plans': [{'Node Type': 'Index Scan', 'Parent Relationship': 'Member', 'Parallel Aware': False, 'Scan Direction': 'Forward', 'Index Name': 'customer_pkey', 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 690, 'Index Cond': '((c_w_id = 25) AND (c_d_id = 1) AND (c_id = 299))'}]}
[{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 25) AND (c_d_id = 1) AND (c_id = 299))'}, {'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}]
! 3 0.06192544475197792
{'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}
! 2 0.06167038902640343
{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 25) AND (c_d_id = 1) AND (c_id = 299))'}
! 0 0.0
empty
! 1 0.0
empty
* 3
{'Node Type': 'ModifyTable', 'Operation': 'Update', 'Parallel Aware': False, 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 690, 'Plans': [{'Node Type': 'Index Scan', 'Parent Relationship': 'Member', 'Parallel Aware': False, 'Scan Direction': 'Forward', 'Index Name': 'customer_pkey', 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 690, 'Index Cond': '((c_w_id = 25) AND (c_d_id = 2) AND (c_id = 238))'}]}
[{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 25) AND (c_d_id = 2) AND (c_id = 238))'}, {'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}]
! 3 0.06192544475197792
{'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}
! 2 0.06167038902640343
{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 25) AND (c_d_id = 2) AND (c_id = 238))'}
! 0 0.0
empty
! 1 0.0
empty
* 4
{'Node Type': 'ModifyTable', 'Operation': 'Update', 'Parallel Aware': False, 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 690, 'Plans': [{'Node Type': 'Index Scan', 'Parent Relationship': 'Member', 'Parallel Aware': False, 'Scan Direction': 'Forward', 'Index Name': 'customer_pkey', 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 690, 'Index Cond': '((c_w_id = 30) AND (c_d_id = 6) AND (c_id = 231))'}]}
[{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 30) AND (c_d_id = 6) AND (c_id = 231))'}, {'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}]
! 3 0.06192544475197792
{'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}
! 2 0.06167038902640343
{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 30) AND (c_d_id = 6) AND (c_id = 231))'}
! 0 0.0
empty
! 1 0.0
empty
* 5
[{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 30) AND (c_d_id = 6) AND (c_id = 231))'}, {'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}]
! 3 0.06192544475197792
{'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}
! 2 0.06167038902640343
{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 30) AND (c_d_id = 6) AND (c_id = 231))'}
! 0 0.0
empty
! 1 0.0
empty
# 2
UPDATE CUSTOMER   SET C_BALANCE = $1,        C_YTD_PAYMENT = $2,        C_PAYMENT_CNT = $3  WHERE C_W_ID = $4    AND C_D_ID = $5    AND C_ID = $6
* 1
{'Node Type': 'ModifyTable', 'Operation': 'Update', 'Parallel Aware': False, 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 577, 'Plans': [{'Node Type': 'Index Scan', 'Parent Relationship': 'Member', 'Parallel Aware': False, 'Scan Direction': 'Forward', 'Index Name': 'customer_pkey', 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 577, 'Index Cond': '((c_w_id = 23) AND (c_d_id = 8) AND (c_id = 1019))'}]}
[{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 23) AND (c_d_id = 8) AND (c_id = 1019))'}, {'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}]
! 3 0.06189192086458206
{'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}
! 2 0.0614020861685276
{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 23) AND (c_d_id = 8) AND (c_id = 1019))'}
! 0 0.0
empty
! 1 0.0
empty
* 2
{'Node Type': 'ModifyTable', 'Operation': 'Update', 'Parallel Aware': False, 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 577, 'Plans': [{'Node Type': 'Index Scan', 'Parent Relationship': 'Member', 'Parallel Aware': False, 'Scan Direction': 'Forward', 'Index Name': 'customer_pkey', 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 577, 'Index Cond': '((c_w_id = 25) AND (c_d_id = 8) AND (c_id = 618))'}]}
[{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 25) AND (c_d_id = 8) AND (c_id = 618))'}, {'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}]
! 3 0.06189192086458206
{'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}
! 2 0.0614020861685276
{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 25) AND (c_d_id = 8) AND (c_id = 618))'}
! 0 0.0
empty
! 1 0.0
empty
* 3
{'Node Type': 'ModifyTable', 'Operation': 'Update', 'Parallel Aware': False, 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 577, 'Plans': [{'Node Type': 'Index Scan', 'Parent Relationship': 'Member', 'Parallel Aware': False, 'Scan Direction': 'Forward', 'Index Name': 'customer_pkey', 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 577, 'Index Cond': '((c_w_id = 30) AND (c_d_id = 4) AND (c_id = 950))'}]}
[{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 30) AND (c_d_id = 4) AND (c_id = 950))'}, {'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}]
! 3 0.06189192086458206
{'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}
! 2 0.0614020861685276
{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 30) AND (c_d_id = 4) AND (c_id = 950))'}
! 0 0.0
empty
! 1 0.0
empty
* 4
{'Node Type': 'ModifyTable', 'Operation': 'Update', 'Parallel Aware': False, 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 577, 'Plans': [{'Node Type': 'Index Scan', 'Parent Relationship': 'Member', 'Parallel Aware': False, 'Scan Direction': 'Forward', 'Index Name': 'customer_pkey', 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 577, 'Index Cond': '((c_w_id = 45) AND (c_d_id = 6) AND (c_id = 747))'}]}
[{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 45) AND (c_d_id = 6) AND (c_id = 747))'}, {'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}]
! 3 0.06189192086458206
{'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}
! 2 0.0614020861685276
{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 45) AND (c_d_id = 6) AND (c_id = 747))'}
! 0 0.0
empty
! 1 0.0
empty
* 5
[{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 45) AND (c_d_id = 6) AND (c_id = 747))'}, {'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}]
! 3 0.06189192086458206
{'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}
! 2 0.0614020861685276
{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 45) AND (c_d_id = 6) AND (c_id = 747))'}
! 0 0.0
empty
! 1 0.0
empty
======Top3 metric
dsk/total_read: 0.5173884851552233
memory usage_cach: 0.12385855715827762
memory usage_used: 0.09073222827596716
igs:
[0, 11.590465545654297, 11.589059829711914, 11.591012001037598, 11.591546058654785, 11.591780662536621, 11.591445922851562, 11.591060638427734, 11.5905122756958, 11.591796875, 11.590465545654297, 11.589637756347656, 11.591445922851562, 11.591506004333496, 11.591581344604492, 11.591333389282227, 11.591775894165039, 11.591703414916992, 11.590548515319824, 11.591693878173828, 11.59082317352295, 11.592146873474121, 11.591978073120117, 11.591530799865723, 11.591041564941406, 11.592177391052246, 11.591429710388184, 11.5921049118042, 11.591126441955566, 11.592087745666504, 11.592044830322266, 11.591967582702637, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Transformer(
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (2): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (3): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (4): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (5): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
    )
    (norm): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
  )
  (decoder): TransformerDecoder(
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (1): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (2): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (3): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (4): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (5): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
    )
    (norm): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
  )
)
Transformer(
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (2): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (3): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (4): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (5): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
    )
    (norm): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
  )
  (decoder): TransformerDecoder(
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (1): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (2): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (3): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (4): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (5): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
    )
    (norm): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
  )
)
[[[0.0, 0.0, 0.06167038902640343, 0.06192544475197792], [0.0, 0.0, 0.06167038902640343, 0.06192544475197792], [0.0, 0.0, 0.06167038902640343, 0.06192544475197792], [0.0, 0.0, 0.06167038902640343, 0.06192544475197792], [0.0, 0.0, 0.06167038902640343, 0.06192544475197792]], [[0.0, 0.0, 0.0614020861685276, 0.06189192086458206], [0.0, 0.0, 0.0614020861685276, 0.06189192086458206], [0.0, 0.0, 0.0614020861685276, 0.06189192086458206], [0.0, 0.0, 0.0614020861685276, 0.06189192086458206], [0.0, 0.0, 0.0614020861685276, 0.06189192086458206]]]
# 1
UPDATE CUSTOMER   SET C_BALANCE = $1,        C_YTD_PAYMENT = $2,        C_PAYMENT_CNT = $3,        C_DATA = $4  WHERE C_W_ID = $5    AND C_D_ID = $6    AND C_ID = $7
* 1
{'Node Type': 'ModifyTable', 'Operation': 'Update', 'Parallel Aware': False, 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 690, 'Plans': [{'Node Type': 'Index Scan', 'Parent Relationship': 'Member', 'Parallel Aware': False, 'Scan Direction': 'Forward', 'Index Name': 'customer_pkey', 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 690, 'Index Cond': '((c_w_id = 15) AND (c_d_id = 6) AND (c_id = 723))'}]}
[{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 15) AND (c_d_id = 6) AND (c_id = 723))'}, {'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}]
! 3 0.06192544475197792
{'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}
! 2 0.06167038902640343
{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 15) AND (c_d_id = 6) AND (c_id = 723))'}
! 0 0.0
empty
! 1 0.0
empty
* 2
{'Node Type': 'ModifyTable', 'Operation': 'Update', 'Parallel Aware': False, 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 690, 'Plans': [{'Node Type': 'Index Scan', 'Parent Relationship': 'Member', 'Parallel Aware': False, 'Scan Direction': 'Forward', 'Index Name': 'customer_pkey', 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 690, 'Index Cond': '((c_w_id = 25) AND (c_d_id = 1) AND (c_id = 299))'}]}
[{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 25) AND (c_d_id = 1) AND (c_id = 299))'}, {'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}]
! 3 0.06192544475197792
{'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}
! 2 0.06167038902640343
{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 25) AND (c_d_id = 1) AND (c_id = 299))'}
! 0 0.0
empty
! 1 0.0
empty
* 3
{'Node Type': 'ModifyTable', 'Operation': 'Update', 'Parallel Aware': False, 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 690, 'Plans': [{'Node Type': 'Index Scan', 'Parent Relationship': 'Member', 'Parallel Aware': False, 'Scan Direction': 'Forward', 'Index Name': 'customer_pkey', 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 690, 'Index Cond': '((c_w_id = 25) AND (c_d_id = 2) AND (c_id = 238))'}]}
[{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 25) AND (c_d_id = 2) AND (c_id = 238))'}, {'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}]
! 3 0.06192544475197792
{'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}
! 2 0.06167038902640343
{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 25) AND (c_d_id = 2) AND (c_id = 238))'}
! 0 0.0
empty
! 1 0.0
empty
* 4
{'Node Type': 'ModifyTable', 'Operation': 'Update', 'Parallel Aware': False, 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 690, 'Plans': [{'Node Type': 'Index Scan', 'Parent Relationship': 'Member', 'Parallel Aware': False, 'Scan Direction': 'Forward', 'Index Name': 'customer_pkey', 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 690, 'Index Cond': '((c_w_id = 30) AND (c_d_id = 6) AND (c_id = 231))'}]}
[{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 30) AND (c_d_id = 6) AND (c_id = 231))'}, {'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}]
! 3 0.06192544475197792
{'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}
! 2 0.06167038902640343
{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 30) AND (c_d_id = 6) AND (c_id = 231))'}
! 0 0.0
empty
! 1 0.0
empty
* 5
[{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 30) AND (c_d_id = 6) AND (c_id = 231))'}, {'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}]
! 3 0.06192544475197792
{'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}
! 2 0.06167038902640343
{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 30) AND (c_d_id = 6) AND (c_id = 231))'}
! 0 0.0
empty
! 1 0.0
empty
# 2
UPDATE CUSTOMER   SET C_BALANCE = $1,        C_YTD_PAYMENT = $2,        C_PAYMENT_CNT = $3  WHERE C_W_ID = $4    AND C_D_ID = $5    AND C_ID = $6
* 1
{'Node Type': 'ModifyTable', 'Operation': 'Update', 'Parallel Aware': False, 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 577, 'Plans': [{'Node Type': 'Index Scan', 'Parent Relationship': 'Member', 'Parallel Aware': False, 'Scan Direction': 'Forward', 'Index Name': 'customer_pkey', 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 577, 'Index Cond': '((c_w_id = 23) AND (c_d_id = 8) AND (c_id = 1019))'}]}
[{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 23) AND (c_d_id = 8) AND (c_id = 1019))'}, {'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}]
! 3 0.06189192086458206
{'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}
! 2 0.0614020861685276
{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 23) AND (c_d_id = 8) AND (c_id = 1019))'}
! 0 0.0
empty
! 1 0.0
empty
* 2
{'Node Type': 'ModifyTable', 'Operation': 'Update', 'Parallel Aware': False, 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 577, 'Plans': [{'Node Type': 'Index Scan', 'Parent Relationship': 'Member', 'Parallel Aware': False, 'Scan Direction': 'Forward', 'Index Name': 'customer_pkey', 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 577, 'Index Cond': '((c_w_id = 25) AND (c_d_id = 8) AND (c_id = 618))'}]}
[{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 25) AND (c_d_id = 8) AND (c_id = 618))'}, {'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}]
! 3 0.06189192086458206
{'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}
! 2 0.0614020861685276
{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 25) AND (c_d_id = 8) AND (c_id = 618))'}
! 0 0.0
empty
! 1 0.0
empty
* 3
{'Node Type': 'ModifyTable', 'Operation': 'Update', 'Parallel Aware': False, 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 577, 'Plans': [{'Node Type': 'Index Scan', 'Parent Relationship': 'Member', 'Parallel Aware': False, 'Scan Direction': 'Forward', 'Index Name': 'customer_pkey', 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 577, 'Index Cond': '((c_w_id = 30) AND (c_d_id = 4) AND (c_id = 950))'}]}
[{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 30) AND (c_d_id = 4) AND (c_id = 950))'}, {'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}]
! 3 0.06189192086458206
{'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}
! 2 0.0614020861685276
{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 30) AND (c_d_id = 4) AND (c_id = 950))'}
! 0 0.0
empty
! 1 0.0
empty
* 4
{'Node Type': 'ModifyTable', 'Operation': 'Update', 'Parallel Aware': False, 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 577, 'Plans': [{'Node Type': 'Index Scan', 'Parent Relationship': 'Member', 'Parallel Aware': False, 'Scan Direction': 'Forward', 'Index Name': 'customer_pkey', 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 577, 'Index Cond': '((c_w_id = 45) AND (c_d_id = 6) AND (c_id = 747))'}]}
[{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 45) AND (c_d_id = 6) AND (c_id = 747))'}, {'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}]
! 3 0.06189192086458206
{'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}
! 2 0.0614020861685276
{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 45) AND (c_d_id = 6) AND (c_id = 747))'}
! 0 0.0
empty
! 1 0.0
empty
* 5
[{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 45) AND (c_d_id = 6) AND (c_id = 747))'}, {'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}]
! 3 0.06189192086458206
{'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}
! 2 0.0614020861685276
{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 45) AND (c_d_id = 6) AND (c_id = 747))'}
! 0 0.0
empty
! 1 0.0
empty
======Top3 metric
dsk/total_read: 0.388044083189367
total cpu usage_sys: 0.26505266044205106
total cpu usage_usr: 0.1501285230437343
igs:
[0, 60.87657165527344, 60.86918640136719, 60.87944030761719, 60.88224792480469, 60.88347625732422, 60.881717681884766, 60.879695892333984, 60.87681198120117, 60.88356399536133, 60.87657165527344, 60.872222900390625, 60.881717681884766, 60.88203430175781, 60.88243103027344, 60.881126403808594, 60.88344955444336, 60.88307189941406, 60.87700271606445, 60.88302230834961, 60.87844467163086, 60.88540267944336, 60.88451385498047, 60.882164001464844, 60.87959671020508, 60.885562896728516, 60.881629943847656, 60.88517761230469, 60.88003921508789, 60.88508987426758, 60.884864807128906, 60.884456634521484, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Transformer(
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (2): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (3): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (4): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (5): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
    )
    (norm): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
  )
  (decoder): TransformerDecoder(
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (1): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (2): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (3): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (4): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (5): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
    )
    (norm): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
  )
)
Transformer(
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (2): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (3): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (4): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (5): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
    )
    (norm): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
  )
  (decoder): TransformerDecoder(
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (1): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (2): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (3): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (4): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (5): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
    )
    (norm): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
  )
)
[[[0.0, 0.0, 0.06167038902640343, 0.06192544475197792], [0.0, 0.0, 0.06167038902640343, 0.06192544475197792], [0.0, 0.0, 0.06167038902640343, 0.06192544475197792], [0.0, 0.0, 0.06167038902640343, 0.06192544475197792], [0.0, 0.0, 0.06167038902640343, 0.06192544475197792]], [[0.0, 0.0, 0.0614020861685276, 0.06189192086458206], [0.0, 0.0, 0.0614020861685276, 0.06189192086458206], [0.0, 0.0, 0.0614020861685276, 0.06189192086458206], [0.0, 0.0, 0.0614020861685276, 0.06189192086458206], [0.0, 0.0, 0.0614020861685276, 0.06189192086458206]]]
# 1
UPDATE CUSTOMER   SET C_BALANCE = $1,        C_YTD_PAYMENT = $2,        C_PAYMENT_CNT = $3,        C_DATA = $4  WHERE C_W_ID = $5    AND C_D_ID = $6    AND C_ID = $7
* 1
{'Node Type': 'ModifyTable', 'Operation': 'Update', 'Parallel Aware': False, 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 690, 'Plans': [{'Node Type': 'Index Scan', 'Parent Relationship': 'Member', 'Parallel Aware': False, 'Scan Direction': 'Forward', 'Index Name': 'customer_pkey', 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 690, 'Index Cond': '((c_w_id = 15) AND (c_d_id = 6) AND (c_id = 723))'}]}
[{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 15) AND (c_d_id = 6) AND (c_id = 723))'}, {'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}]
! 3 0.06192544475197792
{'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}
! 2 0.06167038902640343
{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 15) AND (c_d_id = 6) AND (c_id = 723))'}
! 0 0.0
empty
! 1 0.0
empty
* 2
{'Node Type': 'ModifyTable', 'Operation': 'Update', 'Parallel Aware': False, 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 690, 'Plans': [{'Node Type': 'Index Scan', 'Parent Relationship': 'Member', 'Parallel Aware': False, 'Scan Direction': 'Forward', 'Index Name': 'customer_pkey', 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 690, 'Index Cond': '((c_w_id = 25) AND (c_d_id = 1) AND (c_id = 299))'}]}
[{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 25) AND (c_d_id = 1) AND (c_id = 299))'}, {'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}]
! 3 0.06192544475197792
{'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}
! 2 0.06167038902640343
{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 25) AND (c_d_id = 1) AND (c_id = 299))'}
! 0 0.0
empty
! 1 0.0
empty
* 3
{'Node Type': 'ModifyTable', 'Operation': 'Update', 'Parallel Aware': False, 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 690, 'Plans': [{'Node Type': 'Index Scan', 'Parent Relationship': 'Member', 'Parallel Aware': False, 'Scan Direction': 'Forward', 'Index Name': 'customer_pkey', 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 690, 'Index Cond': '((c_w_id = 25) AND (c_d_id = 2) AND (c_id = 238))'}]}
[{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 25) AND (c_d_id = 2) AND (c_id = 238))'}, {'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}]
! 3 0.06192544475197792
{'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}
! 2 0.06167038902640343
{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 25) AND (c_d_id = 2) AND (c_id = 238))'}
! 0 0.0
empty
! 1 0.0
empty
* 4
{'Node Type': 'ModifyTable', 'Operation': 'Update', 'Parallel Aware': False, 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 690, 'Plans': [{'Node Type': 'Index Scan', 'Parent Relationship': 'Member', 'Parallel Aware': False, 'Scan Direction': 'Forward', 'Index Name': 'customer_pkey', 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 690, 'Index Cond': '((c_w_id = 30) AND (c_d_id = 6) AND (c_id = 231))'}]}
[{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 30) AND (c_d_id = 6) AND (c_id = 231))'}, {'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}]
! 3 0.06192544475197792
{'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}
! 2 0.06167038902640343
{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 30) AND (c_d_id = 6) AND (c_id = 231))'}
! 0 0.0
empty
! 1 0.0
empty
* 5
[{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 30) AND (c_d_id = 6) AND (c_id = 231))'}, {'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}]
! 3 0.06192544475197792
{'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}
! 2 0.06167038902640343
{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 30) AND (c_d_id = 6) AND (c_id = 231))'}
! 0 0.0
empty
! 1 0.0
empty
# 2
UPDATE CUSTOMER   SET C_BALANCE = $1,        C_YTD_PAYMENT = $2,        C_PAYMENT_CNT = $3  WHERE C_W_ID = $4    AND C_D_ID = $5    AND C_ID = $6
* 1
{'Node Type': 'ModifyTable', 'Operation': 'Update', 'Parallel Aware': False, 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 577, 'Plans': [{'Node Type': 'Index Scan', 'Parent Relationship': 'Member', 'Parallel Aware': False, 'Scan Direction': 'Forward', 'Index Name': 'customer_pkey', 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 577, 'Index Cond': '((c_w_id = 23) AND (c_d_id = 8) AND (c_id = 1019))'}]}
[{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 23) AND (c_d_id = 8) AND (c_id = 1019))'}, {'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}]
! 3 0.06189192086458206
{'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}
! 2 0.0614020861685276
{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 23) AND (c_d_id = 8) AND (c_id = 1019))'}
! 0 0.0
empty
! 1 0.0
empty
* 2
{'Node Type': 'ModifyTable', 'Operation': 'Update', 'Parallel Aware': False, 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 577, 'Plans': [{'Node Type': 'Index Scan', 'Parent Relationship': 'Member', 'Parallel Aware': False, 'Scan Direction': 'Forward', 'Index Name': 'customer_pkey', 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 577, 'Index Cond': '((c_w_id = 25) AND (c_d_id = 8) AND (c_id = 618))'}]}
[{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 25) AND (c_d_id = 8) AND (c_id = 618))'}, {'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}]
! 3 0.06189192086458206
{'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}
! 2 0.0614020861685276
{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 25) AND (c_d_id = 8) AND (c_id = 618))'}
! 0 0.0
empty
! 1 0.0
empty
* 3
{'Node Type': 'ModifyTable', 'Operation': 'Update', 'Parallel Aware': False, 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 577, 'Plans': [{'Node Type': 'Index Scan', 'Parent Relationship': 'Member', 'Parallel Aware': False, 'Scan Direction': 'Forward', 'Index Name': 'customer_pkey', 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 577, 'Index Cond': '((c_w_id = 30) AND (c_d_id = 4) AND (c_id = 950))'}]}
[{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 30) AND (c_d_id = 4) AND (c_id = 950))'}, {'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}]
! 3 0.06189192086458206
{'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}
! 2 0.0614020861685276
{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 30) AND (c_d_id = 4) AND (c_id = 950))'}
! 0 0.0
empty
! 1 0.0
empty
* 4
{'Node Type': 'ModifyTable', 'Operation': 'Update', 'Parallel Aware': False, 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 577, 'Plans': [{'Node Type': 'Index Scan', 'Parent Relationship': 'Member', 'Parallel Aware': False, 'Scan Direction': 'Forward', 'Index Name': 'customer_pkey', 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 577, 'Index Cond': '((c_w_id = 45) AND (c_d_id = 6) AND (c_id = 747))'}]}
[{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 45) AND (c_d_id = 6) AND (c_id = 747))'}, {'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}]
! 3 0.06189192086458206
{'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}
! 2 0.0614020861685276
{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 45) AND (c_d_id = 6) AND (c_id = 747))'}
! 0 0.0
empty
! 1 0.0
empty
* 5
[{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 45) AND (c_d_id = 6) AND (c_id = 747))'}, {'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}]
! 3 0.06189192086458206
{'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}
! 2 0.0614020861685276
{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 45) AND (c_d_id = 6) AND (c_id = 747))'}
! 0 0.0
empty
! 1 0.0
empty
======Top3 metric
total cpu usage_sys: 0.4409012495696534
dsk/total_read: 0.2743705419501549
total cpu usage_idl: 0.1457583043394314
igs:
[0, 88.0679931640625, 88.05730438232422, 88.0721435546875, 88.07620239257812, 88.0779800415039, 88.075439453125, 88.072509765625, 88.06834411621094, 88.07810974121094, 88.0679931640625, 88.06169891357422, 88.075439453125, 88.07589721679688, 88.07646942138672, 88.07457733154297, 88.07794189453125, 88.07740020751953, 88.06861877441406, 88.07732391357422, 88.0707015991211, 88.08076477050781, 0, 88.07608032226562, 88.07237243652344, 0, 88.07530975341797, 88.0804443359375, 88.07301330566406, 88.08031463623047, 88.07998657226562, 88.07939910888672, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Transformer(
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (2): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (3): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (4): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (5): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
    )
    (norm): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
  )
  (decoder): TransformerDecoder(
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (1): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (2): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (3): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (4): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (5): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
    )
    (norm): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
  )
)
Transformer(
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (2): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (3): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (4): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (5): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
    )
    (norm): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
  )
  (decoder): TransformerDecoder(
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (1): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (2): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (3): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (4): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (5): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
    )
    (norm): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
  )
)
[[[0.0, 0.0, 0.0614020861685276, 0.06189192086458206], [0.0, 0.0, 0.0614020861685276, 0.06189192086458206], [0.0, 0.0, 0.0614020861685276, 0.06189192086458206], [0.0, 0.0, 0.0614020861685276, 0.06189192086458206], [0.0, 0.0, 0.0614020861685276, 0.06189192086458206]], [[0.0, 0.0, 0.06139722093939781, 0.0618903823196888], [0.0, 0.0, 0.06139722093939781, 0.0618903823196888], [0.0, 0.0, 0.059026505798101425, 0.059126053005456924], [0.0, 0.0, 0.06139722093939781, 0.0618903823196888], [0.0, 0.0, 0.06139722093939781, 0.0618903823196888]]]
# 1
UPDATE CUSTOMER   SET C_BALANCE = $1,        C_YTD_PAYMENT = $2,        C_PAYMENT_CNT = $3  WHERE C_W_ID = $4    AND C_D_ID = $5    AND C_ID = $6
* 1
{'Node Type': 'ModifyTable', 'Operation': 'Update', 'Parallel Aware': False, 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 577, 'Plans': [{'Node Type': 'Index Scan', 'Parent Relationship': 'Member', 'Parallel Aware': False, 'Scan Direction': 'Forward', 'Index Name': 'customer_pkey', 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 577, 'Index Cond': '((c_w_id = 23) AND (c_d_id = 8) AND (c_id = 1019))'}]}
[{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 23) AND (c_d_id = 8) AND (c_id = 1019))'}, {'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}]
! 3 0.06189192086458206
{'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}
! 2 0.0614020861685276
{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 23) AND (c_d_id = 8) AND (c_id = 1019))'}
! 0 0.0
empty
! 1 0.0
empty
* 2
{'Node Type': 'ModifyTable', 'Operation': 'Update', 'Parallel Aware': False, 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 577, 'Plans': [{'Node Type': 'Index Scan', 'Parent Relationship': 'Member', 'Parallel Aware': False, 'Scan Direction': 'Forward', 'Index Name': 'customer_pkey', 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 577, 'Index Cond': '((c_w_id = 25) AND (c_d_id = 8) AND (c_id = 618))'}]}
[{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 25) AND (c_d_id = 8) AND (c_id = 618))'}, {'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}]
! 3 0.06189192086458206
{'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}
! 2 0.0614020861685276
{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 25) AND (c_d_id = 8) AND (c_id = 618))'}
! 0 0.0
empty
! 1 0.0
empty
* 3
{'Node Type': 'ModifyTable', 'Operation': 'Update', 'Parallel Aware': False, 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 577, 'Plans': [{'Node Type': 'Index Scan', 'Parent Relationship': 'Member', 'Parallel Aware': False, 'Scan Direction': 'Forward', 'Index Name': 'customer_pkey', 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 577, 'Index Cond': '((c_w_id = 30) AND (c_d_id = 4) AND (c_id = 950))'}]}
[{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 30) AND (c_d_id = 4) AND (c_id = 950))'}, {'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}]
! 3 0.06189192086458206
{'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}
! 2 0.0614020861685276
{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 30) AND (c_d_id = 4) AND (c_id = 950))'}
! 0 0.0
empty
! 1 0.0
empty
* 4
{'Node Type': 'ModifyTable', 'Operation': 'Update', 'Parallel Aware': False, 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 577, 'Plans': [{'Node Type': 'Index Scan', 'Parent Relationship': 'Member', 'Parallel Aware': False, 'Scan Direction': 'Forward', 'Index Name': 'customer_pkey', 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 577, 'Index Cond': '((c_w_id = 45) AND (c_d_id = 6) AND (c_id = 747))'}]}
[{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 45) AND (c_d_id = 6) AND (c_id = 747))'}, {'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}]
! 3 0.06189192086458206
{'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}
! 2 0.0614020861685276
{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 45) AND (c_d_id = 6) AND (c_id = 747))'}
! 0 0.0
empty
! 1 0.0
empty
* 5
[{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 45) AND (c_d_id = 6) AND (c_id = 747))'}, {'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}]
! 3 0.06189192086458206
{'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}
! 2 0.0614020861685276
{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 45) AND (c_d_id = 6) AND (c_id = 747))'}
! 0 0.0
empty
! 1 0.0
empty
# 2
UPDATE CUSTOMER   SET C_BALANCE = C_BALANCE + $1,       C_DELIVERY_CNT = C_DELIVERY_CNT + 1  WHERE C_W_ID = $2    AND C_D_ID = $3    AND C_ID = $4
* 1
{'Node Type': 'ModifyTable', 'Operation': 'Update', 'Parallel Aware': False, 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.46, 'Plan Rows': 1, 'Plan Width': 577, 'Plans': [{'Node Type': 'Index Scan', 'Parent Relationship': 'Member', 'Parallel Aware': False, 'Scan Direction': 'Forward', 'Index Name': 'customer_pkey', 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.46, 'Plan Rows': 1, 'Plan Width': 577, 'Index Cond': '((c_w_id = 40) AND (c_d_id = 1) AND (c_id = 1076))'}]}
[{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.46, 'Index Cond': '((c_w_id = 40) AND (c_d_id = 1) AND (c_id = 1076))'}, {'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.46}]
! 3 0.0618903823196888
{'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.46}
! 2 0.06139722093939781
{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.46, 'Index Cond': '((c_w_id = 40) AND (c_d_id = 1) AND (c_id = 1076))'}
! 0 0.0
empty
! 1 0.0
empty
* 2
{'Node Type': 'ModifyTable', 'Operation': 'Update', 'Parallel Aware': False, 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.46, 'Plan Rows': 1, 'Plan Width': 577, 'Plans': [{'Node Type': 'Index Scan', 'Parent Relationship': 'Member', 'Parallel Aware': False, 'Scan Direction': 'Forward', 'Index Name': 'customer_pkey', 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.46, 'Plan Rows': 1, 'Plan Width': 577, 'Index Cond': '((c_w_id = 8) AND (c_d_id = 7) AND (c_id = 2429))'}]}
[{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.46, 'Index Cond': '((c_w_id = 8) AND (c_d_id = 7) AND (c_id = 2429))'}, {'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.46}]
! 3 0.0618903823196888
{'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.46}
! 2 0.06139722093939781
{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.46, 'Index Cond': '((c_w_id = 8) AND (c_d_id = 7) AND (c_id = 2429))'}
! 0 0.0
empty
! 1 0.0
empty
* 3
{'Node Type': 'ModifyTable', 'Operation': 'Update', 'Parallel Aware': False, 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 12.0, 'Plan Rows': 2, 'Plan Width': 577, 'Plans': [{'Node Type': 'Index Scan', 'Parent Relationship': 'Member', 'Parallel Aware': False, 'Scan Direction': 'Forward', 'Index Name': 'customer_pkey', 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 12.0, 'Plan Rows': 2, 'Plan Width': 577, 'Index Cond': '((c_w_id = 43) AND (c_d_id = 3) AND (c_id = 1172))'}]}
[{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 12.0, 'Index Cond': '((c_w_id = 43) AND (c_d_id = 3) AND (c_id = 1172))'}, {'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 12.0}]
! 3 0.059126053005456924
{'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 12.0}
! 2 0.059026505798101425
{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 12.0, 'Index Cond': '((c_w_id = 43) AND (c_d_id = 3) AND (c_id = 1172))'}
! 0 0.0
empty
! 1 0.0
empty
* 4
{'Node Type': 'ModifyTable', 'Operation': 'Update', 'Parallel Aware': False, 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.46, 'Plan Rows': 1, 'Plan Width': 577, 'Plans': [{'Node Type': 'Index Scan', 'Parent Relationship': 'Member', 'Parallel Aware': False, 'Scan Direction': 'Forward', 'Index Name': 'customer_pkey', 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.46, 'Plan Rows': 1, 'Plan Width': 577, 'Index Cond': '((c_w_id = 35) AND (c_d_id = 4) AND (c_id = 1117))'}]}
[{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.46, 'Index Cond': '((c_w_id = 35) AND (c_d_id = 4) AND (c_id = 1117))'}, {'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.46}]
! 3 0.0618903823196888
{'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.46}
! 2 0.06139722093939781
{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.46, 'Index Cond': '((c_w_id = 35) AND (c_d_id = 4) AND (c_id = 1117))'}
! 0 0.0
empty
! 1 0.0
empty
* 5
[{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.46, 'Index Cond': '((c_w_id = 35) AND (c_d_id = 4) AND (c_id = 1117))'}, {'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.46}]
! 3 0.0618903823196888
{'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.46}
! 2 0.06139722093939781
{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.46, 'Index Cond': '((c_w_id = 35) AND (c_d_id = 4) AND (c_id = 1117))'}
! 0 0.0
empty
! 1 0.0
empty
======Top3 metric
total cpu usage_sys: 0.43708383062792905
dsk/total_read: 0.2668116601704605
total cpu usage_idl: 0.1459962264838198
igs:
[0, 110.71248626708984, 110.69905853271484, 110.71771240234375, 110.72280883789062, 110.72504425048828, 110.72185516357422, 110.71817016601562, 110.71292877197266, 110.72520446777344, 110.71248626708984, 110.70458221435547, 110.72185516357422, 110.72242736816406, 110.72314453125, 110.72077178955078, 110.7249984741211, 110.72431182861328, 110.7132797241211, 110.7242202758789, 110.71589660644531, 110.72854614257812, 110.72693634033203, 110.72265625, 110.7179946899414, 110.72883605957031, 110.72169494628906, 110.72814178466797, 110.71880340576172, 110.72798156738281, 110.72756958007812, 110.7268295288086, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Transformer(
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (2): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (3): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (4): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (5): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
    )
    (norm): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
  )
  (decoder): TransformerDecoder(
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (1): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (2): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (3): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (4): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (5): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
    )
    (norm): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
  )
)
Transformer(
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (2): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (3): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (4): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (5): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
    )
    (norm): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
  )
  (decoder): TransformerDecoder(
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (1): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (2): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (3): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (4): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (5): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
    )
    (norm): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
  )
)
[[[0.0, 0.0, 0.06167038902640343, 0.06192544475197792], [0.0, 0.0, 0.06167038902640343, 0.06192544475197792], [0.0, 0.0, 0.06167038902640343, 0.06192544475197792], [0.0, 0.0, 0.06167038902640343, 0.06192544475197792], [0.0, 0.0, 0.06167038902640343, 0.06192544475197792]], [[0.0, 0.0, 0.0614020861685276, 0.06189192086458206], [0.0, 0.0, 0.0614020861685276, 0.06189192086458206], [0.0, 0.0, 0.0614020861685276, 0.06189192086458206], [0.0, 0.0, 0.0614020861685276, 0.06189192086458206], [0.0, 0.0, 0.0614020861685276, 0.06189192086458206]]]
# 1
UPDATE CUSTOMER   SET C_BALANCE = $1,        C_YTD_PAYMENT = $2,        C_PAYMENT_CNT = $3,        C_DATA = $4  WHERE C_W_ID = $5    AND C_D_ID = $6    AND C_ID = $7
* 1
{'Node Type': 'ModifyTable', 'Operation': 'Update', 'Parallel Aware': False, 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 690, 'Plans': [{'Node Type': 'Index Scan', 'Parent Relationship': 'Member', 'Parallel Aware': False, 'Scan Direction': 'Forward', 'Index Name': 'customer_pkey', 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 690, 'Index Cond': '((c_w_id = 15) AND (c_d_id = 6) AND (c_id = 723))'}]}
[{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 15) AND (c_d_id = 6) AND (c_id = 723))'}, {'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}]
! 3 0.06192544475197792
{'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}
! 2 0.06167038902640343
{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 15) AND (c_d_id = 6) AND (c_id = 723))'}
! 0 0.0
empty
! 1 0.0
empty
* 2
{'Node Type': 'ModifyTable', 'Operation': 'Update', 'Parallel Aware': False, 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 690, 'Plans': [{'Node Type': 'Index Scan', 'Parent Relationship': 'Member', 'Parallel Aware': False, 'Scan Direction': 'Forward', 'Index Name': 'customer_pkey', 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 690, 'Index Cond': '((c_w_id = 25) AND (c_d_id = 1) AND (c_id = 299))'}]}
[{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 25) AND (c_d_id = 1) AND (c_id = 299))'}, {'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}]
! 3 0.06192544475197792
{'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}
! 2 0.06167038902640343
{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 25) AND (c_d_id = 1) AND (c_id = 299))'}
! 0 0.0
empty
! 1 0.0
empty
* 3
{'Node Type': 'ModifyTable', 'Operation': 'Update', 'Parallel Aware': False, 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 690, 'Plans': [{'Node Type': 'Index Scan', 'Parent Relationship': 'Member', 'Parallel Aware': False, 'Scan Direction': 'Forward', 'Index Name': 'customer_pkey', 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 690, 'Index Cond': '((c_w_id = 25) AND (c_d_id = 2) AND (c_id = 238))'}]}
[{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 25) AND (c_d_id = 2) AND (c_id = 238))'}, {'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}]
! 3 0.06192544475197792
{'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}
! 2 0.06167038902640343
{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 25) AND (c_d_id = 2) AND (c_id = 238))'}
! 0 0.0
empty
! 1 0.0
empty
* 4
{'Node Type': 'ModifyTable', 'Operation': 'Update', 'Parallel Aware': False, 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 690, 'Plans': [{'Node Type': 'Index Scan', 'Parent Relationship': 'Member', 'Parallel Aware': False, 'Scan Direction': 'Forward', 'Index Name': 'customer_pkey', 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 690, 'Index Cond': '((c_w_id = 30) AND (c_d_id = 6) AND (c_id = 231))'}]}
[{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 30) AND (c_d_id = 6) AND (c_id = 231))'}, {'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}]
! 3 0.06192544475197792
{'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}
! 2 0.06167038902640343
{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 30) AND (c_d_id = 6) AND (c_id = 231))'}
! 0 0.0
empty
! 1 0.0
empty
* 5
[{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 30) AND (c_d_id = 6) AND (c_id = 231))'}, {'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}]
! 3 0.06192544475197792
{'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}
! 2 0.06167038902640343
{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 30) AND (c_d_id = 6) AND (c_id = 231))'}
! 0 0.0
empty
! 1 0.0
empty
# 2
UPDATE CUSTOMER   SET C_BALANCE = $1,        C_YTD_PAYMENT = $2,        C_PAYMENT_CNT = $3  WHERE C_W_ID = $4    AND C_D_ID = $5    AND C_ID = $6
* 1
{'Node Type': 'ModifyTable', 'Operation': 'Update', 'Parallel Aware': False, 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 577, 'Plans': [{'Node Type': 'Index Scan', 'Parent Relationship': 'Member', 'Parallel Aware': False, 'Scan Direction': 'Forward', 'Index Name': 'customer_pkey', 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 577, 'Index Cond': '((c_w_id = 23) AND (c_d_id = 8) AND (c_id = 1019))'}]}
[{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 23) AND (c_d_id = 8) AND (c_id = 1019))'}, {'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}]
! 3 0.06189192086458206
{'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}
! 2 0.0614020861685276
{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 23) AND (c_d_id = 8) AND (c_id = 1019))'}
! 0 0.0
empty
! 1 0.0
empty
* 2
{'Node Type': 'ModifyTable', 'Operation': 'Update', 'Parallel Aware': False, 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 577, 'Plans': [{'Node Type': 'Index Scan', 'Parent Relationship': 'Member', 'Parallel Aware': False, 'Scan Direction': 'Forward', 'Index Name': 'customer_pkey', 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 577, 'Index Cond': '((c_w_id = 25) AND (c_d_id = 8) AND (c_id = 618))'}]}
[{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 25) AND (c_d_id = 8) AND (c_id = 618))'}, {'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}]
! 3 0.06189192086458206
{'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}
! 2 0.0614020861685276
{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 25) AND (c_d_id = 8) AND (c_id = 618))'}
! 0 0.0
empty
! 1 0.0
empty
* 3
{'Node Type': 'ModifyTable', 'Operation': 'Update', 'Parallel Aware': False, 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 577, 'Plans': [{'Node Type': 'Index Scan', 'Parent Relationship': 'Member', 'Parallel Aware': False, 'Scan Direction': 'Forward', 'Index Name': 'customer_pkey', 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 577, 'Index Cond': '((c_w_id = 30) AND (c_d_id = 4) AND (c_id = 950))'}]}
[{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 30) AND (c_d_id = 4) AND (c_id = 950))'}, {'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}]
! 3 0.06189192086458206
{'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}
! 2 0.0614020861685276
{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 30) AND (c_d_id = 4) AND (c_id = 950))'}
! 0 0.0
empty
! 1 0.0
empty
* 4
{'Node Type': 'ModifyTable', 'Operation': 'Update', 'Parallel Aware': False, 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 577, 'Plans': [{'Node Type': 'Index Scan', 'Parent Relationship': 'Member', 'Parallel Aware': False, 'Scan Direction': 'Forward', 'Index Name': 'customer_pkey', 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 577, 'Index Cond': '((c_w_id = 45) AND (c_d_id = 6) AND (c_id = 747))'}]}
[{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 45) AND (c_d_id = 6) AND (c_id = 747))'}, {'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}]
! 3 0.06189192086458206
{'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}
! 2 0.0614020861685276
{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 45) AND (c_d_id = 6) AND (c_id = 747))'}
! 0 0.0
empty
! 1 0.0
empty
* 5
[{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 45) AND (c_d_id = 6) AND (c_id = 747))'}, {'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}]
! 3 0.06189192086458206
{'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}
! 2 0.0614020861685276
{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 45) AND (c_d_id = 6) AND (c_id = 747))'}
! 0 0.0
empty
! 1 0.0
empty
======Top3 metric
total cpu usage_sys: 0.40559282075005165
dsk/total_read: 0.25369498285811826
total cpu usage_idl: 0.1362856319682466
igs:
[0, 343.687744140625, 343.64605712890625, 343.7039794921875, 343.71978759765625, 343.72674560546875, 343.7168273925781, 343.70538330078125, 343.6891174316406, 343.72723388671875, 343.687744140625, 343.6632080078125, 343.7168273925781, 343.7185974121094, 343.7208557128906, 343.7134704589844, 343.7265930175781, 343.7244567871094, 343.6902160644531, 343.72418212890625, 343.6983337402344, 343.73760986328125, 343.73260498046875, 343.7193298339844, 343.704833984375, 343.738525390625, 343.7163391113281, 343.7363586425781, 343.70733642578125, 343.7358703613281, 343.7345886230469, 343.7322692871094, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Transformer(
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (2): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (3): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (4): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (5): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
    )
    (norm): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
  )
  (decoder): TransformerDecoder(
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (1): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (2): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (3): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (4): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (5): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
    )
    (norm): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
  )
)
Transformer(
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (2): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (3): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (4): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (5): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
    )
    (norm): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
  )
  (decoder): TransformerDecoder(
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (1): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (2): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (3): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (4): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (5): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
    )
    (norm): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
  )
)
[[[0.0, 0.0, 0.06167038902640343, 0.06192544475197792], [0.0, 0.0, 0.06167038902640343, 0.06192544475197792], [0.0, 0.0, 0.06167038902640343, 0.06192544475197792], [0.0, 0.0, 0.06167038902640343, 0.06192544475197792], [0.0, 0.0, 0.06167038902640343, 0.06192544475197792]], [[0.0, 0.0, 0.0614020861685276, 0.06189192086458206], [0.0, 0.0, 0.0614020861685276, 0.06189192086458206], [0.0, 0.0, 0.0614020861685276, 0.06189192086458206], [0.0, 0.0, 0.0614020861685276, 0.06189192086458206], [0.0, 0.0, 0.0614020861685276, 0.06189192086458206]]]
# 1
UPDATE CUSTOMER   SET C_BALANCE = $1,        C_YTD_PAYMENT = $2,        C_PAYMENT_CNT = $3,        C_DATA = $4  WHERE C_W_ID = $5    AND C_D_ID = $6    AND C_ID = $7
* 1
{'Node Type': 'ModifyTable', 'Operation': 'Update', 'Parallel Aware': False, 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 690, 'Plans': [{'Node Type': 'Index Scan', 'Parent Relationship': 'Member', 'Parallel Aware': False, 'Scan Direction': 'Forward', 'Index Name': 'customer_pkey', 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 690, 'Index Cond': '((c_w_id = 15) AND (c_d_id = 6) AND (c_id = 723))'}]}
[{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 15) AND (c_d_id = 6) AND (c_id = 723))'}, {'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}]
! 3 0.06192544475197792
{'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}
! 2 0.06167038902640343
{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 15) AND (c_d_id = 6) AND (c_id = 723))'}
! 0 0.0
empty
! 1 0.0
empty
* 2
{'Node Type': 'ModifyTable', 'Operation': 'Update', 'Parallel Aware': False, 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 690, 'Plans': [{'Node Type': 'Index Scan', 'Parent Relationship': 'Member', 'Parallel Aware': False, 'Scan Direction': 'Forward', 'Index Name': 'customer_pkey', 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 690, 'Index Cond': '((c_w_id = 25) AND (c_d_id = 1) AND (c_id = 299))'}]}
[{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 25) AND (c_d_id = 1) AND (c_id = 299))'}, {'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}]
! 3 0.06192544475197792
{'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}
! 2 0.06167038902640343
{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 25) AND (c_d_id = 1) AND (c_id = 299))'}
! 0 0.0
empty
! 1 0.0
empty
* 3
{'Node Type': 'ModifyTable', 'Operation': 'Update', 'Parallel Aware': False, 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 690, 'Plans': [{'Node Type': 'Index Scan', 'Parent Relationship': 'Member', 'Parallel Aware': False, 'Scan Direction': 'Forward', 'Index Name': 'customer_pkey', 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 690, 'Index Cond': '((c_w_id = 25) AND (c_d_id = 2) AND (c_id = 238))'}]}
[{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 25) AND (c_d_id = 2) AND (c_id = 238))'}, {'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}]
! 3 0.06192544475197792
{'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}
! 2 0.06167038902640343
{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 25) AND (c_d_id = 2) AND (c_id = 238))'}
! 0 0.0
empty
! 1 0.0
empty
* 4
{'Node Type': 'ModifyTable', 'Operation': 'Update', 'Parallel Aware': False, 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 690, 'Plans': [{'Node Type': 'Index Scan', 'Parent Relationship': 'Member', 'Parallel Aware': False, 'Scan Direction': 'Forward', 'Index Name': 'customer_pkey', 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 690, 'Index Cond': '((c_w_id = 30) AND (c_d_id = 6) AND (c_id = 231))'}]}
[{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 30) AND (c_d_id = 6) AND (c_id = 231))'}, {'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}]
! 3 0.06192544475197792
{'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}
! 2 0.06167038902640343
{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 30) AND (c_d_id = 6) AND (c_id = 231))'}
! 0 0.0
empty
! 1 0.0
empty
* 5
[{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 30) AND (c_d_id = 6) AND (c_id = 231))'}, {'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}]
! 3 0.06192544475197792
{'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}
! 2 0.06167038902640343
{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 30) AND (c_d_id = 6) AND (c_id = 231))'}
! 0 0.0
empty
! 1 0.0
empty
# 2
UPDATE CUSTOMER   SET C_BALANCE = $1,        C_YTD_PAYMENT = $2,        C_PAYMENT_CNT = $3  WHERE C_W_ID = $4    AND C_D_ID = $5    AND C_ID = $6
* 1
{'Node Type': 'ModifyTable', 'Operation': 'Update', 'Parallel Aware': False, 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 577, 'Plans': [{'Node Type': 'Index Scan', 'Parent Relationship': 'Member', 'Parallel Aware': False, 'Scan Direction': 'Forward', 'Index Name': 'customer_pkey', 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 577, 'Index Cond': '((c_w_id = 23) AND (c_d_id = 8) AND (c_id = 1019))'}]}
[{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 23) AND (c_d_id = 8) AND (c_id = 1019))'}, {'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}]
! 3 0.06189192086458206
{'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}
! 2 0.0614020861685276
{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 23) AND (c_d_id = 8) AND (c_id = 1019))'}
! 0 0.0
empty
! 1 0.0
empty
* 2
{'Node Type': 'ModifyTable', 'Operation': 'Update', 'Parallel Aware': False, 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 577, 'Plans': [{'Node Type': 'Index Scan', 'Parent Relationship': 'Member', 'Parallel Aware': False, 'Scan Direction': 'Forward', 'Index Name': 'customer_pkey', 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 577, 'Index Cond': '((c_w_id = 25) AND (c_d_id = 8) AND (c_id = 618))'}]}
[{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 25) AND (c_d_id = 8) AND (c_id = 618))'}, {'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}]
! 3 0.06189192086458206
{'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}
! 2 0.0614020861685276
{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 25) AND (c_d_id = 8) AND (c_id = 618))'}
! 0 0.0
empty
! 1 0.0
empty
* 3
{'Node Type': 'ModifyTable', 'Operation': 'Update', 'Parallel Aware': False, 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 577, 'Plans': [{'Node Type': 'Index Scan', 'Parent Relationship': 'Member', 'Parallel Aware': False, 'Scan Direction': 'Forward', 'Index Name': 'customer_pkey', 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 577, 'Index Cond': '((c_w_id = 30) AND (c_d_id = 4) AND (c_id = 950))'}]}
[{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 30) AND (c_d_id = 4) AND (c_id = 950))'}, {'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}]
! 3 0.06189192086458206
{'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}
! 2 0.0614020861685276
{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 30) AND (c_d_id = 4) AND (c_id = 950))'}
! 0 0.0
empty
! 1 0.0
empty
* 4
{'Node Type': 'ModifyTable', 'Operation': 'Update', 'Parallel Aware': False, 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 577, 'Plans': [{'Node Type': 'Index Scan', 'Parent Relationship': 'Member', 'Parallel Aware': False, 'Scan Direction': 'Forward', 'Index Name': 'customer_pkey', 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 577, 'Index Cond': '((c_w_id = 45) AND (c_d_id = 6) AND (c_id = 747))'}]}
[{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 45) AND (c_d_id = 6) AND (c_id = 747))'}, {'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}]
! 3 0.06189192086458206
{'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}
! 2 0.0614020861685276
{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 45) AND (c_d_id = 6) AND (c_id = 747))'}
! 0 0.0
empty
! 1 0.0
empty
* 5
[{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 45) AND (c_d_id = 6) AND (c_id = 747))'}, {'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}]
! 3 0.06189192086458206
{'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}
! 2 0.0614020861685276
{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 45) AND (c_d_id = 6) AND (c_id = 747))'}
! 0 0.0
empty
! 1 0.0
empty
======Top3 metric
total cpu usage_sys: 0.3981467764965006
dsk/total_read: 0.2505465765040472
total cpu usage_idl: 0.14147394013278447
igs:
[0, 297.75897216796875, 297.72283935546875, 297.77301025390625, 297.7867126464844, 297.792724609375, 297.7841491699219, 297.77423095703125, 297.7601623535156, 297.79315185546875, 297.75897216796875, 297.7377014160156, 297.7841491699219, 297.7856750488281, 297.7876281738281, 297.78125, 297.7926025390625, 297.790771484375, 297.7610778808594, 297.7904968261719, 297.76812744140625, 297.8021545410156, 297.7978210449219, 297.78631591796875, 297.7737731933594, 297.8029479980469, 297.7837219238281, 297.8010559082031, 297.77593994140625, 297.8006286621094, 297.7995300292969, 297.79754638671875, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Transformer(
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (2): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (3): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (4): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (5): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
    )
    (norm): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
  )
  (decoder): TransformerDecoder(
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (1): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (2): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (3): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (4): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (5): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
    )
    (norm): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
  )
)
Transformer(
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (2): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (3): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (4): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (5): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
    )
    (norm): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
  )
  (decoder): TransformerDecoder(
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (1): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (2): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (3): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (4): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (5): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
    )
    (norm): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
  )
)
[[[0.0, 0.0, 0.06167038902640343, 0.06192544475197792], [0.0, 0.0, 0.06167038902640343, 0.06192544475197792], [0.0, 0.0, 0.06167038902640343, 0.06192544475197792], [0.0, 0.0, 0.06167038902640343, 0.06192544475197792], [0.0, 0.0, 0.06167038902640343, 0.06192544475197792]], [[0.0, 0.0, 0.0614020861685276, 0.06189192086458206], [0.0, 0.0, 0.0614020861685276, 0.06189192086458206], [0.0, 0.0, 0.0614020861685276, 0.06189192086458206], [0.0, 0.0, 0.0614020861685276, 0.06189192086458206], [0.0, 0.0, 0.0614020861685276, 0.06189192086458206]]]
# 1
UPDATE CUSTOMER   SET C_BALANCE = $1,        C_YTD_PAYMENT = $2,        C_PAYMENT_CNT = $3,        C_DATA = $4  WHERE C_W_ID = $5    AND C_D_ID = $6    AND C_ID = $7
* 1
{'Node Type': 'ModifyTable', 'Operation': 'Update', 'Parallel Aware': False, 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 690, 'Plans': [{'Node Type': 'Index Scan', 'Parent Relationship': 'Member', 'Parallel Aware': False, 'Scan Direction': 'Forward', 'Index Name': 'customer_pkey', 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 690, 'Index Cond': '((c_w_id = 15) AND (c_d_id = 6) AND (c_id = 723))'}]}
[{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 15) AND (c_d_id = 6) AND (c_id = 723))'}, {'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}]
! 3 0.06192544475197792
{'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}
! 2 0.06167038902640343
{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 15) AND (c_d_id = 6) AND (c_id = 723))'}
! 0 0.0
empty
! 1 0.0
empty
* 2
{'Node Type': 'ModifyTable', 'Operation': 'Update', 'Parallel Aware': False, 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 690, 'Plans': [{'Node Type': 'Index Scan', 'Parent Relationship': 'Member', 'Parallel Aware': False, 'Scan Direction': 'Forward', 'Index Name': 'customer_pkey', 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 690, 'Index Cond': '((c_w_id = 25) AND (c_d_id = 1) AND (c_id = 299))'}]}
[{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 25) AND (c_d_id = 1) AND (c_id = 299))'}, {'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}]
! 3 0.06192544475197792
{'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}
! 2 0.06167038902640343
{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 25) AND (c_d_id = 1) AND (c_id = 299))'}
! 0 0.0
empty
! 1 0.0
empty
* 3
{'Node Type': 'ModifyTable', 'Operation': 'Update', 'Parallel Aware': False, 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 690, 'Plans': [{'Node Type': 'Index Scan', 'Parent Relationship': 'Member', 'Parallel Aware': False, 'Scan Direction': 'Forward', 'Index Name': 'customer_pkey', 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 690, 'Index Cond': '((c_w_id = 25) AND (c_d_id = 2) AND (c_id = 238))'}]}
[{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 25) AND (c_d_id = 2) AND (c_id = 238))'}, {'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}]
! 3 0.06192544475197792
{'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}
! 2 0.06167038902640343
{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 25) AND (c_d_id = 2) AND (c_id = 238))'}
! 0 0.0
empty
! 1 0.0
empty
* 4
{'Node Type': 'ModifyTable', 'Operation': 'Update', 'Parallel Aware': False, 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 690, 'Plans': [{'Node Type': 'Index Scan', 'Parent Relationship': 'Member', 'Parallel Aware': False, 'Scan Direction': 'Forward', 'Index Name': 'customer_pkey', 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 690, 'Index Cond': '((c_w_id = 30) AND (c_d_id = 6) AND (c_id = 231))'}]}
[{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 30) AND (c_d_id = 6) AND (c_id = 231))'}, {'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}]
! 3 0.06192544475197792
{'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}
! 2 0.06167038902640343
{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 30) AND (c_d_id = 6) AND (c_id = 231))'}
! 0 0.0
empty
! 1 0.0
empty
* 5
[{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 30) AND (c_d_id = 6) AND (c_id = 231))'}, {'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}]
! 3 0.06192544475197792
{'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}
! 2 0.06167038902640343
{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 30) AND (c_d_id = 6) AND (c_id = 231))'}
! 0 0.0
empty
! 1 0.0
empty
# 2
UPDATE CUSTOMER   SET C_BALANCE = $1,        C_YTD_PAYMENT = $2,        C_PAYMENT_CNT = $3  WHERE C_W_ID = $4    AND C_D_ID = $5    AND C_ID = $6
* 1
{'Node Type': 'ModifyTable', 'Operation': 'Update', 'Parallel Aware': False, 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 577, 'Plans': [{'Node Type': 'Index Scan', 'Parent Relationship': 'Member', 'Parallel Aware': False, 'Scan Direction': 'Forward', 'Index Name': 'customer_pkey', 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 577, 'Index Cond': '((c_w_id = 23) AND (c_d_id = 8) AND (c_id = 1019))'}]}
[{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 23) AND (c_d_id = 8) AND (c_id = 1019))'}, {'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}]
! 3 0.06189192086458206
{'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}
! 2 0.0614020861685276
{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 23) AND (c_d_id = 8) AND (c_id = 1019))'}
! 0 0.0
empty
! 1 0.0
empty
* 2
{'Node Type': 'ModifyTable', 'Operation': 'Update', 'Parallel Aware': False, 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 577, 'Plans': [{'Node Type': 'Index Scan', 'Parent Relationship': 'Member', 'Parallel Aware': False, 'Scan Direction': 'Forward', 'Index Name': 'customer_pkey', 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 577, 'Index Cond': '((c_w_id = 25) AND (c_d_id = 8) AND (c_id = 618))'}]}
[{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 25) AND (c_d_id = 8) AND (c_id = 618))'}, {'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}]
! 3 0.06189192086458206
{'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}
! 2 0.0614020861685276
{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 25) AND (c_d_id = 8) AND (c_id = 618))'}
! 0 0.0
empty
! 1 0.0
empty
* 3
{'Node Type': 'ModifyTable', 'Operation': 'Update', 'Parallel Aware': False, 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 577, 'Plans': [{'Node Type': 'Index Scan', 'Parent Relationship': 'Member', 'Parallel Aware': False, 'Scan Direction': 'Forward', 'Index Name': 'customer_pkey', 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 577, 'Index Cond': '((c_w_id = 30) AND (c_d_id = 4) AND (c_id = 950))'}]}
[{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 30) AND (c_d_id = 4) AND (c_id = 950))'}, {'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}]
! 3 0.06189192086458206
{'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}
! 2 0.0614020861685276
{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 30) AND (c_d_id = 4) AND (c_id = 950))'}
! 0 0.0
empty
! 1 0.0
empty
* 4
{'Node Type': 'ModifyTable', 'Operation': 'Update', 'Parallel Aware': False, 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 577, 'Plans': [{'Node Type': 'Index Scan', 'Parent Relationship': 'Member', 'Parallel Aware': False, 'Scan Direction': 'Forward', 'Index Name': 'customer_pkey', 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 577, 'Index Cond': '((c_w_id = 45) AND (c_d_id = 6) AND (c_id = 747))'}]}
[{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 45) AND (c_d_id = 6) AND (c_id = 747))'}, {'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}]
! 3 0.06189192086458206
{'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}
! 2 0.0614020861685276
{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 45) AND (c_d_id = 6) AND (c_id = 747))'}
! 0 0.0
empty
! 1 0.0
empty
* 5
[{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 45) AND (c_d_id = 6) AND (c_id = 747))'}, {'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}]
! 3 0.06189192086458206
{'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}
! 2 0.0614020861685276
{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 45) AND (c_d_id = 6) AND (c_id = 747))'}
! 0 0.0
empty
! 1 0.0
empty
======Top3 metric
total cpu usage_sys: 0.3911260003794482
dsk/total_read: 0.25052459596697274
total cpu usage_idl: 0.14172034574717438
igs:
[0, 307.4426574707031, 307.40533447265625, 307.4571533203125, 307.4713134765625, 307.4775085449219, 307.4686584472656, 307.45843505859375, 307.4438781738281, 307.47796630859375, 307.4426574707031, 307.4206848144531, 307.4686584472656, 307.4702453613281, 307.4722595214844, 307.46563720703125, 307.4773864746094, 307.4754638671875, 307.44482421875, 307.4752197265625, 307.4521179199219, 307.48724365234375, 307.4827575683594, 307.47088623046875, 307.4579162597656, 307.488037109375, 307.46820068359375, 307.4861145019531, 307.4601745605469, 307.4856872558594, 307.4845275878906, 307.48248291015625, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Transformer(
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (2): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (3): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (4): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (5): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
    )
    (norm): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
  )
  (decoder): TransformerDecoder(
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (1): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (2): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (3): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (4): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (5): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
    )
    (norm): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
  )
)
Transformer(
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (2): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (3): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (4): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (5): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
    )
    (norm): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
  )
  (decoder): TransformerDecoder(
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (1): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (2): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (3): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (4): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (5): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
    )
    (norm): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
  )
)
[[[0.0, 0.0, 0.06167038902640343, 0.06192544475197792], [0.0, 0.0, 0.06167038902640343, 0.06192544475197792], [0.0, 0.0, 0.06167038902640343, 0.06192544475197792], [0.0, 0.0, 0.06167038902640343, 0.06192544475197792], [0.0, 0.0, 0.06167038902640343, 0.06192544475197792]], [[0.0, 0.0, 0.0614020861685276, 0.06189192086458206], [0.0, 0.0, 0.0614020861685276, 0.06189192086458206], [0.0, 0.0, 0.0614020861685276, 0.06189192086458206], [0.0, 0.0, 0.0614020861685276, 0.06189192086458206], [0.0, 0.0, 0.0614020861685276, 0.06189192086458206]]]
# 1
UPDATE CUSTOMER   SET C_BALANCE = $1,        C_YTD_PAYMENT = $2,        C_PAYMENT_CNT = $3,        C_DATA = $4  WHERE C_W_ID = $5    AND C_D_ID = $6    AND C_ID = $7
* 1
{'Node Type': 'ModifyTable', 'Operation': 'Update', 'Parallel Aware': False, 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 690, 'Plans': [{'Node Type': 'Index Scan', 'Parent Relationship': 'Member', 'Parallel Aware': False, 'Scan Direction': 'Forward', 'Index Name': 'customer_pkey', 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 690, 'Index Cond': '((c_w_id = 15) AND (c_d_id = 6) AND (c_id = 723))'}]}
[{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 15) AND (c_d_id = 6) AND (c_id = 723))'}, {'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}]
! 3 0.06192544475197792
{'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}
! 2 0.06167038902640343
{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 15) AND (c_d_id = 6) AND (c_id = 723))'}
! 0 0.0
empty
! 1 0.0
empty
* 2
{'Node Type': 'ModifyTable', 'Operation': 'Update', 'Parallel Aware': False, 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 690, 'Plans': [{'Node Type': 'Index Scan', 'Parent Relationship': 'Member', 'Parallel Aware': False, 'Scan Direction': 'Forward', 'Index Name': 'customer_pkey', 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 690, 'Index Cond': '((c_w_id = 25) AND (c_d_id = 1) AND (c_id = 299))'}]}
[{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 25) AND (c_d_id = 1) AND (c_id = 299))'}, {'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}]
! 3 0.06192544475197792
{'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}
! 2 0.06167038902640343
{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 25) AND (c_d_id = 1) AND (c_id = 299))'}
! 0 0.0
empty
! 1 0.0
empty
* 3
{'Node Type': 'ModifyTable', 'Operation': 'Update', 'Parallel Aware': False, 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 690, 'Plans': [{'Node Type': 'Index Scan', 'Parent Relationship': 'Member', 'Parallel Aware': False, 'Scan Direction': 'Forward', 'Index Name': 'customer_pkey', 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 690, 'Index Cond': '((c_w_id = 25) AND (c_d_id = 2) AND (c_id = 238))'}]}
[{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 25) AND (c_d_id = 2) AND (c_id = 238))'}, {'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}]
! 3 0.06192544475197792
{'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}
! 2 0.06167038902640343
{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 25) AND (c_d_id = 2) AND (c_id = 238))'}
! 0 0.0
empty
! 1 0.0
empty
* 4
{'Node Type': 'ModifyTable', 'Operation': 'Update', 'Parallel Aware': False, 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 690, 'Plans': [{'Node Type': 'Index Scan', 'Parent Relationship': 'Member', 'Parallel Aware': False, 'Scan Direction': 'Forward', 'Index Name': 'customer_pkey', 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 690, 'Index Cond': '((c_w_id = 30) AND (c_d_id = 6) AND (c_id = 231))'}]}
[{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 30) AND (c_d_id = 6) AND (c_id = 231))'}, {'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}]
! 3 0.06192544475197792
{'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}
! 2 0.06167038902640343
{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 30) AND (c_d_id = 6) AND (c_id = 231))'}
! 0 0.0
empty
! 1 0.0
empty
* 5
[{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 30) AND (c_d_id = 6) AND (c_id = 231))'}, {'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}]
! 3 0.06192544475197792
{'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}
! 2 0.06167038902640343
{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 30) AND (c_d_id = 6) AND (c_id = 231))'}
! 0 0.0
empty
! 1 0.0
empty
# 2
UPDATE CUSTOMER   SET C_BALANCE = $1,        C_YTD_PAYMENT = $2,        C_PAYMENT_CNT = $3  WHERE C_W_ID = $4    AND C_D_ID = $5    AND C_ID = $6
* 1
{'Node Type': 'ModifyTable', 'Operation': 'Update', 'Parallel Aware': False, 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 577, 'Plans': [{'Node Type': 'Index Scan', 'Parent Relationship': 'Member', 'Parallel Aware': False, 'Scan Direction': 'Forward', 'Index Name': 'customer_pkey', 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 577, 'Index Cond': '((c_w_id = 23) AND (c_d_id = 8) AND (c_id = 1019))'}]}
[{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 23) AND (c_d_id = 8) AND (c_id = 1019))'}, {'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}]
! 3 0.06189192086458206
{'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}
! 2 0.0614020861685276
{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 23) AND (c_d_id = 8) AND (c_id = 1019))'}
! 0 0.0
empty
! 1 0.0
empty
* 2
{'Node Type': 'ModifyTable', 'Operation': 'Update', 'Parallel Aware': False, 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 577, 'Plans': [{'Node Type': 'Index Scan', 'Parent Relationship': 'Member', 'Parallel Aware': False, 'Scan Direction': 'Forward', 'Index Name': 'customer_pkey', 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 577, 'Index Cond': '((c_w_id = 25) AND (c_d_id = 8) AND (c_id = 618))'}]}
[{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 25) AND (c_d_id = 8) AND (c_id = 618))'}, {'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}]
! 3 0.06189192086458206
{'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}
! 2 0.0614020861685276
{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 25) AND (c_d_id = 8) AND (c_id = 618))'}
! 0 0.0
empty
! 1 0.0
empty
* 3
{'Node Type': 'ModifyTable', 'Operation': 'Update', 'Parallel Aware': False, 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 577, 'Plans': [{'Node Type': 'Index Scan', 'Parent Relationship': 'Member', 'Parallel Aware': False, 'Scan Direction': 'Forward', 'Index Name': 'customer_pkey', 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 577, 'Index Cond': '((c_w_id = 30) AND (c_d_id = 4) AND (c_id = 950))'}]}
[{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 30) AND (c_d_id = 4) AND (c_id = 950))'}, {'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}]
! 3 0.06189192086458206
{'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}
! 2 0.0614020861685276
{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 30) AND (c_d_id = 4) AND (c_id = 950))'}
! 0 0.0
empty
! 1 0.0
empty
* 4
{'Node Type': 'ModifyTable', 'Operation': 'Update', 'Parallel Aware': False, 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 577, 'Plans': [{'Node Type': 'Index Scan', 'Parent Relationship': 'Member', 'Parallel Aware': False, 'Scan Direction': 'Forward', 'Index Name': 'customer_pkey', 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 577, 'Index Cond': '((c_w_id = 45) AND (c_d_id = 6) AND (c_id = 747))'}]}
[{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 45) AND (c_d_id = 6) AND (c_id = 747))'}, {'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}]
! 3 0.06189192086458206
{'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}
! 2 0.0614020861685276
{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 45) AND (c_d_id = 6) AND (c_id = 747))'}
! 0 0.0
empty
! 1 0.0
empty
* 5
[{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 45) AND (c_d_id = 6) AND (c_id = 747))'}, {'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}]
! 3 0.06189192086458206
{'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}
! 2 0.0614020861685276
{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 45) AND (c_d_id = 6) AND (c_id = 747))'}
! 0 0.0
empty
! 1 0.0
empty
======Top3 metric
total cpu usage_sys: 0.39121193016067735
dsk/total_read: 0.2504769208242727
total cpu usage_idl: 0.13795332483111336
igs:
[0, 305.1979064941406, 305.1608581542969, 305.2122802734375, 305.2263488769531, 305.2325134277344, 305.22369384765625, 305.21356201171875, 305.1990966796875, 305.2329406738281, 305.1979064941406, 305.17608642578125, 305.22369384765625, 305.22528076171875, 305.2272644042969, 305.2207336425781, 305.2323913574219, 305.2304992675781, 305.2000732421875, 305.230224609375, 305.2073059082031, 305.2421569824219, 305.2377014160156, 305.2259216308594, 305.21307373046875, 305.24298095703125, 305.2232666015625, 305.2410583496094, 305.2153015136719, 305.2406005859375, 305.2394714355469, 305.2374267578125, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Transformer(
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (2): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (3): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (4): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (5): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
    )
    (norm): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
  )
  (decoder): TransformerDecoder(
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (1): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (2): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (3): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (4): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (5): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
    )
    (norm): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
  )
)
Transformer(
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (2): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (3): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (4): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (5): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
    )
    (norm): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
  )
  (decoder): TransformerDecoder(
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (1): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (2): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (3): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (4): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (5): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
    )
    (norm): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
  )
)
[[[0.0, 0.0, 0.06167038902640343, 0.06192544475197792], [0.0, 0.0, 0.06167038902640343, 0.06192544475197792], [0.0, 0.0, 0.06167038902640343, 0.06192544475197792], [0.0, 0.0, 0.06167038902640343, 0.06192544475197792], [0.0, 0.0, 0.06167038902640343, 0.06192544475197792]], [[0.0, 0.0, 0.0614020861685276, 0.06189192086458206], [0.0, 0.0, 0.0614020861685276, 0.06189192086458206], [0.0, 0.0, 0.0614020861685276, 0.06189192086458206], [0.0, 0.0, 0.0614020861685276, 0.06189192086458206], [0.0, 0.0, 0.0614020861685276, 0.06189192086458206]]]
# 1
UPDATE CUSTOMER   SET C_BALANCE = $1,        C_YTD_PAYMENT = $2,        C_PAYMENT_CNT = $3,        C_DATA = $4  WHERE C_W_ID = $5    AND C_D_ID = $6    AND C_ID = $7
* 1
{'Node Type': 'ModifyTable', 'Operation': 'Update', 'Parallel Aware': False, 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 690, 'Plans': [{'Node Type': 'Index Scan', 'Parent Relationship': 'Member', 'Parallel Aware': False, 'Scan Direction': 'Forward', 'Index Name': 'customer_pkey', 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 690, 'Index Cond': '((c_w_id = 15) AND (c_d_id = 6) AND (c_id = 723))'}]}
[{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 15) AND (c_d_id = 6) AND (c_id = 723))'}, {'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}]
! 3 0.06192544475197792
{'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}
! 2 0.06167038902640343
{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 15) AND (c_d_id = 6) AND (c_id = 723))'}
! 0 0.0
empty
! 1 0.0
empty
* 2
{'Node Type': 'ModifyTable', 'Operation': 'Update', 'Parallel Aware': False, 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 690, 'Plans': [{'Node Type': 'Index Scan', 'Parent Relationship': 'Member', 'Parallel Aware': False, 'Scan Direction': 'Forward', 'Index Name': 'customer_pkey', 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 690, 'Index Cond': '((c_w_id = 25) AND (c_d_id = 1) AND (c_id = 299))'}]}
[{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 25) AND (c_d_id = 1) AND (c_id = 299))'}, {'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}]
! 3 0.06192544475197792
{'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}
! 2 0.06167038902640343
{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 25) AND (c_d_id = 1) AND (c_id = 299))'}
! 0 0.0
empty
! 1 0.0
empty
* 3
{'Node Type': 'ModifyTable', 'Operation': 'Update', 'Parallel Aware': False, 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 690, 'Plans': [{'Node Type': 'Index Scan', 'Parent Relationship': 'Member', 'Parallel Aware': False, 'Scan Direction': 'Forward', 'Index Name': 'customer_pkey', 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 690, 'Index Cond': '((c_w_id = 25) AND (c_d_id = 2) AND (c_id = 238))'}]}
[{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 25) AND (c_d_id = 2) AND (c_id = 238))'}, {'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}]
! 3 0.06192544475197792
{'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}
! 2 0.06167038902640343
{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 25) AND (c_d_id = 2) AND (c_id = 238))'}
! 0 0.0
empty
! 1 0.0
empty
* 4
{'Node Type': 'ModifyTable', 'Operation': 'Update', 'Parallel Aware': False, 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 690, 'Plans': [{'Node Type': 'Index Scan', 'Parent Relationship': 'Member', 'Parallel Aware': False, 'Scan Direction': 'Forward', 'Index Name': 'customer_pkey', 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 690, 'Index Cond': '((c_w_id = 30) AND (c_d_id = 6) AND (c_id = 231))'}]}
[{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 30) AND (c_d_id = 6) AND (c_id = 231))'}, {'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}]
! 3 0.06192544475197792
{'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}
! 2 0.06167038902640343
{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 30) AND (c_d_id = 6) AND (c_id = 231))'}
! 0 0.0
empty
! 1 0.0
empty
* 5
[{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 30) AND (c_d_id = 6) AND (c_id = 231))'}, {'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}]
! 3 0.06192544475197792
{'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}
! 2 0.06167038902640343
{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 30) AND (c_d_id = 6) AND (c_id = 231))'}
! 0 0.0
empty
! 1 0.0
empty
# 2
UPDATE CUSTOMER   SET C_BALANCE = $1,        C_YTD_PAYMENT = $2,        C_PAYMENT_CNT = $3  WHERE C_W_ID = $4    AND C_D_ID = $5    AND C_ID = $6
* 1
{'Node Type': 'ModifyTable', 'Operation': 'Update', 'Parallel Aware': False, 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 577, 'Plans': [{'Node Type': 'Index Scan', 'Parent Relationship': 'Member', 'Parallel Aware': False, 'Scan Direction': 'Forward', 'Index Name': 'customer_pkey', 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 577, 'Index Cond': '((c_w_id = 23) AND (c_d_id = 8) AND (c_id = 1019))'}]}
[{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 23) AND (c_d_id = 8) AND (c_id = 1019))'}, {'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}]
! 3 0.06189192086458206
{'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}
! 2 0.0614020861685276
{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 23) AND (c_d_id = 8) AND (c_id = 1019))'}
! 0 0.0
empty
! 1 0.0
empty
* 2
{'Node Type': 'ModifyTable', 'Operation': 'Update', 'Parallel Aware': False, 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 577, 'Plans': [{'Node Type': 'Index Scan', 'Parent Relationship': 'Member', 'Parallel Aware': False, 'Scan Direction': 'Forward', 'Index Name': 'customer_pkey', 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 577, 'Index Cond': '((c_w_id = 25) AND (c_d_id = 8) AND (c_id = 618))'}]}
[{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 25) AND (c_d_id = 8) AND (c_id = 618))'}, {'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}]
! 3 0.06189192086458206
{'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}
! 2 0.0614020861685276
{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 25) AND (c_d_id = 8) AND (c_id = 618))'}
! 0 0.0
empty
! 1 0.0
empty
* 3
{'Node Type': 'ModifyTable', 'Operation': 'Update', 'Parallel Aware': False, 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 577, 'Plans': [{'Node Type': 'Index Scan', 'Parent Relationship': 'Member', 'Parallel Aware': False, 'Scan Direction': 'Forward', 'Index Name': 'customer_pkey', 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 577, 'Index Cond': '((c_w_id = 30) AND (c_d_id = 4) AND (c_id = 950))'}]}
[{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 30) AND (c_d_id = 4) AND (c_id = 950))'}, {'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}]
! 3 0.06189192086458206
{'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}
! 2 0.0614020861685276
{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 30) AND (c_d_id = 4) AND (c_id = 950))'}
! 0 0.0
empty
! 1 0.0
empty
* 4
{'Node Type': 'ModifyTable', 'Operation': 'Update', 'Parallel Aware': False, 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 577, 'Plans': [{'Node Type': 'Index Scan', 'Parent Relationship': 'Member', 'Parallel Aware': False, 'Scan Direction': 'Forward', 'Index Name': 'customer_pkey', 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 577, 'Index Cond': '((c_w_id = 45) AND (c_d_id = 6) AND (c_id = 747))'}]}
[{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 45) AND (c_d_id = 6) AND (c_id = 747))'}, {'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}]
! 3 0.06189192086458206
{'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}
! 2 0.0614020861685276
{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 45) AND (c_d_id = 6) AND (c_id = 747))'}
! 0 0.0
empty
! 1 0.0
empty
* 5
[{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 45) AND (c_d_id = 6) AND (c_id = 747))'}, {'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}]
! 3 0.06189192086458206
{'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}
! 2 0.0614020861685276
{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 45) AND (c_d_id = 6) AND (c_id = 747))'}
! 0 0.0
empty
! 1 0.0
empty
======Top3 metric
total cpu usage_sys: 0.444634875937776
dsk/total_read: 0.26699840882099757
total cpu usage_idl: 0.05714652826969164
igs:
[0, 116.0832748413086, 116.0691909790039, 116.08875274658203, 116.09410095214844, 116.09644317626953, 116.09309387207031, 116.0892333984375, 116.083740234375, 116.09661102294922, 116.0832748413086, 116.07498168945312, 116.09309387207031, 116.09369659423828, 116.09445190429688, 116.09196472167969, 116.09638977050781, 116.09567260742188, 116.0841064453125, 116.09557342529297, 116.08685302734375, 116.10011291503906, 116.09841918945312, 116.09394073486328, 116.08905029296875, 116.10041809082031, 116.09292602539062, 116.09969329833984, 116.08989715576172, 116.09952545166016, 116.09909057617188, 116.09831237792969, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Transformer(
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (2): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (3): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (4): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (5): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
    )
    (norm): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
  )
  (decoder): TransformerDecoder(
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (1): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (2): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (3): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (4): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (5): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
    )
    (norm): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
  )
)
Transformer(
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (2): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (3): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (4): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (5): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
    )
    (norm): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
  )
  (decoder): TransformerDecoder(
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (1): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (2): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (3): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (4): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (5): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
    )
    (norm): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
  )
)
[[[0.0, 0.0, 0.06167038902640343, 0.06192544475197792], [0.0, 0.0, 0.06167038902640343, 0.06192544475197792], [0.0, 0.0, 0.06167038902640343, 0.06192544475197792], [0.0, 0.0, 0.06167038902640343, 0.06192544475197792], [0.0, 0.0, 0.06167038902640343, 0.06192544475197792]], [[0.0, 0.0, 0.0614020861685276, 0.06189192086458206], [0.0, 0.0, 0.0614020861685276, 0.06189192086458206], [0.0, 0.0, 0.0614020861685276, 0.06189192086458206], [0.0, 0.0, 0.0614020861685276, 0.06189192086458206], [0.0, 0.0, 0.0614020861685276, 0.06189192086458206]]]
# 1
UPDATE CUSTOMER   SET C_BALANCE = $1,        C_YTD_PAYMENT = $2,        C_PAYMENT_CNT = $3,        C_DATA = $4  WHERE C_W_ID = $5    AND C_D_ID = $6    AND C_ID = $7
* 1
{'Node Type': 'ModifyTable', 'Operation': 'Update', 'Parallel Aware': False, 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 690, 'Plans': [{'Node Type': 'Index Scan', 'Parent Relationship': 'Member', 'Parallel Aware': False, 'Scan Direction': 'Forward', 'Index Name': 'customer_pkey', 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 690, 'Index Cond': '((c_w_id = 15) AND (c_d_id = 6) AND (c_id = 723))'}]}
[{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 15) AND (c_d_id = 6) AND (c_id = 723))'}, {'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}]
! 3 0.06192544475197792
{'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}
! 2 0.06167038902640343
{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 15) AND (c_d_id = 6) AND (c_id = 723))'}
! 0 0.0
empty
! 1 0.0
empty
* 2
{'Node Type': 'ModifyTable', 'Operation': 'Update', 'Parallel Aware': False, 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 690, 'Plans': [{'Node Type': 'Index Scan', 'Parent Relationship': 'Member', 'Parallel Aware': False, 'Scan Direction': 'Forward', 'Index Name': 'customer_pkey', 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 690, 'Index Cond': '((c_w_id = 25) AND (c_d_id = 1) AND (c_id = 299))'}]}
[{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 25) AND (c_d_id = 1) AND (c_id = 299))'}, {'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}]
! 3 0.06192544475197792
{'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}
! 2 0.06167038902640343
{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 25) AND (c_d_id = 1) AND (c_id = 299))'}
! 0 0.0
empty
! 1 0.0
empty
* 3
{'Node Type': 'ModifyTable', 'Operation': 'Update', 'Parallel Aware': False, 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 690, 'Plans': [{'Node Type': 'Index Scan', 'Parent Relationship': 'Member', 'Parallel Aware': False, 'Scan Direction': 'Forward', 'Index Name': 'customer_pkey', 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 690, 'Index Cond': '((c_w_id = 25) AND (c_d_id = 2) AND (c_id = 238))'}]}
[{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 25) AND (c_d_id = 2) AND (c_id = 238))'}, {'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}]
! 3 0.06192544475197792
{'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}
! 2 0.06167038902640343
{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 25) AND (c_d_id = 2) AND (c_id = 238))'}
! 0 0.0
empty
! 1 0.0
empty
* 4
{'Node Type': 'ModifyTable', 'Operation': 'Update', 'Parallel Aware': False, 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 690, 'Plans': [{'Node Type': 'Index Scan', 'Parent Relationship': 'Member', 'Parallel Aware': False, 'Scan Direction': 'Forward', 'Index Name': 'customer_pkey', 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 690, 'Index Cond': '((c_w_id = 30) AND (c_d_id = 6) AND (c_id = 231))'}]}
[{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 30) AND (c_d_id = 6) AND (c_id = 231))'}, {'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}]
! 3 0.06192544475197792
{'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}
! 2 0.06167038902640343
{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 30) AND (c_d_id = 6) AND (c_id = 231))'}
! 0 0.0
empty
! 1 0.0
empty
* 5
[{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 30) AND (c_d_id = 6) AND (c_id = 231))'}, {'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}]
! 3 0.06192544475197792
{'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}
! 2 0.06167038902640343
{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 30) AND (c_d_id = 6) AND (c_id = 231))'}
! 0 0.0
empty
! 1 0.0
empty
# 2
UPDATE CUSTOMER   SET C_BALANCE = $1,        C_YTD_PAYMENT = $2,        C_PAYMENT_CNT = $3  WHERE C_W_ID = $4    AND C_D_ID = $5    AND C_ID = $6
* 1
{'Node Type': 'ModifyTable', 'Operation': 'Update', 'Parallel Aware': False, 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 577, 'Plans': [{'Node Type': 'Index Scan', 'Parent Relationship': 'Member', 'Parallel Aware': False, 'Scan Direction': 'Forward', 'Index Name': 'customer_pkey', 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 577, 'Index Cond': '((c_w_id = 23) AND (c_d_id = 8) AND (c_id = 1019))'}]}
[{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 23) AND (c_d_id = 8) AND (c_id = 1019))'}, {'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}]
! 3 0.06189192086458206
{'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}
! 2 0.0614020861685276
{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 23) AND (c_d_id = 8) AND (c_id = 1019))'}
! 0 0.0
empty
! 1 0.0
empty
* 2
{'Node Type': 'ModifyTable', 'Operation': 'Update', 'Parallel Aware': False, 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 577, 'Plans': [{'Node Type': 'Index Scan', 'Parent Relationship': 'Member', 'Parallel Aware': False, 'Scan Direction': 'Forward', 'Index Name': 'customer_pkey', 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 577, 'Index Cond': '((c_w_id = 25) AND (c_d_id = 8) AND (c_id = 618))'}]}
[{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 25) AND (c_d_id = 8) AND (c_id = 618))'}, {'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}]
! 3 0.06189192086458206
{'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}
! 2 0.0614020861685276
{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 25) AND (c_d_id = 8) AND (c_id = 618))'}
! 0 0.0
empty
! 1 0.0
empty
* 3
{'Node Type': 'ModifyTable', 'Operation': 'Update', 'Parallel Aware': False, 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 577, 'Plans': [{'Node Type': 'Index Scan', 'Parent Relationship': 'Member', 'Parallel Aware': False, 'Scan Direction': 'Forward', 'Index Name': 'customer_pkey', 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 577, 'Index Cond': '((c_w_id = 30) AND (c_d_id = 4) AND (c_id = 950))'}]}
[{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 30) AND (c_d_id = 4) AND (c_id = 950))'}, {'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}]
! 3 0.06189192086458206
{'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}
! 2 0.0614020861685276
{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 30) AND (c_d_id = 4) AND (c_id = 950))'}
! 0 0.0
empty
! 1 0.0
empty
* 4
{'Node Type': 'ModifyTable', 'Operation': 'Update', 'Parallel Aware': False, 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 577, 'Plans': [{'Node Type': 'Index Scan', 'Parent Relationship': 'Member', 'Parallel Aware': False, 'Scan Direction': 'Forward', 'Index Name': 'customer_pkey', 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 577, 'Index Cond': '((c_w_id = 45) AND (c_d_id = 6) AND (c_id = 747))'}]}
[{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 45) AND (c_d_id = 6) AND (c_id = 747))'}, {'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}]
! 3 0.06189192086458206
{'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}
! 2 0.0614020861685276
{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 45) AND (c_d_id = 6) AND (c_id = 747))'}
! 0 0.0
empty
! 1 0.0
empty
* 5
[{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 45) AND (c_d_id = 6) AND (c_id = 747))'}, {'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}]
! 3 0.06189192086458206
{'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}
! 2 0.0614020861685276
{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 45) AND (c_d_id = 6) AND (c_id = 747))'}
! 0 0.0
empty
! 1 0.0
empty
======Top3 metric
total cpu usage_sys: 0.3929289290338247
dsk/total_read: 0.2511535146634985
total cpu usage_usr: 0.13523525130390637
igs:
[0, 147.59848022460938, 147.58058166503906, 147.60543823242188, 147.61224365234375, 147.61521911621094, 147.6109619140625, 147.60606384277344, 147.5990753173828, 147.6154327392578, 147.59848022460938, 147.5879364013672, 147.6109619140625, 147.61172485351562, 147.61268615722656, 147.60952758789062, 147.6151580810547, 147.61424255371094, 147.5995330810547, 147.61412048339844, 147.60302734375, 147.61988830566406, 147.61773681640625, 147.61204528808594, 147.60581970214844, 147.6202850341797, 147.61074829101562, 147.61935424804688, 147.60690307617188, 147.619140625, 147.61859130859375, 147.6175994873047, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Transformer(
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (2): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (3): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (4): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (5): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
    )
    (norm): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
  )
  (decoder): TransformerDecoder(
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (1): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (2): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (3): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (4): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (5): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
    )
    (norm): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
  )
)
Transformer(
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (2): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (3): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (4): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (5): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
    )
    (norm): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
  )
  (decoder): TransformerDecoder(
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (1): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (2): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (3): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (4): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (5): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
    )
    (norm): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
  )
)
[[[0.0, 0.0, 0.06167038902640343, 0.06192544475197792], [0.0, 0.0, 0.06167038902640343, 0.06192544475197792], [0.0, 0.0, 0.06167038902640343, 0.06192544475197792], [0.0, 0.0, 0.06167038902640343, 0.06192544475197792], [0.0, 0.0, 0.06167038902640343, 0.06192544475197792]], [[0.0, 0.0, 0.0614020861685276, 0.06189192086458206], [0.0, 0.0, 0.0614020861685276, 0.06189192086458206], [0.0, 0.0, 0.0614020861685276, 0.06189192086458206], [0.0, 0.0, 0.0614020861685276, 0.06189192086458206], [0.0, 0.0, 0.0614020861685276, 0.06189192086458206]]]
# 1
UPDATE CUSTOMER   SET C_BALANCE = $1,        C_YTD_PAYMENT = $2,        C_PAYMENT_CNT = $3,        C_DATA = $4  WHERE C_W_ID = $5    AND C_D_ID = $6    AND C_ID = $7
* 1
{'Node Type': 'ModifyTable', 'Operation': 'Update', 'Parallel Aware': False, 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 690, 'Plans': [{'Node Type': 'Index Scan', 'Parent Relationship': 'Member', 'Parallel Aware': False, 'Scan Direction': 'Forward', 'Index Name': 'customer_pkey', 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 690, 'Index Cond': '((c_w_id = 15) AND (c_d_id = 6) AND (c_id = 723))'}]}
[{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 15) AND (c_d_id = 6) AND (c_id = 723))'}, {'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}]
! 3 0.06192544475197792
{'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}
! 2 0.06167038902640343
{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 15) AND (c_d_id = 6) AND (c_id = 723))'}
! 0 0.0
empty
! 1 0.0
empty
* 2
{'Node Type': 'ModifyTable', 'Operation': 'Update', 'Parallel Aware': False, 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 690, 'Plans': [{'Node Type': 'Index Scan', 'Parent Relationship': 'Member', 'Parallel Aware': False, 'Scan Direction': 'Forward', 'Index Name': 'customer_pkey', 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 690, 'Index Cond': '((c_w_id = 25) AND (c_d_id = 1) AND (c_id = 299))'}]}
[{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 25) AND (c_d_id = 1) AND (c_id = 299))'}, {'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}]
! 3 0.06192544475197792
{'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}
! 2 0.06167038902640343
{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 25) AND (c_d_id = 1) AND (c_id = 299))'}
! 0 0.0
empty
! 1 0.0
empty
* 3
{'Node Type': 'ModifyTable', 'Operation': 'Update', 'Parallel Aware': False, 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 690, 'Plans': [{'Node Type': 'Index Scan', 'Parent Relationship': 'Member', 'Parallel Aware': False, 'Scan Direction': 'Forward', 'Index Name': 'customer_pkey', 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 690, 'Index Cond': '((c_w_id = 25) AND (c_d_id = 2) AND (c_id = 238))'}]}
[{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 25) AND (c_d_id = 2) AND (c_id = 238))'}, {'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}]
! 3 0.06192544475197792
{'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}
! 2 0.06167038902640343
{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 25) AND (c_d_id = 2) AND (c_id = 238))'}
! 0 0.0
empty
! 1 0.0
empty
* 4
{'Node Type': 'ModifyTable', 'Operation': 'Update', 'Parallel Aware': False, 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 690, 'Plans': [{'Node Type': 'Index Scan', 'Parent Relationship': 'Member', 'Parallel Aware': False, 'Scan Direction': 'Forward', 'Index Name': 'customer_pkey', 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 690, 'Index Cond': '((c_w_id = 30) AND (c_d_id = 6) AND (c_id = 231))'}]}
[{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 30) AND (c_d_id = 6) AND (c_id = 231))'}, {'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}]
! 3 0.06192544475197792
{'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}
! 2 0.06167038902640343
{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 30) AND (c_d_id = 6) AND (c_id = 231))'}
! 0 0.0
empty
! 1 0.0
empty
* 5
[{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 30) AND (c_d_id = 6) AND (c_id = 231))'}, {'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}]
! 3 0.06192544475197792
{'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}
! 2 0.06167038902640343
{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 30) AND (c_d_id = 6) AND (c_id = 231))'}
! 0 0.0
empty
! 1 0.0
empty
# 2
UPDATE CUSTOMER   SET C_BALANCE = $1,        C_YTD_PAYMENT = $2,        C_PAYMENT_CNT = $3  WHERE C_W_ID = $4    AND C_D_ID = $5    AND C_ID = $6
* 1
{'Node Type': 'ModifyTable', 'Operation': 'Update', 'Parallel Aware': False, 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 577, 'Plans': [{'Node Type': 'Index Scan', 'Parent Relationship': 'Member', 'Parallel Aware': False, 'Scan Direction': 'Forward', 'Index Name': 'customer_pkey', 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 577, 'Index Cond': '((c_w_id = 23) AND (c_d_id = 8) AND (c_id = 1019))'}]}
[{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 23) AND (c_d_id = 8) AND (c_id = 1019))'}, {'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}]
! 3 0.06189192086458206
{'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}
! 2 0.0614020861685276
{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 23) AND (c_d_id = 8) AND (c_id = 1019))'}
! 0 0.0
empty
! 1 0.0
empty
* 2
{'Node Type': 'ModifyTable', 'Operation': 'Update', 'Parallel Aware': False, 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 577, 'Plans': [{'Node Type': 'Index Scan', 'Parent Relationship': 'Member', 'Parallel Aware': False, 'Scan Direction': 'Forward', 'Index Name': 'customer_pkey', 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 577, 'Index Cond': '((c_w_id = 25) AND (c_d_id = 8) AND (c_id = 618))'}]}
[{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 25) AND (c_d_id = 8) AND (c_id = 618))'}, {'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}]
! 3 0.06189192086458206
{'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}
! 2 0.0614020861685276
{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 25) AND (c_d_id = 8) AND (c_id = 618))'}
! 0 0.0
empty
! 1 0.0
empty
* 3
{'Node Type': 'ModifyTable', 'Operation': 'Update', 'Parallel Aware': False, 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 577, 'Plans': [{'Node Type': 'Index Scan', 'Parent Relationship': 'Member', 'Parallel Aware': False, 'Scan Direction': 'Forward', 'Index Name': 'customer_pkey', 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 577, 'Index Cond': '((c_w_id = 30) AND (c_d_id = 4) AND (c_id = 950))'}]}
[{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 30) AND (c_d_id = 4) AND (c_id = 950))'}, {'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}]
! 3 0.06189192086458206
{'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}
! 2 0.0614020861685276
{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 30) AND (c_d_id = 4) AND (c_id = 950))'}
! 0 0.0
empty
! 1 0.0
empty
* 4
{'Node Type': 'ModifyTable', 'Operation': 'Update', 'Parallel Aware': False, 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 577, 'Plans': [{'Node Type': 'Index Scan', 'Parent Relationship': 'Member', 'Parallel Aware': False, 'Scan Direction': 'Forward', 'Index Name': 'customer_pkey', 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 577, 'Index Cond': '((c_w_id = 45) AND (c_d_id = 6) AND (c_id = 747))'}]}
[{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 45) AND (c_d_id = 6) AND (c_id = 747))'}, {'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}]
! 3 0.06189192086458206
{'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}
! 2 0.0614020861685276
{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 45) AND (c_d_id = 6) AND (c_id = 747))'}
! 0 0.0
empty
! 1 0.0
empty
* 5
[{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 45) AND (c_d_id = 6) AND (c_id = 747))'}, {'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}]
! 3 0.06189192086458206
{'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}
! 2 0.0614020861685276
{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 45) AND (c_d_id = 6) AND (c_id = 747))'}
! 0 0.0
empty
! 1 0.0
empty
======Top3 metric
total cpu usage_sys: 0.4170576705382567
dsk/total_read: 0.2586061683462716
total cpu usage_idl: 0.12198295683486639
igs:
[0, 219.9181671142578, 219.89149475097656, 219.9285430908203, 219.9386749267578, 219.943115234375, 219.936767578125, 219.92945861816406, 219.91905212402344, 219.9434356689453, 219.9181671142578, 219.9024658203125, 219.936767578125, 219.9379119873047, 219.93934631347656, 219.93463134765625, 219.94302368164062, 219.94166564941406, 219.91973876953125, 219.94146728515625, 219.92494201660156, 219.9500732421875, 219.94686889648438, 219.93836975097656, 219.92910766601562, 219.95065307617188, 219.9364471435547, 219.9492645263672, 219.9307098388672, 219.94895935058594, 219.94813537597656, 219.9466552734375, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Transformer(
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (2): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (3): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (4): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (5): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
    )
    (norm): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
  )
  (decoder): TransformerDecoder(
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (1): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (2): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (3): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (4): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (5): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
    )
    (norm): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
  )
)
Transformer(
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (2): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (3): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (4): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (5): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
    )
    (norm): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
  )
  (decoder): TransformerDecoder(
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (1): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (2): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (3): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (4): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (5): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
    )
    (norm): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
  )
)
[[[0.0, 0.0, 0.06167038902640343, 0.06192544475197792], [0.0, 0.0, 0.06167038902640343, 0.06192544475197792], [0.0, 0.0, 0.06167038902640343, 0.06192544475197792], [0.0, 0.0, 0.06167038902640343, 0.06192544475197792], [0.0, 0.0, 0.06167038902640343, 0.06192544475197792]], [[0.0, 0.0, 0.0614020861685276, 0.06189192086458206], [0.0, 0.0, 0.0614020861685276, 0.06189192086458206], [0.0, 0.0, 0.0614020861685276, 0.06189192086458206], [0.0, 0.0, 0.0614020861685276, 0.06189192086458206], [0.0, 0.0, 0.0614020861685276, 0.06189192086458206]]]
# 1
UPDATE CUSTOMER   SET C_BALANCE = $1,        C_YTD_PAYMENT = $2,        C_PAYMENT_CNT = $3,        C_DATA = $4  WHERE C_W_ID = $5    AND C_D_ID = $6    AND C_ID = $7
* 1
{'Node Type': 'ModifyTable', 'Operation': 'Update', 'Parallel Aware': False, 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 690, 'Plans': [{'Node Type': 'Index Scan', 'Parent Relationship': 'Member', 'Parallel Aware': False, 'Scan Direction': 'Forward', 'Index Name': 'customer_pkey', 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 690, 'Index Cond': '((c_w_id = 15) AND (c_d_id = 6) AND (c_id = 723))'}]}
[{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 15) AND (c_d_id = 6) AND (c_id = 723))'}, {'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}]
! 3 0.06192544475197792
{'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}
! 2 0.06167038902640343
{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 15) AND (c_d_id = 6) AND (c_id = 723))'}
! 0 0.0
empty
! 1 0.0
empty
* 2
{'Node Type': 'ModifyTable', 'Operation': 'Update', 'Parallel Aware': False, 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 690, 'Plans': [{'Node Type': 'Index Scan', 'Parent Relationship': 'Member', 'Parallel Aware': False, 'Scan Direction': 'Forward', 'Index Name': 'customer_pkey', 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 690, 'Index Cond': '((c_w_id = 25) AND (c_d_id = 1) AND (c_id = 299))'}]}
[{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 25) AND (c_d_id = 1) AND (c_id = 299))'}, {'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}]
! 3 0.06192544475197792
{'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}
! 2 0.06167038902640343
{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 25) AND (c_d_id = 1) AND (c_id = 299))'}
! 0 0.0
empty
! 1 0.0
empty
* 3
{'Node Type': 'ModifyTable', 'Operation': 'Update', 'Parallel Aware': False, 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 690, 'Plans': [{'Node Type': 'Index Scan', 'Parent Relationship': 'Member', 'Parallel Aware': False, 'Scan Direction': 'Forward', 'Index Name': 'customer_pkey', 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 690, 'Index Cond': '((c_w_id = 25) AND (c_d_id = 2) AND (c_id = 238))'}]}
[{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 25) AND (c_d_id = 2) AND (c_id = 238))'}, {'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}]
! 3 0.06192544475197792
{'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}
! 2 0.06167038902640343
{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 25) AND (c_d_id = 2) AND (c_id = 238))'}
! 0 0.0
empty
! 1 0.0
empty
* 4
{'Node Type': 'ModifyTable', 'Operation': 'Update', 'Parallel Aware': False, 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 690, 'Plans': [{'Node Type': 'Index Scan', 'Parent Relationship': 'Member', 'Parallel Aware': False, 'Scan Direction': 'Forward', 'Index Name': 'customer_pkey', 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 690, 'Index Cond': '((c_w_id = 30) AND (c_d_id = 6) AND (c_id = 231))'}]}
[{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 30) AND (c_d_id = 6) AND (c_id = 231))'}, {'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}]
! 3 0.06192544475197792
{'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}
! 2 0.06167038902640343
{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 30) AND (c_d_id = 6) AND (c_id = 231))'}
! 0 0.0
empty
! 1 0.0
empty
* 5
[{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 30) AND (c_d_id = 6) AND (c_id = 231))'}, {'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}]
! 3 0.06192544475197792
{'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}
! 2 0.06167038902640343
{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 30) AND (c_d_id = 6) AND (c_id = 231))'}
! 0 0.0
empty
! 1 0.0
empty
# 2
UPDATE CUSTOMER   SET C_BALANCE = $1,        C_YTD_PAYMENT = $2,        C_PAYMENT_CNT = $3  WHERE C_W_ID = $4    AND C_D_ID = $5    AND C_ID = $6
* 1
{'Node Type': 'ModifyTable', 'Operation': 'Update', 'Parallel Aware': False, 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 577, 'Plans': [{'Node Type': 'Index Scan', 'Parent Relationship': 'Member', 'Parallel Aware': False, 'Scan Direction': 'Forward', 'Index Name': 'customer_pkey', 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 577, 'Index Cond': '((c_w_id = 23) AND (c_d_id = 8) AND (c_id = 1019))'}]}
[{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 23) AND (c_d_id = 8) AND (c_id = 1019))'}, {'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}]
! 3 0.06189192086458206
{'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}
! 2 0.0614020861685276
{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 23) AND (c_d_id = 8) AND (c_id = 1019))'}
! 0 0.0
empty
! 1 0.0
empty
* 2
{'Node Type': 'ModifyTable', 'Operation': 'Update', 'Parallel Aware': False, 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 577, 'Plans': [{'Node Type': 'Index Scan', 'Parent Relationship': 'Member', 'Parallel Aware': False, 'Scan Direction': 'Forward', 'Index Name': 'customer_pkey', 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 577, 'Index Cond': '((c_w_id = 25) AND (c_d_id = 8) AND (c_id = 618))'}]}
[{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 25) AND (c_d_id = 8) AND (c_id = 618))'}, {'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}]
! 3 0.06189192086458206
{'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}
! 2 0.0614020861685276
{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 25) AND (c_d_id = 8) AND (c_id = 618))'}
! 0 0.0
empty
! 1 0.0
empty
* 3
{'Node Type': 'ModifyTable', 'Operation': 'Update', 'Parallel Aware': False, 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 577, 'Plans': [{'Node Type': 'Index Scan', 'Parent Relationship': 'Member', 'Parallel Aware': False, 'Scan Direction': 'Forward', 'Index Name': 'customer_pkey', 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 577, 'Index Cond': '((c_w_id = 30) AND (c_d_id = 4) AND (c_id = 950))'}]}
[{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 30) AND (c_d_id = 4) AND (c_id = 950))'}, {'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}]
! 3 0.06189192086458206
{'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}
! 2 0.0614020861685276
{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 30) AND (c_d_id = 4) AND (c_id = 950))'}
! 0 0.0
empty
! 1 0.0
empty
* 4
{'Node Type': 'ModifyTable', 'Operation': 'Update', 'Parallel Aware': False, 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 577, 'Plans': [{'Node Type': 'Index Scan', 'Parent Relationship': 'Member', 'Parallel Aware': False, 'Scan Direction': 'Forward', 'Index Name': 'customer_pkey', 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 577, 'Index Cond': '((c_w_id = 45) AND (c_d_id = 6) AND (c_id = 747))'}]}
[{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 45) AND (c_d_id = 6) AND (c_id = 747))'}, {'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}]
! 3 0.06189192086458206
{'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}
! 2 0.0614020861685276
{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 45) AND (c_d_id = 6) AND (c_id = 747))'}
! 0 0.0
empty
! 1 0.0
empty
* 5
[{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 45) AND (c_d_id = 6) AND (c_id = 747))'}, {'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}]
! 3 0.06189192086458206
{'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}
! 2 0.0614020861685276
{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 45) AND (c_d_id = 6) AND (c_id = 747))'}
! 0 0.0
empty
! 1 0.0
empty
======Top3 metric
total cpu usage_sys: 0.33272687745580176
dsk/total_read: 0.22776597093874232
total cpu usage_usr: 0.1337800779424548
igs:
[0, 19.500755310058594, 19.498390197753906, 19.50167465209961, 19.502573013305664, 19.502965927124023, 19.502405166625977, 19.50175666809082, 19.50083351135254, 19.502994537353516, 19.500755310058594, 19.49936294555664, 19.502405166625977, 19.502504348754883, 19.50263214111328, 19.502214431762695, 19.502958297729492, 19.502838134765625, 19.50089454650879, 19.50282096862793, 19.50135612487793, 19.503583908081055, 19.503299713134766, 19.502546310424805, 19.501724243164062, 19.50363540649414, 19.502376556396484, 19.50351333618164, 19.501867294311523, 19.50348472595215, 19.5034122467041, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Transformer(
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (2): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (3): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (4): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (5): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
    )
    (norm): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
  )
  (decoder): TransformerDecoder(
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (1): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (2): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (3): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (4): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (5): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
    )
    (norm): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
  )
)
Transformer(
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (2): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (3): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (4): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (5): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
    )
    (norm): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
  )
  (decoder): TransformerDecoder(
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (1): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (2): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (3): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (4): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (5): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
    )
    (norm): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
  )
)
[[[0.0, 0.0, 0.06167038902640343, 0.06192544475197792], [0.0, 0.0, 0.06167038902640343, 0.06192544475197792], [0.0, 0.0, 0.06167038902640343, 0.06192544475197792], [0.0, 0.0, 0.06167038902640343, 0.06192544475197792], [0.0, 0.0, 0.06167038902640343, 0.06192544475197792]], [[0.0, 0.0, 0.0614020861685276, 0.06189192086458206], [0.0, 0.0, 0.0614020861685276, 0.06189192086458206], [0.0, 0.0, 0.0614020861685276, 0.06189192086458206], [0.0, 0.0, 0.0614020861685276, 0.06189192086458206], [0.0, 0.0, 0.0614020861685276, 0.06189192086458206]]]
# 1
UPDATE CUSTOMER   SET C_BALANCE = $1,        C_YTD_PAYMENT = $2,        C_PAYMENT_CNT = $3,        C_DATA = $4  WHERE C_W_ID = $5    AND C_D_ID = $6    AND C_ID = $7
* 1
{'Node Type': 'ModifyTable', 'Operation': 'Update', 'Parallel Aware': False, 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 690, 'Plans': [{'Node Type': 'Index Scan', 'Parent Relationship': 'Member', 'Parallel Aware': False, 'Scan Direction': 'Forward', 'Index Name': 'customer_pkey', 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 690, 'Index Cond': '((c_w_id = 15) AND (c_d_id = 6) AND (c_id = 723))'}]}
[{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 15) AND (c_d_id = 6) AND (c_id = 723))'}, {'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}]
! 3 0.06192544475197792
{'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}
! 2 0.06167038902640343
{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 15) AND (c_d_id = 6) AND (c_id = 723))'}
! 0 0.0
empty
! 1 0.0
empty
* 2
{'Node Type': 'ModifyTable', 'Operation': 'Update', 'Parallel Aware': False, 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 690, 'Plans': [{'Node Type': 'Index Scan', 'Parent Relationship': 'Member', 'Parallel Aware': False, 'Scan Direction': 'Forward', 'Index Name': 'customer_pkey', 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 690, 'Index Cond': '((c_w_id = 25) AND (c_d_id = 1) AND (c_id = 299))'}]}
[{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 25) AND (c_d_id = 1) AND (c_id = 299))'}, {'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}]
! 3 0.06192544475197792
{'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}
! 2 0.06167038902640343
{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 25) AND (c_d_id = 1) AND (c_id = 299))'}
! 0 0.0
empty
! 1 0.0
empty
* 3
{'Node Type': 'ModifyTable', 'Operation': 'Update', 'Parallel Aware': False, 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 690, 'Plans': [{'Node Type': 'Index Scan', 'Parent Relationship': 'Member', 'Parallel Aware': False, 'Scan Direction': 'Forward', 'Index Name': 'customer_pkey', 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 690, 'Index Cond': '((c_w_id = 25) AND (c_d_id = 2) AND (c_id = 238))'}]}
[{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 25) AND (c_d_id = 2) AND (c_id = 238))'}, {'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}]
! 3 0.06192544475197792
{'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}
! 2 0.06167038902640343
{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 25) AND (c_d_id = 2) AND (c_id = 238))'}
! 0 0.0
empty
! 1 0.0
empty
* 4
{'Node Type': 'ModifyTable', 'Operation': 'Update', 'Parallel Aware': False, 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 690, 'Plans': [{'Node Type': 'Index Scan', 'Parent Relationship': 'Member', 'Parallel Aware': False, 'Scan Direction': 'Forward', 'Index Name': 'customer_pkey', 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 690, 'Index Cond': '((c_w_id = 30) AND (c_d_id = 6) AND (c_id = 231))'}]}
[{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 30) AND (c_d_id = 6) AND (c_id = 231))'}, {'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}]
! 3 0.06192544475197792
{'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}
! 2 0.06167038902640343
{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 30) AND (c_d_id = 6) AND (c_id = 231))'}
! 0 0.0
empty
! 1 0.0
empty
* 5
[{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 30) AND (c_d_id = 6) AND (c_id = 231))'}, {'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}]
! 3 0.06192544475197792
{'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}
! 2 0.06167038902640343
{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 30) AND (c_d_id = 6) AND (c_id = 231))'}
! 0 0.0
empty
! 1 0.0
empty
# 2
UPDATE CUSTOMER   SET C_BALANCE = $1,        C_YTD_PAYMENT = $2,        C_PAYMENT_CNT = $3  WHERE C_W_ID = $4    AND C_D_ID = $5    AND C_ID = $6
* 1
{'Node Type': 'ModifyTable', 'Operation': 'Update', 'Parallel Aware': False, 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 577, 'Plans': [{'Node Type': 'Index Scan', 'Parent Relationship': 'Member', 'Parallel Aware': False, 'Scan Direction': 'Forward', 'Index Name': 'customer_pkey', 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 577, 'Index Cond': '((c_w_id = 23) AND (c_d_id = 8) AND (c_id = 1019))'}]}
[{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 23) AND (c_d_id = 8) AND (c_id = 1019))'}, {'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}]
! 3 0.06189192086458206
{'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}
! 2 0.0614020861685276
{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 23) AND (c_d_id = 8) AND (c_id = 1019))'}
! 0 0.0
empty
! 1 0.0
empty
* 2
{'Node Type': 'ModifyTable', 'Operation': 'Update', 'Parallel Aware': False, 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 577, 'Plans': [{'Node Type': 'Index Scan', 'Parent Relationship': 'Member', 'Parallel Aware': False, 'Scan Direction': 'Forward', 'Index Name': 'customer_pkey', 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 577, 'Index Cond': '((c_w_id = 25) AND (c_d_id = 8) AND (c_id = 618))'}]}
[{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 25) AND (c_d_id = 8) AND (c_id = 618))'}, {'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}]
! 3 0.06189192086458206
{'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}
! 2 0.0614020861685276
{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 25) AND (c_d_id = 8) AND (c_id = 618))'}
! 0 0.0
empty
! 1 0.0
empty
* 3
{'Node Type': 'ModifyTable', 'Operation': 'Update', 'Parallel Aware': False, 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 577, 'Plans': [{'Node Type': 'Index Scan', 'Parent Relationship': 'Member', 'Parallel Aware': False, 'Scan Direction': 'Forward', 'Index Name': 'customer_pkey', 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 577, 'Index Cond': '((c_w_id = 30) AND (c_d_id = 4) AND (c_id = 950))'}]}
[{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 30) AND (c_d_id = 4) AND (c_id = 950))'}, {'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}]
! 3 0.06189192086458206
{'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}
! 2 0.0614020861685276
{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 30) AND (c_d_id = 4) AND (c_id = 950))'}
! 0 0.0
empty
! 1 0.0
empty
* 4
{'Node Type': 'ModifyTable', 'Operation': 'Update', 'Parallel Aware': False, 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 577, 'Plans': [{'Node Type': 'Index Scan', 'Parent Relationship': 'Member', 'Parallel Aware': False, 'Scan Direction': 'Forward', 'Index Name': 'customer_pkey', 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 577, 'Index Cond': '((c_w_id = 45) AND (c_d_id = 6) AND (c_id = 747))'}]}
[{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 45) AND (c_d_id = 6) AND (c_id = 747))'}, {'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}]
! 3 0.06189192086458206
{'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}
! 2 0.0614020861685276
{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 45) AND (c_d_id = 6) AND (c_id = 747))'}
! 0 0.0
empty
! 1 0.0
empty
* 5
[{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 45) AND (c_d_id = 6) AND (c_id = 747))'}, {'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}]
! 3 0.06189192086458206
{'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}
! 2 0.0614020861685276
{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 45) AND (c_d_id = 6) AND (c_id = 747))'}
! 0 0.0
empty
! 1 0.0
empty
======Top3 metric
total cpu usage_sys: 0.4040526332381016
dsk/total_read: 0.35904994447620425
total cpu usage_idl: 0.06749152688137702
igs:
[0, 62.57093811035156, 62.563350677490234, 62.573890686035156, 62.57677459716797, 62.57803726196289, 62.57623291015625, 62.57415008544922, 62.571189880371094, 62.578125, 62.57093811035156, 62.566471099853516, 62.57623291015625, 62.57655715942383, 62.57696533203125, 62.57562255859375, 62.57801055908203, 62.57762145996094, 62.57138442993164, 62.57756805419922, 62.57286834716797, 62.58001708984375, 62.5791015625, 62.57668685913086, 62.57405090332031, 62.58018112182617, 62.576141357421875, 62.57978820800781, 62.57450485229492, 62.57969665527344, 62.579463958740234, 62.579044342041016, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Transformer(
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (2): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (3): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (4): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (5): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
    )
    (norm): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
  )
  (decoder): TransformerDecoder(
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (1): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (2): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (3): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (4): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (5): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
    )
    (norm): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
  )
)
Transformer(
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (2): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (3): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (4): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (5): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
    )
    (norm): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
  )
  (decoder): TransformerDecoder(
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (1): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (2): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (3): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (4): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (5): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
    )
    (norm): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
  )
)
[[[0.0, 0.0, 0.06167038902640343, 0.06192544475197792], [0.0, 0.0, 0.06167038902640343, 0.06192544475197792], [0.0, 0.0, 0.06167038902640343, 0.06192544475197792], [0.0, 0.0, 0.06167038902640343, 0.06192544475197792], [0.0, 0.0, 0.06167038902640343, 0.06192544475197792]], [[0.0, 0.0, 0.0614020861685276, 0.06189192086458206], [0.0, 0.0, 0.0614020861685276, 0.06189192086458206], [0.0, 0.0, 0.0614020861685276, 0.06189192086458206], [0.0, 0.0, 0.0614020861685276, 0.06189192086458206], [0.0, 0.0, 0.0614020861685276, 0.06189192086458206]]]
# 1
UPDATE CUSTOMER   SET C_BALANCE = $1,        C_YTD_PAYMENT = $2,        C_PAYMENT_CNT = $3,        C_DATA = $4  WHERE C_W_ID = $5    AND C_D_ID = $6    AND C_ID = $7
* 1
{'Node Type': 'ModifyTable', 'Operation': 'Update', 'Parallel Aware': False, 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 690, 'Plans': [{'Node Type': 'Index Scan', 'Parent Relationship': 'Member', 'Parallel Aware': False, 'Scan Direction': 'Forward', 'Index Name': 'customer_pkey', 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 690, 'Index Cond': '((c_w_id = 15) AND (c_d_id = 6) AND (c_id = 723))'}]}
[{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 15) AND (c_d_id = 6) AND (c_id = 723))'}, {'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}]
! 3 0.06192544475197792
{'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}
! 2 0.06167038902640343
{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 15) AND (c_d_id = 6) AND (c_id = 723))'}
! 0 0.0
empty
! 1 0.0
empty
* 2
{'Node Type': 'ModifyTable', 'Operation': 'Update', 'Parallel Aware': False, 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 690, 'Plans': [{'Node Type': 'Index Scan', 'Parent Relationship': 'Member', 'Parallel Aware': False, 'Scan Direction': 'Forward', 'Index Name': 'customer_pkey', 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 690, 'Index Cond': '((c_w_id = 25) AND (c_d_id = 1) AND (c_id = 299))'}]}
[{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 25) AND (c_d_id = 1) AND (c_id = 299))'}, {'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}]
! 3 0.06192544475197792
{'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}
! 2 0.06167038902640343
{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 25) AND (c_d_id = 1) AND (c_id = 299))'}
! 0 0.0
empty
! 1 0.0
empty
* 3
{'Node Type': 'ModifyTable', 'Operation': 'Update', 'Parallel Aware': False, 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 690, 'Plans': [{'Node Type': 'Index Scan', 'Parent Relationship': 'Member', 'Parallel Aware': False, 'Scan Direction': 'Forward', 'Index Name': 'customer_pkey', 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 690, 'Index Cond': '((c_w_id = 25) AND (c_d_id = 2) AND (c_id = 238))'}]}
[{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 25) AND (c_d_id = 2) AND (c_id = 238))'}, {'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}]
! 3 0.06192544475197792
{'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}
! 2 0.06167038902640343
{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 25) AND (c_d_id = 2) AND (c_id = 238))'}
! 0 0.0
empty
! 1 0.0
empty
* 4
{'Node Type': 'ModifyTable', 'Operation': 'Update', 'Parallel Aware': False, 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 690, 'Plans': [{'Node Type': 'Index Scan', 'Parent Relationship': 'Member', 'Parallel Aware': False, 'Scan Direction': 'Forward', 'Index Name': 'customer_pkey', 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 690, 'Index Cond': '((c_w_id = 30) AND (c_d_id = 6) AND (c_id = 231))'}]}
[{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 30) AND (c_d_id = 6) AND (c_id = 231))'}, {'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}]
! 3 0.06192544475197792
{'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}
! 2 0.06167038902640343
{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 30) AND (c_d_id = 6) AND (c_id = 231))'}
! 0 0.0
empty
! 1 0.0
empty
* 5
[{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 30) AND (c_d_id = 6) AND (c_id = 231))'}, {'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}]
! 3 0.06192544475197792
{'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}
! 2 0.06167038902640343
{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 30) AND (c_d_id = 6) AND (c_id = 231))'}
! 0 0.0
empty
! 1 0.0
empty
# 2
UPDATE CUSTOMER   SET C_BALANCE = $1,        C_YTD_PAYMENT = $2,        C_PAYMENT_CNT = $3  WHERE C_W_ID = $4    AND C_D_ID = $5    AND C_ID = $6
* 1
{'Node Type': 'ModifyTable', 'Operation': 'Update', 'Parallel Aware': False, 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 577, 'Plans': [{'Node Type': 'Index Scan', 'Parent Relationship': 'Member', 'Parallel Aware': False, 'Scan Direction': 'Forward', 'Index Name': 'customer_pkey', 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 577, 'Index Cond': '((c_w_id = 23) AND (c_d_id = 8) AND (c_id = 1019))'}]}
[{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 23) AND (c_d_id = 8) AND (c_id = 1019))'}, {'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}]
! 3 0.06189192086458206
{'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}
! 2 0.0614020861685276
{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 23) AND (c_d_id = 8) AND (c_id = 1019))'}
! 0 0.0
empty
! 1 0.0
empty
* 2
{'Node Type': 'ModifyTable', 'Operation': 'Update', 'Parallel Aware': False, 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 577, 'Plans': [{'Node Type': 'Index Scan', 'Parent Relationship': 'Member', 'Parallel Aware': False, 'Scan Direction': 'Forward', 'Index Name': 'customer_pkey', 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 577, 'Index Cond': '((c_w_id = 25) AND (c_d_id = 8) AND (c_id = 618))'}]}
[{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 25) AND (c_d_id = 8) AND (c_id = 618))'}, {'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}]
! 3 0.06189192086458206
{'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}
! 2 0.0614020861685276
{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 25) AND (c_d_id = 8) AND (c_id = 618))'}
! 0 0.0
empty
! 1 0.0
empty
* 3
{'Node Type': 'ModifyTable', 'Operation': 'Update', 'Parallel Aware': False, 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 577, 'Plans': [{'Node Type': 'Index Scan', 'Parent Relationship': 'Member', 'Parallel Aware': False, 'Scan Direction': 'Forward', 'Index Name': 'customer_pkey', 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 577, 'Index Cond': '((c_w_id = 30) AND (c_d_id = 4) AND (c_id = 950))'}]}
[{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 30) AND (c_d_id = 4) AND (c_id = 950))'}, {'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}]
! 3 0.06189192086458206
{'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}
! 2 0.0614020861685276
{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 30) AND (c_d_id = 4) AND (c_id = 950))'}
! 0 0.0
empty
! 1 0.0
empty
* 4
{'Node Type': 'ModifyTable', 'Operation': 'Update', 'Parallel Aware': False, 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 577, 'Plans': [{'Node Type': 'Index Scan', 'Parent Relationship': 'Member', 'Parallel Aware': False, 'Scan Direction': 'Forward', 'Index Name': 'customer_pkey', 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 577, 'Index Cond': '((c_w_id = 45) AND (c_d_id = 6) AND (c_id = 747))'}]}
[{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 45) AND (c_d_id = 6) AND (c_id = 747))'}, {'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}]
! 3 0.06189192086458206
{'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}
! 2 0.0614020861685276
{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 45) AND (c_d_id = 6) AND (c_id = 747))'}
! 0 0.0
empty
! 1 0.0
empty
* 5
[{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 45) AND (c_d_id = 6) AND (c_id = 747))'}, {'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}]
! 3 0.06189192086458206
{'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}
! 2 0.0614020861685276
{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 45) AND (c_d_id = 6) AND (c_id = 747))'}
! 0 0.0
empty
! 1 0.0
empty
======Top3 metric
total cpu usage_sys: 0.4443169805653861
dsk/total_read: 0.26425603197901537
total cpu usage_idl: 0.12106967448813764
igs:
[0, 71.74685668945312, 71.73815155029297, 71.750244140625, 71.75354766845703, 71.75499725341797, 71.7529296875, 71.75054168701172, 71.74714660644531, 71.75509643554688, 71.74685668945312, 71.74172973632812, 71.7529296875, 71.7532958984375, 71.75376892089844, 71.75222778320312, 71.75496673583984, 71.75452423095703, 71.74736785888672, 71.75446319580078, 71.74906921386719, 71.75726318359375, 71.75621795654297, 71.75344848632812, 71.75042724609375, 71.75745391845703, 71.75282287597656, 71.75700378417969, 71.75094604492188, 71.75689697265625, 71.75662994384766, 71.75614929199219, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Transformer(
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (2): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (3): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (4): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (5): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
    )
    (norm): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
  )
  (decoder): TransformerDecoder(
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (1): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (2): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (3): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (4): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (5): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
    )
    (norm): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
  )
)
Transformer(
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (2): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (3): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (4): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (5): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
    )
    (norm): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
  )
  (decoder): TransformerDecoder(
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (1): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (2): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (3): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (4): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (5): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
    )
    (norm): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
  )
)
[[[0.0, 0.0, 0.06167038902640343, 0.06192544475197792], [0.0, 0.0, 0.06167038902640343, 0.06192544475197792], [0.0, 0.0, 0.06167038902640343, 0.06192544475197792], [0.0, 0.0, 0.06167038902640343, 0.06192544475197792], [0.0, 0.0, 0.06167038902640343, 0.06192544475197792]], [[0.0, 0.0, 0.0614020861685276, 0.06189192086458206], [0.0, 0.0, 0.0614020861685276, 0.06189192086458206], [0.0, 0.0, 0.0614020861685276, 0.06189192086458206], [0.0, 0.0, 0.0614020861685276, 0.06189192086458206], [0.0, 0.0, 0.0614020861685276, 0.06189192086458206]]]
# 1
UPDATE CUSTOMER   SET C_BALANCE = $1,        C_YTD_PAYMENT = $2,        C_PAYMENT_CNT = $3,        C_DATA = $4  WHERE C_W_ID = $5    AND C_D_ID = $6    AND C_ID = $7
* 1
{'Node Type': 'ModifyTable', 'Operation': 'Update', 'Parallel Aware': False, 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 690, 'Plans': [{'Node Type': 'Index Scan', 'Parent Relationship': 'Member', 'Parallel Aware': False, 'Scan Direction': 'Forward', 'Index Name': 'customer_pkey', 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 690, 'Index Cond': '((c_w_id = 15) AND (c_d_id = 6) AND (c_id = 723))'}]}
[{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 15) AND (c_d_id = 6) AND (c_id = 723))'}, {'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}]
! 3 0.06192544475197792
{'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}
! 2 0.06167038902640343
{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 15) AND (c_d_id = 6) AND (c_id = 723))'}
! 0 0.0
empty
! 1 0.0
empty
* 2
{'Node Type': 'ModifyTable', 'Operation': 'Update', 'Parallel Aware': False, 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 690, 'Plans': [{'Node Type': 'Index Scan', 'Parent Relationship': 'Member', 'Parallel Aware': False, 'Scan Direction': 'Forward', 'Index Name': 'customer_pkey', 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 690, 'Index Cond': '((c_w_id = 25) AND (c_d_id = 1) AND (c_id = 299))'}]}
[{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 25) AND (c_d_id = 1) AND (c_id = 299))'}, {'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}]
! 3 0.06192544475197792
{'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}
! 2 0.06167038902640343
{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 25) AND (c_d_id = 1) AND (c_id = 299))'}
! 0 0.0
empty
! 1 0.0
empty
* 3
{'Node Type': 'ModifyTable', 'Operation': 'Update', 'Parallel Aware': False, 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 690, 'Plans': [{'Node Type': 'Index Scan', 'Parent Relationship': 'Member', 'Parallel Aware': False, 'Scan Direction': 'Forward', 'Index Name': 'customer_pkey', 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 690, 'Index Cond': '((c_w_id = 25) AND (c_d_id = 2) AND (c_id = 238))'}]}
[{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 25) AND (c_d_id = 2) AND (c_id = 238))'}, {'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}]
! 3 0.06192544475197792
{'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}
! 2 0.06167038902640343
{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 25) AND (c_d_id = 2) AND (c_id = 238))'}
! 0 0.0
empty
! 1 0.0
empty
* 4
{'Node Type': 'ModifyTable', 'Operation': 'Update', 'Parallel Aware': False, 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 690, 'Plans': [{'Node Type': 'Index Scan', 'Parent Relationship': 'Member', 'Parallel Aware': False, 'Scan Direction': 'Forward', 'Index Name': 'customer_pkey', 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 690, 'Index Cond': '((c_w_id = 30) AND (c_d_id = 6) AND (c_id = 231))'}]}
[{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 30) AND (c_d_id = 6) AND (c_id = 231))'}, {'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}]
! 3 0.06192544475197792
{'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}
! 2 0.06167038902640343
{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 30) AND (c_d_id = 6) AND (c_id = 231))'}
! 0 0.0
empty
! 1 0.0
empty
* 5
[{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 30) AND (c_d_id = 6) AND (c_id = 231))'}, {'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}]
! 3 0.06192544475197792
{'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}
! 2 0.06167038902640343
{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 30) AND (c_d_id = 6) AND (c_id = 231))'}
! 0 0.0
empty
! 1 0.0
empty
# 2
UPDATE CUSTOMER   SET C_BALANCE = $1,        C_YTD_PAYMENT = $2,        C_PAYMENT_CNT = $3  WHERE C_W_ID = $4    AND C_D_ID = $5    AND C_ID = $6
* 1
{'Node Type': 'ModifyTable', 'Operation': 'Update', 'Parallel Aware': False, 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 577, 'Plans': [{'Node Type': 'Index Scan', 'Parent Relationship': 'Member', 'Parallel Aware': False, 'Scan Direction': 'Forward', 'Index Name': 'customer_pkey', 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 577, 'Index Cond': '((c_w_id = 23) AND (c_d_id = 8) AND (c_id = 1019))'}]}
[{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 23) AND (c_d_id = 8) AND (c_id = 1019))'}, {'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}]
! 3 0.06189192086458206
{'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}
! 2 0.0614020861685276
{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 23) AND (c_d_id = 8) AND (c_id = 1019))'}
! 0 0.0
empty
! 1 0.0
empty
* 2
{'Node Type': 'ModifyTable', 'Operation': 'Update', 'Parallel Aware': False, 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 577, 'Plans': [{'Node Type': 'Index Scan', 'Parent Relationship': 'Member', 'Parallel Aware': False, 'Scan Direction': 'Forward', 'Index Name': 'customer_pkey', 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 577, 'Index Cond': '((c_w_id = 25) AND (c_d_id = 8) AND (c_id = 618))'}]}
[{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 25) AND (c_d_id = 8) AND (c_id = 618))'}, {'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}]
! 3 0.06189192086458206
{'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}
! 2 0.0614020861685276
{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 25) AND (c_d_id = 8) AND (c_id = 618))'}
! 0 0.0
empty
! 1 0.0
empty
* 3
{'Node Type': 'ModifyTable', 'Operation': 'Update', 'Parallel Aware': False, 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 577, 'Plans': [{'Node Type': 'Index Scan', 'Parent Relationship': 'Member', 'Parallel Aware': False, 'Scan Direction': 'Forward', 'Index Name': 'customer_pkey', 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 577, 'Index Cond': '((c_w_id = 30) AND (c_d_id = 4) AND (c_id = 950))'}]}
[{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 30) AND (c_d_id = 4) AND (c_id = 950))'}, {'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}]
! 3 0.06189192086458206
{'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}
! 2 0.0614020861685276
{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 30) AND (c_d_id = 4) AND (c_id = 950))'}
! 0 0.0
empty
! 1 0.0
empty
* 4
{'Node Type': 'ModifyTable', 'Operation': 'Update', 'Parallel Aware': False, 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 577, 'Plans': [{'Node Type': 'Index Scan', 'Parent Relationship': 'Member', 'Parallel Aware': False, 'Scan Direction': 'Forward', 'Index Name': 'customer_pkey', 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 577, 'Index Cond': '((c_w_id = 45) AND (c_d_id = 6) AND (c_id = 747))'}]}
[{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 45) AND (c_d_id = 6) AND (c_id = 747))'}, {'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}]
! 3 0.06189192086458206
{'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}
! 2 0.0614020861685276
{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 45) AND (c_d_id = 6) AND (c_id = 747))'}
! 0 0.0
empty
! 1 0.0
empty
* 5
[{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 45) AND (c_d_id = 6) AND (c_id = 747))'}, {'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}]
! 3 0.06189192086458206
{'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}
! 2 0.0614020861685276
{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 45) AND (c_d_id = 6) AND (c_id = 747))'}
! 0 0.0
empty
! 1 0.0
empty
======Top3 metric
total cpu usage_sys: 0.4462735276808662
dsk/total_read: 0.24552693702512302
total cpu usage_idl: 0.11594664315491159
igs:
[0, 85.06745147705078, 85.05712890625, 85.07145690917969, 85.07537841796875, 85.07709503173828, 85.07464599609375, 85.07181549072266, 85.06778717041016, 85.07721710205078, 85.06745147705078, 85.06137084960938, 85.07464599609375, 85.07508087158203, 85.07563781738281, 85.07381439208984, 85.07705688476562, 85.07653045654297, 85.06805419921875, 85.07646179199219, 85.070068359375, 85.07978820800781, 85.07854461669922, 85.07526397705078, 85.0716781616211, 85.08000946044922, 85.07451629638672, 85.07947540283203, 85.07229614257812, 85.07935333251953, 85.07904052734375, 85.0784683227539, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Transformer(
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (2): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (3): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (4): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (5): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
    )
    (norm): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
  )
  (decoder): TransformerDecoder(
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (1): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (2): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (3): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (4): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (5): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
    )
    (norm): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
  )
)
Transformer(
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (2): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (3): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (4): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (5): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
    )
    (norm): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
  )
  (decoder): TransformerDecoder(
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (1): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (2): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (3): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (4): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (5): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
    )
    (norm): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
  )
)
[[[0.0, 0.0, 0.06167038902640343, 0.06192544475197792], [0.0, 0.0, 0.06167038902640343, 0.06192544475197792], [0.0, 0.0, 0.06167038902640343, 0.06192544475197792], [0.0, 0.0, 0.06167038902640343, 0.06192544475197792], [0.0, 0.0, 0.06167038902640343, 0.06192544475197792]], [[0.0, 0.0, 0.0614020861685276, 0.06189192086458206], [0.0, 0.0, 0.0614020861685276, 0.06189192086458206], [0.0, 0.0, 0.0614020861685276, 0.06189192086458206], [0.0, 0.0, 0.0614020861685276, 0.06189192086458206], [0.0, 0.0, 0.0614020861685276, 0.06189192086458206]]]
# 1
UPDATE CUSTOMER   SET C_BALANCE = $1,        C_YTD_PAYMENT = $2,        C_PAYMENT_CNT = $3,        C_DATA = $4  WHERE C_W_ID = $5    AND C_D_ID = $6    AND C_ID = $7
* 1
{'Node Type': 'ModifyTable', 'Operation': 'Update', 'Parallel Aware': False, 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 690, 'Plans': [{'Node Type': 'Index Scan', 'Parent Relationship': 'Member', 'Parallel Aware': False, 'Scan Direction': 'Forward', 'Index Name': 'customer_pkey', 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 690, 'Index Cond': '((c_w_id = 15) AND (c_d_id = 6) AND (c_id = 723))'}]}
[{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 15) AND (c_d_id = 6) AND (c_id = 723))'}, {'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}]
! 3 0.06192544475197792
{'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}
! 2 0.06167038902640343
{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 15) AND (c_d_id = 6) AND (c_id = 723))'}
! 0 0.0
empty
! 1 0.0
empty
* 2
{'Node Type': 'ModifyTable', 'Operation': 'Update', 'Parallel Aware': False, 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 690, 'Plans': [{'Node Type': 'Index Scan', 'Parent Relationship': 'Member', 'Parallel Aware': False, 'Scan Direction': 'Forward', 'Index Name': 'customer_pkey', 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 690, 'Index Cond': '((c_w_id = 25) AND (c_d_id = 1) AND (c_id = 299))'}]}
[{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 25) AND (c_d_id = 1) AND (c_id = 299))'}, {'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}]
! 3 0.06192544475197792
{'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}
! 2 0.06167038902640343
{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 25) AND (c_d_id = 1) AND (c_id = 299))'}
! 0 0.0
empty
! 1 0.0
empty
* 3
{'Node Type': 'ModifyTable', 'Operation': 'Update', 'Parallel Aware': False, 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 690, 'Plans': [{'Node Type': 'Index Scan', 'Parent Relationship': 'Member', 'Parallel Aware': False, 'Scan Direction': 'Forward', 'Index Name': 'customer_pkey', 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 690, 'Index Cond': '((c_w_id = 25) AND (c_d_id = 2) AND (c_id = 238))'}]}
[{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 25) AND (c_d_id = 2) AND (c_id = 238))'}, {'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}]
! 3 0.06192544475197792
{'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}
! 2 0.06167038902640343
{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 25) AND (c_d_id = 2) AND (c_id = 238))'}
! 0 0.0
empty
! 1 0.0
empty
* 4
{'Node Type': 'ModifyTable', 'Operation': 'Update', 'Parallel Aware': False, 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 690, 'Plans': [{'Node Type': 'Index Scan', 'Parent Relationship': 'Member', 'Parallel Aware': False, 'Scan Direction': 'Forward', 'Index Name': 'customer_pkey', 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 690, 'Index Cond': '((c_w_id = 30) AND (c_d_id = 6) AND (c_id = 231))'}]}
[{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 30) AND (c_d_id = 6) AND (c_id = 231))'}, {'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}]
! 3 0.06192544475197792
{'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}
! 2 0.06167038902640343
{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 30) AND (c_d_id = 6) AND (c_id = 231))'}
! 0 0.0
empty
! 1 0.0
empty
* 5
[{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 30) AND (c_d_id = 6) AND (c_id = 231))'}, {'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}]
! 3 0.06192544475197792
{'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}
! 2 0.06167038902640343
{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 30) AND (c_d_id = 6) AND (c_id = 231))'}
! 0 0.0
empty
! 1 0.0
empty
# 2
UPDATE CUSTOMER   SET C_BALANCE = $1,        C_YTD_PAYMENT = $2,        C_PAYMENT_CNT = $3  WHERE C_W_ID = $4    AND C_D_ID = $5    AND C_ID = $6
* 1
{'Node Type': 'ModifyTable', 'Operation': 'Update', 'Parallel Aware': False, 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 577, 'Plans': [{'Node Type': 'Index Scan', 'Parent Relationship': 'Member', 'Parallel Aware': False, 'Scan Direction': 'Forward', 'Index Name': 'customer_pkey', 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 577, 'Index Cond': '((c_w_id = 23) AND (c_d_id = 8) AND (c_id = 1019))'}]}
[{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 23) AND (c_d_id = 8) AND (c_id = 1019))'}, {'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}]
! 3 0.06189192086458206
{'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}
! 2 0.0614020861685276
{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 23) AND (c_d_id = 8) AND (c_id = 1019))'}
! 0 0.0
empty
! 1 0.0
empty
* 2
{'Node Type': 'ModifyTable', 'Operation': 'Update', 'Parallel Aware': False, 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 577, 'Plans': [{'Node Type': 'Index Scan', 'Parent Relationship': 'Member', 'Parallel Aware': False, 'Scan Direction': 'Forward', 'Index Name': 'customer_pkey', 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 577, 'Index Cond': '((c_w_id = 25) AND (c_d_id = 8) AND (c_id = 618))'}]}
[{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 25) AND (c_d_id = 8) AND (c_id = 618))'}, {'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}]
! 3 0.06189192086458206
{'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}
! 2 0.0614020861685276
{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 25) AND (c_d_id = 8) AND (c_id = 618))'}
! 0 0.0
empty
! 1 0.0
empty
* 3
{'Node Type': 'ModifyTable', 'Operation': 'Update', 'Parallel Aware': False, 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 577, 'Plans': [{'Node Type': 'Index Scan', 'Parent Relationship': 'Member', 'Parallel Aware': False, 'Scan Direction': 'Forward', 'Index Name': 'customer_pkey', 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 577, 'Index Cond': '((c_w_id = 30) AND (c_d_id = 4) AND (c_id = 950))'}]}
[{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 30) AND (c_d_id = 4) AND (c_id = 950))'}, {'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}]
! 3 0.06189192086458206
{'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}
! 2 0.0614020861685276
{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 30) AND (c_d_id = 4) AND (c_id = 950))'}
! 0 0.0
empty
! 1 0.0
empty
* 4
{'Node Type': 'ModifyTable', 'Operation': 'Update', 'Parallel Aware': False, 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 577, 'Plans': [{'Node Type': 'Index Scan', 'Parent Relationship': 'Member', 'Parallel Aware': False, 'Scan Direction': 'Forward', 'Index Name': 'customer_pkey', 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 577, 'Index Cond': '((c_w_id = 45) AND (c_d_id = 6) AND (c_id = 747))'}]}
[{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 45) AND (c_d_id = 6) AND (c_id = 747))'}, {'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}]
! 3 0.06189192086458206
{'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}
! 2 0.0614020861685276
{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 45) AND (c_d_id = 6) AND (c_id = 747))'}
! 0 0.0
empty
! 1 0.0
empty
* 5
[{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 45) AND (c_d_id = 6) AND (c_id = 747))'}, {'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}]
! 3 0.06189192086458206
{'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}
! 2 0.0614020861685276
{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 45) AND (c_d_id = 6) AND (c_id = 747))'}
! 0 0.0
empty
! 1 0.0
empty
======Top3 metric
total cpu usage_sys: 0.4541609111654036
dsk/total_read: 0.36016312793374
total cpu usage_usr: 0.03572243635330171
igs:
[0, 44.36769104003906, 44.36231231689453, 44.36978530883789, 44.371829986572266, 44.37272644042969, 44.37144470214844, 44.36996841430664, 44.36787033081055, 44.37278747558594, 44.36769104003906, 44.364524841308594, 44.37144470214844, 44.371673583984375, 44.37196350097656, 44.37101364135742, 44.37270736694336, 44.372432708740234, 44.36800765991211, 44.37239456176758, 44.36906051635742, 44.37413024902344, 44.37348175048828, 44.371768951416016, 44.36989974975586, 44.374244689941406, 44.37137985229492, 44.373966217041016, 44.37022018432617, 44.3739013671875, 44.37373733520508, 44.37343978881836, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Transformer(
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (2): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (3): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (4): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (5): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
    )
    (norm): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
  )
  (decoder): TransformerDecoder(
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (1): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (2): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (3): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (4): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (5): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
    )
    (norm): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
  )
)
Transformer(
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (2): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (3): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (4): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (5): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
    )
    (norm): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
  )
  (decoder): TransformerDecoder(
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (1): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (2): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (3): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (4): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (5): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
    )
    (norm): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
  )
)
[[[0.0, 0.0, 0.06167038902640343, 0.06192544475197792], [0.0, 0.0, 0.06167038902640343, 0.06192544475197792], [0.0, 0.0, 0.06167038902640343, 0.06192544475197792], [0.0, 0.0, 0.06167038902640343, 0.06192544475197792], [0.0, 0.0, 0.06167038902640343, 0.06192544475197792]], [[0.0, 0.0, 0.0614020861685276, 0.06189192086458206], [0.0, 0.0, 0.0614020861685276, 0.06189192086458206], [0.0, 0.0, 0.0614020861685276, 0.06189192086458206], [0.0, 0.0, 0.0614020861685276, 0.06189192086458206], [0.0, 0.0, 0.0614020861685276, 0.06189192086458206]]]
# 1
UPDATE CUSTOMER   SET C_BALANCE = $1,        C_YTD_PAYMENT = $2,        C_PAYMENT_CNT = $3,        C_DATA = $4  WHERE C_W_ID = $5    AND C_D_ID = $6    AND C_ID = $7
* 1
{'Node Type': 'ModifyTable', 'Operation': 'Update', 'Parallel Aware': False, 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 690, 'Plans': [{'Node Type': 'Index Scan', 'Parent Relationship': 'Member', 'Parallel Aware': False, 'Scan Direction': 'Forward', 'Index Name': 'customer_pkey', 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 690, 'Index Cond': '((c_w_id = 15) AND (c_d_id = 6) AND (c_id = 723))'}]}
[{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 15) AND (c_d_id = 6) AND (c_id = 723))'}, {'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}]
! 3 0.06192544475197792
{'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}
! 2 0.06167038902640343
{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 15) AND (c_d_id = 6) AND (c_id = 723))'}
! 0 0.0
empty
! 1 0.0
empty
* 2
{'Node Type': 'ModifyTable', 'Operation': 'Update', 'Parallel Aware': False, 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 690, 'Plans': [{'Node Type': 'Index Scan', 'Parent Relationship': 'Member', 'Parallel Aware': False, 'Scan Direction': 'Forward', 'Index Name': 'customer_pkey', 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 690, 'Index Cond': '((c_w_id = 25) AND (c_d_id = 1) AND (c_id = 299))'}]}
[{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 25) AND (c_d_id = 1) AND (c_id = 299))'}, {'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}]
! 3 0.06192544475197792
{'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}
! 2 0.06167038902640343
{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 25) AND (c_d_id = 1) AND (c_id = 299))'}
! 0 0.0
empty
! 1 0.0
empty
* 3
{'Node Type': 'ModifyTable', 'Operation': 'Update', 'Parallel Aware': False, 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 690, 'Plans': [{'Node Type': 'Index Scan', 'Parent Relationship': 'Member', 'Parallel Aware': False, 'Scan Direction': 'Forward', 'Index Name': 'customer_pkey', 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 690, 'Index Cond': '((c_w_id = 25) AND (c_d_id = 2) AND (c_id = 238))'}]}
[{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 25) AND (c_d_id = 2) AND (c_id = 238))'}, {'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}]
! 3 0.06192544475197792
{'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}
! 2 0.06167038902640343
{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 25) AND (c_d_id = 2) AND (c_id = 238))'}
! 0 0.0
empty
! 1 0.0
empty
* 4
{'Node Type': 'ModifyTable', 'Operation': 'Update', 'Parallel Aware': False, 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 690, 'Plans': [{'Node Type': 'Index Scan', 'Parent Relationship': 'Member', 'Parallel Aware': False, 'Scan Direction': 'Forward', 'Index Name': 'customer_pkey', 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 690, 'Index Cond': '((c_w_id = 30) AND (c_d_id = 6) AND (c_id = 231))'}]}
[{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 30) AND (c_d_id = 6) AND (c_id = 231))'}, {'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}]
! 3 0.06192544475197792
{'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}
! 2 0.06167038902640343
{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 30) AND (c_d_id = 6) AND (c_id = 231))'}
! 0 0.0
empty
! 1 0.0
empty
* 5
[{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 30) AND (c_d_id = 6) AND (c_id = 231))'}, {'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}]
! 3 0.06192544475197792
{'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}
! 2 0.06167038902640343
{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 30) AND (c_d_id = 6) AND (c_id = 231))'}
! 0 0.0
empty
! 1 0.0
empty
# 2
UPDATE CUSTOMER   SET C_BALANCE = $1,        C_YTD_PAYMENT = $2,        C_PAYMENT_CNT = $3  WHERE C_W_ID = $4    AND C_D_ID = $5    AND C_ID = $6
* 1
{'Node Type': 'ModifyTable', 'Operation': 'Update', 'Parallel Aware': False, 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 577, 'Plans': [{'Node Type': 'Index Scan', 'Parent Relationship': 'Member', 'Parallel Aware': False, 'Scan Direction': 'Forward', 'Index Name': 'customer_pkey', 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 577, 'Index Cond': '((c_w_id = 23) AND (c_d_id = 8) AND (c_id = 1019))'}]}
[{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 23) AND (c_d_id = 8) AND (c_id = 1019))'}, {'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}]
! 3 0.06189192086458206
{'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}
! 2 0.0614020861685276
{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 23) AND (c_d_id = 8) AND (c_id = 1019))'}
! 0 0.0
empty
! 1 0.0
empty
* 2
{'Node Type': 'ModifyTable', 'Operation': 'Update', 'Parallel Aware': False, 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 577, 'Plans': [{'Node Type': 'Index Scan', 'Parent Relationship': 'Member', 'Parallel Aware': False, 'Scan Direction': 'Forward', 'Index Name': 'customer_pkey', 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 577, 'Index Cond': '((c_w_id = 25) AND (c_d_id = 8) AND (c_id = 618))'}]}
[{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 25) AND (c_d_id = 8) AND (c_id = 618))'}, {'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}]
! 3 0.06189192086458206
{'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}
! 2 0.0614020861685276
{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 25) AND (c_d_id = 8) AND (c_id = 618))'}
! 0 0.0
empty
! 1 0.0
empty
* 3
{'Node Type': 'ModifyTable', 'Operation': 'Update', 'Parallel Aware': False, 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 577, 'Plans': [{'Node Type': 'Index Scan', 'Parent Relationship': 'Member', 'Parallel Aware': False, 'Scan Direction': 'Forward', 'Index Name': 'customer_pkey', 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 577, 'Index Cond': '((c_w_id = 30) AND (c_d_id = 4) AND (c_id = 950))'}]}
[{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 30) AND (c_d_id = 4) AND (c_id = 950))'}, {'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}]
! 3 0.06189192086458206
{'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}
! 2 0.0614020861685276
{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 30) AND (c_d_id = 4) AND (c_id = 950))'}
! 0 0.0
empty
! 1 0.0
empty
* 4
{'Node Type': 'ModifyTable', 'Operation': 'Update', 'Parallel Aware': False, 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 577, 'Plans': [{'Node Type': 'Index Scan', 'Parent Relationship': 'Member', 'Parallel Aware': False, 'Scan Direction': 'Forward', 'Index Name': 'customer_pkey', 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 577, 'Index Cond': '((c_w_id = 45) AND (c_d_id = 6) AND (c_id = 747))'}]}
[{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 45) AND (c_d_id = 6) AND (c_id = 747))'}, {'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}]
! 3 0.06189192086458206
{'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}
! 2 0.0614020861685276
{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 45) AND (c_d_id = 6) AND (c_id = 747))'}
! 0 0.0
empty
! 1 0.0
empty
* 5
[{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 45) AND (c_d_id = 6) AND (c_id = 747))'}, {'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}]
! 3 0.06189192086458206
{'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}
! 2 0.0614020861685276
{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 45) AND (c_d_id = 6) AND (c_id = 747))'}
! 0 0.0
empty
! 1 0.0
empty
======Top3 metric
total cpu usage_sys: 0.4661724651328028
dsk/total_read: 0.24133769527827345
total cpu usage_idl: 0.11200123306253974
igs:
[0, 71.19200897216797, 71.1833724975586, 71.19536590576172, 71.19864654541016, 71.20008087158203, 71.19802856445312, 71.19566345214844, 71.19229125976562, 71.20018768310547, 71.19200897216797, 71.18692016601562, 71.19802856445312, 71.19839477539062, 71.19886016845703, 71.19733428955078, 71.2000503540039, 71.1996078491211, 71.19251251220703, 71.19954681396484, 71.19419860839844, 71.20233154296875, 71.2012939453125, 71.19854736328125, 71.19554901123047, 71.20252227783203, 71.19792938232422, 71.20207214355469, 71.1960678100586, 71.20197296142578, 71.20170593261719, 71.20122528076172, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Transformer(
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (2): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (3): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (4): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (5): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
    )
    (norm): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
  )
  (decoder): TransformerDecoder(
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (1): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (2): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (3): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (4): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (5): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
    )
    (norm): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
  )
)
Transformer(
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (2): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (3): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (4): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (5): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
    )
    (norm): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
  )
  (decoder): TransformerDecoder(
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (1): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (2): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (3): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (4): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (5): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
    )
    (norm): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
  )
)
[[[0.0, 0.0, 0.06167038902640343, 0.06192544475197792], [0.0, 0.0, 0.06167038902640343, 0.06192544475197792], [0.0, 0.0, 0.06167038902640343, 0.06192544475197792], [0.0, 0.0, 0.06167038902640343, 0.06192544475197792], [0.0, 0.0, 0.06167038902640343, 0.06192544475197792]], [[0.0, 0.0, 0.0614020861685276, 0.06189192086458206], [0.0, 0.0, 0.0614020861685276, 0.06189192086458206], [0.0, 0.0, 0.0614020861685276, 0.06189192086458206], [0.0, 0.0, 0.0614020861685276, 0.06189192086458206], [0.0, 0.0, 0.0614020861685276, 0.06189192086458206]]]
# 1
UPDATE CUSTOMER   SET C_BALANCE = $1,        C_YTD_PAYMENT = $2,        C_PAYMENT_CNT = $3,        C_DATA = $4  WHERE C_W_ID = $5    AND C_D_ID = $6    AND C_ID = $7
* 1
{'Node Type': 'ModifyTable', 'Operation': 'Update', 'Parallel Aware': False, 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 690, 'Plans': [{'Node Type': 'Index Scan', 'Parent Relationship': 'Member', 'Parallel Aware': False, 'Scan Direction': 'Forward', 'Index Name': 'customer_pkey', 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 690, 'Index Cond': '((c_w_id = 15) AND (c_d_id = 6) AND (c_id = 723))'}]}
[{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 15) AND (c_d_id = 6) AND (c_id = 723))'}, {'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}]
! 3 0.06192544475197792
{'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}
! 2 0.06167038902640343
{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 15) AND (c_d_id = 6) AND (c_id = 723))'}
! 0 0.0
empty
! 1 0.0
empty
* 2
{'Node Type': 'ModifyTable', 'Operation': 'Update', 'Parallel Aware': False, 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 690, 'Plans': [{'Node Type': 'Index Scan', 'Parent Relationship': 'Member', 'Parallel Aware': False, 'Scan Direction': 'Forward', 'Index Name': 'customer_pkey', 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 690, 'Index Cond': '((c_w_id = 25) AND (c_d_id = 1) AND (c_id = 299))'}]}
[{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 25) AND (c_d_id = 1) AND (c_id = 299))'}, {'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}]
! 3 0.06192544475197792
{'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}
! 2 0.06167038902640343
{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 25) AND (c_d_id = 1) AND (c_id = 299))'}
! 0 0.0
empty
! 1 0.0
empty
* 3
{'Node Type': 'ModifyTable', 'Operation': 'Update', 'Parallel Aware': False, 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 690, 'Plans': [{'Node Type': 'Index Scan', 'Parent Relationship': 'Member', 'Parallel Aware': False, 'Scan Direction': 'Forward', 'Index Name': 'customer_pkey', 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 690, 'Index Cond': '((c_w_id = 25) AND (c_d_id = 2) AND (c_id = 238))'}]}
[{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 25) AND (c_d_id = 2) AND (c_id = 238))'}, {'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}]
! 3 0.06192544475197792
{'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}
! 2 0.06167038902640343
{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 25) AND (c_d_id = 2) AND (c_id = 238))'}
! 0 0.0
empty
! 1 0.0
empty
* 4
{'Node Type': 'ModifyTable', 'Operation': 'Update', 'Parallel Aware': False, 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 690, 'Plans': [{'Node Type': 'Index Scan', 'Parent Relationship': 'Member', 'Parallel Aware': False, 'Scan Direction': 'Forward', 'Index Name': 'customer_pkey', 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 690, 'Index Cond': '((c_w_id = 30) AND (c_d_id = 6) AND (c_id = 231))'}]}
[{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 30) AND (c_d_id = 6) AND (c_id = 231))'}, {'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}]
! 3 0.06192544475197792
{'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}
! 2 0.06167038902640343
{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 30) AND (c_d_id = 6) AND (c_id = 231))'}
! 0 0.0
empty
! 1 0.0
empty
* 5
[{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 30) AND (c_d_id = 6) AND (c_id = 231))'}, {'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}]
! 3 0.06192544475197792
{'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}
! 2 0.06167038902640343
{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 30) AND (c_d_id = 6) AND (c_id = 231))'}
! 0 0.0
empty
! 1 0.0
empty
# 2
UPDATE CUSTOMER   SET C_BALANCE = $1,        C_YTD_PAYMENT = $2,        C_PAYMENT_CNT = $3  WHERE C_W_ID = $4    AND C_D_ID = $5    AND C_ID = $6
* 1
{'Node Type': 'ModifyTable', 'Operation': 'Update', 'Parallel Aware': False, 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 577, 'Plans': [{'Node Type': 'Index Scan', 'Parent Relationship': 'Member', 'Parallel Aware': False, 'Scan Direction': 'Forward', 'Index Name': 'customer_pkey', 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 577, 'Index Cond': '((c_w_id = 23) AND (c_d_id = 8) AND (c_id = 1019))'}]}
[{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 23) AND (c_d_id = 8) AND (c_id = 1019))'}, {'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}]
! 3 0.06189192086458206
{'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}
! 2 0.0614020861685276
{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 23) AND (c_d_id = 8) AND (c_id = 1019))'}
! 0 0.0
empty
! 1 0.0
empty
* 2
{'Node Type': 'ModifyTable', 'Operation': 'Update', 'Parallel Aware': False, 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 577, 'Plans': [{'Node Type': 'Index Scan', 'Parent Relationship': 'Member', 'Parallel Aware': False, 'Scan Direction': 'Forward', 'Index Name': 'customer_pkey', 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 577, 'Index Cond': '((c_w_id = 25) AND (c_d_id = 8) AND (c_id = 618))'}]}
[{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 25) AND (c_d_id = 8) AND (c_id = 618))'}, {'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}]
! 3 0.06189192086458206
{'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}
! 2 0.0614020861685276
{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 25) AND (c_d_id = 8) AND (c_id = 618))'}
! 0 0.0
empty
! 1 0.0
empty
* 3
{'Node Type': 'ModifyTable', 'Operation': 'Update', 'Parallel Aware': False, 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 577, 'Plans': [{'Node Type': 'Index Scan', 'Parent Relationship': 'Member', 'Parallel Aware': False, 'Scan Direction': 'Forward', 'Index Name': 'customer_pkey', 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 577, 'Index Cond': '((c_w_id = 30) AND (c_d_id = 4) AND (c_id = 950))'}]}
[{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 30) AND (c_d_id = 4) AND (c_id = 950))'}, {'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}]
! 3 0.06189192086458206
{'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}
! 2 0.0614020861685276
{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 30) AND (c_d_id = 4) AND (c_id = 950))'}
! 0 0.0
empty
! 1 0.0
empty
* 4
{'Node Type': 'ModifyTable', 'Operation': 'Update', 'Parallel Aware': False, 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 577, 'Plans': [{'Node Type': 'Index Scan', 'Parent Relationship': 'Member', 'Parallel Aware': False, 'Scan Direction': 'Forward', 'Index Name': 'customer_pkey', 'Relation Name': 'customer', 'Alias': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 577, 'Index Cond': '((c_w_id = 45) AND (c_d_id = 6) AND (c_id = 747))'}]}
[{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 45) AND (c_d_id = 6) AND (c_id = 747))'}, {'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}]
! 3 0.06189192086458206
{'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}
! 2 0.0614020861685276
{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 45) AND (c_d_id = 6) AND (c_id = 747))'}
! 0 0.0
empty
! 1 0.0
empty
* 5
[{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 45) AND (c_d_id = 6) AND (c_id = 747))'}, {'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}]
! 3 0.06189192086458206
{'Node Type': 'ModifyTable', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45}
! 2 0.0614020861685276
{'Node Type': 'Index Scan', 'Relation Name': 'customer', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': '((c_w_id = 45) AND (c_d_id = 6) AND (c_id = 747))'}
! 0 0.0
empty
! 1 0.0
empty
======Top3 metric
total cpu usage_sys: 0.5080548486369146
dsk/total_read: 0.2767132250063369
total cpu usage_usr: 0.0817702203300358
igs:
[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 52.2121467590332, 52.2121467590332, 52.208335876464844, 52.208335876464844, 52.21168518066406, 52.21168518066406, 52.21168518066406, 52.211692810058594]
Transformer(
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (2): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (3): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (4): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (5): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
    )
    (norm): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
  )
  (decoder): TransformerDecoder(
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (1): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (2): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (3): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (4): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (5): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
    )
    (norm): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
  )
)
Transformer(
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (2): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (3): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (4): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (5): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
    )
    (norm): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
  )
  (decoder): TransformerDecoder(
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (1): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (2): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (3): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (4): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (5): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
    )
    (norm): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
  )
)
[[[0.0, 0.0, 0.0, 0.3938876986503601], [0.0, 0.0, 0.0, 0.3938876986503601], [0.0, 0.0, 0.0, 0.3938876986503601], [0.0, 0.0, 0.0, 0.3938876986503601], [0.0, 0.0, 0.0, 0.3938876986503601]], [[0.0, 0.0, 0.0, 0.3938876986503601], [0.0, 0.0, 0.0, 0.3938876986503601], [0.0, 0.0, 0.0, 0.3938876986503601], [0.0, 0.0, 0.0, 0.3938876986503601], [0.0, 0.0, 0.0, 0.3938876986503601]]]
# 1
SELECT * FROM ACCOUNTS WHERE custid = $1
* 1
{'Node Type': 'Index Scan', 'Parallel Aware': False, 'Scan Direction': 'Forward', 'Index Name': 'pk_accounts', 'Relation Name': 'accounts', 'Alias': 'accounts', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 73, 'Index Cond': "(custid = '4091455'::bigint)"}
[{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': "(custid = '4091455'::bigint)"}]
! 3 0.3938876986503601
{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': "(custid = '4091455'::bigint)"}
! 0 0.0
empty
! 1 0.0
empty
! 2 0.0
empty
* 2
{'Node Type': 'Index Scan', 'Parallel Aware': False, 'Scan Direction': 'Forward', 'Index Name': 'pk_accounts', 'Relation Name': 'accounts', 'Alias': 'accounts', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 73, 'Index Cond': "(custid = '8258886'::bigint)"}
[{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': "(custid = '8258886'::bigint)"}]
! 3 0.3938876986503601
{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': "(custid = '8258886'::bigint)"}
! 0 0.0
empty
! 1 0.0
empty
! 2 0.0
empty
* 3
[{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': "(custid = '8258886'::bigint)"}]
! 3 0.3938876986503601
{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': "(custid = '8258886'::bigint)"}
! 0 0.0
empty
! 1 0.0
empty
! 2 0.0
empty
* 4
[{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': "(custid = '8258886'::bigint)"}]
! 3 0.3938876986503601
{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': "(custid = '8258886'::bigint)"}
! 0 0.0
empty
! 1 0.0
empty
! 2 0.0
empty
* 5
[{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': "(custid = '8258886'::bigint)"}]
! 3 0.3938876986503601
{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': "(custid = '8258886'::bigint)"}
! 0 0.0
empty
! 1 0.0
empty
! 2 0.0
empty
# 2
SELECT * FROM ACCOUNTS WHERE name = $1
* 1
{'Node Type': 'Index Scan', 'Parallel Aware': False, 'Scan Direction': 'Forward', 'Index Name': 'idx_accounts_name', 'Relation Name': 'accounts', 'Alias': 'accounts', 'Startup Cost': 0.56, 'Total Cost': 8.58, 'Plan Rows': 1, 'Plan Width': 73, 'Index Cond': "((name)::text = '0000000000000000000000000000000000000000000000000000000009338014'::text)"}
[{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.56, 'Total Cost': 8.58, 'Index Cond': "((name)::text = '0000000000000000000000000000000000000000000000000000000009338014'::text)"}]
! 3 0.3938876986503601
{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.56, 'Total Cost': 8.58, 'Index Cond': "((name)::text = '0000000000000000000000000000000000000000000000000000000009338014'::text)"}
! 0 0.0
empty
! 1 0.0
empty
! 2 0.0
empty
* 2
{'Node Type': 'Index Scan', 'Parallel Aware': False, 'Scan Direction': 'Forward', 'Index Name': 'idx_accounts_name', 'Relation Name': 'accounts', 'Alias': 'accounts', 'Startup Cost': 0.56, 'Total Cost': 8.58, 'Plan Rows': 1, 'Plan Width': 73, 'Index Cond': "((name)::text = '0000000000000000000000000000000000000000000000000000000006872584'::text)"}
[{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.56, 'Total Cost': 8.58, 'Index Cond': "((name)::text = '0000000000000000000000000000000000000000000000000000000006872584'::text)"}]
! 3 0.3938876986503601
{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.56, 'Total Cost': 8.58, 'Index Cond': "((name)::text = '0000000000000000000000000000000000000000000000000000000006872584'::text)"}
! 0 0.0
empty
! 1 0.0
empty
! 2 0.0
empty
* 3
[{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.56, 'Total Cost': 8.58, 'Index Cond': "((name)::text = '0000000000000000000000000000000000000000000000000000000006872584'::text)"}]
! 3 0.3938876986503601
{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.56, 'Total Cost': 8.58, 'Index Cond': "((name)::text = '0000000000000000000000000000000000000000000000000000000006872584'::text)"}
! 0 0.0
empty
! 1 0.0
empty
! 2 0.0
empty
* 4
[{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.56, 'Total Cost': 8.58, 'Index Cond': "((name)::text = '0000000000000000000000000000000000000000000000000000000006872584'::text)"}]
! 3 0.3938876986503601
{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.56, 'Total Cost': 8.58, 'Index Cond': "((name)::text = '0000000000000000000000000000000000000000000000000000000006872584'::text)"}
! 0 0.0
empty
! 1 0.0
empty
! 2 0.0
empty
* 5
[{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.56, 'Total Cost': 8.58, 'Index Cond': "((name)::text = '0000000000000000000000000000000000000000000000000000000006872584'::text)"}]
! 3 0.3938876986503601
{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.56, 'Total Cost': 8.58, 'Index Cond': "((name)::text = '0000000000000000000000000000000000000000000000000000000006872584'::text)"}
! 0 0.0
empty
! 1 0.0
empty
! 2 0.0
empty
======Top3 metric
total cpu usage_sys: 0.4135292655460013
dsk/total_read: 0.2686196968107357
total cpu usage_usr: 0.16562840048914845
igs:
[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 60.956146240234375, 60.956146240234375, 60.951698303222656, 60.951698303222656, 60.95560836791992, 60.95560836791992, 60.95560836791992, 60.95561599731445]
Transformer(
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (2): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (3): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (4): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (5): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
    )
    (norm): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
  )
  (decoder): TransformerDecoder(
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (1): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (2): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (3): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (4): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (5): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
    )
    (norm): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
  )
)
Transformer(
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (2): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (3): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (4): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (5): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
    )
    (norm): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
  )
  (decoder): TransformerDecoder(
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (1): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (2): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (3): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (4): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (5): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
    )
    (norm): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
  )
)
[[[0.0, 0.0, 0.0, 0.3938876986503601], [0.0, 0.0, 0.0, 0.3938876986503601], [0.0, 0.0, 0.0, 0.3938876986503601], [0.0, 0.0, 0.0, 0.3938876986503601], [0.0, 0.0, 0.0, 0.3938876986503601]], [[0.0, 0.0, 0.0, 0.3938876986503601], [0.0, 0.0, 0.0, 0.3938876986503601], [0.0, 0.0, 0.0, 0.3938876986503601], [0.0, 0.0, 0.0, 0.3938876986503601], [0.0, 0.0, 0.0, 0.3938876986503601]]]
# 1
SELECT * FROM ACCOUNTS WHERE custid = $1
* 1
{'Node Type': 'Index Scan', 'Parallel Aware': False, 'Scan Direction': 'Forward', 'Index Name': 'pk_accounts', 'Relation Name': 'accounts', 'Alias': 'accounts', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 73, 'Index Cond': "(custid = '4091455'::bigint)"}
[{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': "(custid = '4091455'::bigint)"}]
! 3 0.3938876986503601
{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': "(custid = '4091455'::bigint)"}
! 0 0.0
empty
! 1 0.0
empty
! 2 0.0
empty
* 2
{'Node Type': 'Index Scan', 'Parallel Aware': False, 'Scan Direction': 'Forward', 'Index Name': 'pk_accounts', 'Relation Name': 'accounts', 'Alias': 'accounts', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 73, 'Index Cond': "(custid = '8258886'::bigint)"}
[{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': "(custid = '8258886'::bigint)"}]
! 3 0.3938876986503601
{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': "(custid = '8258886'::bigint)"}
! 0 0.0
empty
! 1 0.0
empty
! 2 0.0
empty
* 3
[{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': "(custid = '8258886'::bigint)"}]
! 3 0.3938876986503601
{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': "(custid = '8258886'::bigint)"}
! 0 0.0
empty
! 1 0.0
empty
! 2 0.0
empty
* 4
[{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': "(custid = '8258886'::bigint)"}]
! 3 0.3938876986503601
{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': "(custid = '8258886'::bigint)"}
! 0 0.0
empty
! 1 0.0
empty
! 2 0.0
empty
* 5
[{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': "(custid = '8258886'::bigint)"}]
! 3 0.3938876986503601
{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': "(custid = '8258886'::bigint)"}
! 0 0.0
empty
! 1 0.0
empty
! 2 0.0
empty
# 2
SELECT * FROM ACCOUNTS WHERE name = $1
* 1
{'Node Type': 'Index Scan', 'Parallel Aware': False, 'Scan Direction': 'Forward', 'Index Name': 'idx_accounts_name', 'Relation Name': 'accounts', 'Alias': 'accounts', 'Startup Cost': 0.56, 'Total Cost': 8.58, 'Plan Rows': 1, 'Plan Width': 73, 'Index Cond': "((name)::text = '0000000000000000000000000000000000000000000000000000000009338014'::text)"}
[{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.56, 'Total Cost': 8.58, 'Index Cond': "((name)::text = '0000000000000000000000000000000000000000000000000000000009338014'::text)"}]
! 3 0.3938876986503601
{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.56, 'Total Cost': 8.58, 'Index Cond': "((name)::text = '0000000000000000000000000000000000000000000000000000000009338014'::text)"}
! 0 0.0
empty
! 1 0.0
empty
! 2 0.0
empty
* 2
{'Node Type': 'Index Scan', 'Parallel Aware': False, 'Scan Direction': 'Forward', 'Index Name': 'idx_accounts_name', 'Relation Name': 'accounts', 'Alias': 'accounts', 'Startup Cost': 0.56, 'Total Cost': 8.58, 'Plan Rows': 1, 'Plan Width': 73, 'Index Cond': "((name)::text = '0000000000000000000000000000000000000000000000000000000006872584'::text)"}
[{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.56, 'Total Cost': 8.58, 'Index Cond': "((name)::text = '0000000000000000000000000000000000000000000000000000000006872584'::text)"}]
! 3 0.3938876986503601
{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.56, 'Total Cost': 8.58, 'Index Cond': "((name)::text = '0000000000000000000000000000000000000000000000000000000006872584'::text)"}
! 0 0.0
empty
! 1 0.0
empty
! 2 0.0
empty
* 3
[{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.56, 'Total Cost': 8.58, 'Index Cond': "((name)::text = '0000000000000000000000000000000000000000000000000000000006872584'::text)"}]
! 3 0.3938876986503601
{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.56, 'Total Cost': 8.58, 'Index Cond': "((name)::text = '0000000000000000000000000000000000000000000000000000000006872584'::text)"}
! 0 0.0
empty
! 1 0.0
empty
! 2 0.0
empty
* 4
[{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.56, 'Total Cost': 8.58, 'Index Cond': "((name)::text = '0000000000000000000000000000000000000000000000000000000006872584'::text)"}]
! 3 0.3938876986503601
{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.56, 'Total Cost': 8.58, 'Index Cond': "((name)::text = '0000000000000000000000000000000000000000000000000000000006872584'::text)"}
! 0 0.0
empty
! 1 0.0
empty
! 2 0.0
empty
* 5
[{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.56, 'Total Cost': 8.58, 'Index Cond': "((name)::text = '0000000000000000000000000000000000000000000000000000000006872584'::text)"}]
! 3 0.3938876986503601
{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.56, 'Total Cost': 8.58, 'Index Cond': "((name)::text = '0000000000000000000000000000000000000000000000000000000006872584'::text)"}
! 0 0.0
empty
! 1 0.0
empty
! 2 0.0
empty
======Top3 metric
total cpu usage_sys: 0.41850833555735345
dsk/total_read: 0.2873966970887326
total cpu usage_usr: 0.1321748729634406
igs:
[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 75.5293197631836, 75.5293197631836, 75.52381134033203, 75.52381134033203, 75.52865600585938, 75.52865600585938, 75.52865600585938, 75.5286636352539]
Transformer(
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (2): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (3): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (4): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (5): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
    )
    (norm): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
  )
  (decoder): TransformerDecoder(
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (1): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (2): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (3): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (4): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (5): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
    )
    (norm): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
  )
)
Transformer(
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (2): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (3): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (4): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (5): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
    )
    (norm): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
  )
  (decoder): TransformerDecoder(
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (1): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (2): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (3): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (4): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (5): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
    )
    (norm): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
  )
)
[[[0.0, 0.0, 0.0, 0.3938876986503601], [0.0, 0.0, 0.0, 0.3938876986503601], [0.0, 0.0, 0.0, 0.3938876986503601], [0.0, 0.0, 0.0, 0.3938876986503601], [0.0, 0.0, 0.0, 0.3938876986503601]], [[0.0, 0.0, 0.0, 0.3938876986503601], [0.0, 0.0, 0.0, 0.3938876986503601], [0.0, 0.0, 0.0, 0.3938876986503601], [0.0, 0.0, 0.0, 0.3938876986503601], [0.0, 0.0, 0.0, 0.3938876986503601]]]
# 1
SELECT * FROM ACCOUNTS WHERE custid = $1
* 1
{'Node Type': 'Index Scan', 'Parallel Aware': False, 'Scan Direction': 'Forward', 'Index Name': 'pk_accounts', 'Relation Name': 'accounts', 'Alias': 'accounts', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 73, 'Index Cond': "(custid = '4091455'::bigint)"}
[{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': "(custid = '4091455'::bigint)"}]
! 3 0.3938876986503601
{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': "(custid = '4091455'::bigint)"}
! 0 0.0
empty
! 1 0.0
empty
! 2 0.0
empty
* 2
{'Node Type': 'Index Scan', 'Parallel Aware': False, 'Scan Direction': 'Forward', 'Index Name': 'pk_accounts', 'Relation Name': 'accounts', 'Alias': 'accounts', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 73, 'Index Cond': "(custid = '8258886'::bigint)"}
[{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': "(custid = '8258886'::bigint)"}]
! 3 0.3938876986503601
{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': "(custid = '8258886'::bigint)"}
! 0 0.0
empty
! 1 0.0
empty
! 2 0.0
empty
* 3
[{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': "(custid = '8258886'::bigint)"}]
! 3 0.3938876986503601
{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': "(custid = '8258886'::bigint)"}
! 0 0.0
empty
! 1 0.0
empty
! 2 0.0
empty
* 4
[{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': "(custid = '8258886'::bigint)"}]
! 3 0.3938876986503601
{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': "(custid = '8258886'::bigint)"}
! 0 0.0
empty
! 1 0.0
empty
! 2 0.0
empty
* 5
[{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': "(custid = '8258886'::bigint)"}]
! 3 0.3938876986503601
{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': "(custid = '8258886'::bigint)"}
! 0 0.0
empty
! 1 0.0
empty
! 2 0.0
empty
# 2
SELECT * FROM ACCOUNTS WHERE name = $1
* 1
{'Node Type': 'Index Scan', 'Parallel Aware': False, 'Scan Direction': 'Forward', 'Index Name': 'idx_accounts_name', 'Relation Name': 'accounts', 'Alias': 'accounts', 'Startup Cost': 0.56, 'Total Cost': 8.58, 'Plan Rows': 1, 'Plan Width': 73, 'Index Cond': "((name)::text = '0000000000000000000000000000000000000000000000000000000009338014'::text)"}
[{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.56, 'Total Cost': 8.58, 'Index Cond': "((name)::text = '0000000000000000000000000000000000000000000000000000000009338014'::text)"}]
! 3 0.3938876986503601
{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.56, 'Total Cost': 8.58, 'Index Cond': "((name)::text = '0000000000000000000000000000000000000000000000000000000009338014'::text)"}
! 0 0.0
empty
! 1 0.0
empty
! 2 0.0
empty
* 2
{'Node Type': 'Index Scan', 'Parallel Aware': False, 'Scan Direction': 'Forward', 'Index Name': 'idx_accounts_name', 'Relation Name': 'accounts', 'Alias': 'accounts', 'Startup Cost': 0.56, 'Total Cost': 8.58, 'Plan Rows': 1, 'Plan Width': 73, 'Index Cond': "((name)::text = '0000000000000000000000000000000000000000000000000000000006872584'::text)"}
[{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.56, 'Total Cost': 8.58, 'Index Cond': "((name)::text = '0000000000000000000000000000000000000000000000000000000006872584'::text)"}]
! 3 0.3938876986503601
{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.56, 'Total Cost': 8.58, 'Index Cond': "((name)::text = '0000000000000000000000000000000000000000000000000000000006872584'::text)"}
! 0 0.0
empty
! 1 0.0
empty
! 2 0.0
empty
* 3
[{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.56, 'Total Cost': 8.58, 'Index Cond': "((name)::text = '0000000000000000000000000000000000000000000000000000000006872584'::text)"}]
! 3 0.3938876986503601
{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.56, 'Total Cost': 8.58, 'Index Cond': "((name)::text = '0000000000000000000000000000000000000000000000000000000006872584'::text)"}
! 0 0.0
empty
! 1 0.0
empty
! 2 0.0
empty
* 4
[{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.56, 'Total Cost': 8.58, 'Index Cond': "((name)::text = '0000000000000000000000000000000000000000000000000000000006872584'::text)"}]
! 3 0.3938876986503601
{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.56, 'Total Cost': 8.58, 'Index Cond': "((name)::text = '0000000000000000000000000000000000000000000000000000000006872584'::text)"}
! 0 0.0
empty
! 1 0.0
empty
! 2 0.0
empty
* 5
[{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.56, 'Total Cost': 8.58, 'Index Cond': "((name)::text = '0000000000000000000000000000000000000000000000000000000006872584'::text)"}]
! 3 0.3938876986503601
{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.56, 'Total Cost': 8.58, 'Index Cond': "((name)::text = '0000000000000000000000000000000000000000000000000000000006872584'::text)"}
! 0 0.0
empty
! 1 0.0
empty
! 2 0.0
empty
======Top3 metric
total cpu usage_sys: 0.46586822765906716
dsk/total_read: 0.28999939752437603
total cpu usage_usr: 0.08323234513158427
igs:
[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 11.437004089355469, 11.437004089355469, 11.436169624328613, 11.436169624328613, 11.43690299987793, 11.43690299987793, 11.43690299987793, 11.436904907226562]
Transformer(
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (2): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (3): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (4): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (5): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
    )
    (norm): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
  )
  (decoder): TransformerDecoder(
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (1): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (2): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (3): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (4): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (5): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
    )
    (norm): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
  )
)
Transformer(
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (2): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (3): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (4): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (5): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
    )
    (norm): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
  )
  (decoder): TransformerDecoder(
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (1): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (2): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (3): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (4): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (5): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
    )
    (norm): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
  )
)
[[[0.0, 0.0, 0.0, 0.3938876986503601], [0.0, 0.0, 0.0, 0.3938876986503601], [0.0, 0.0, 0.0, 0.3938876986503601], [0.0, 0.0, 0.0, 0.3938876986503601], [0.0, 0.0, 0.0, 0.3938876986503601]], [[0.0, 0.0, 0.0, 0.3938876986503601], [0.0, 0.0, 0.0, 0.3938876986503601], [0.0, 0.0, 0.0, 0.3938876986503601], [0.0, 0.0, 0.0, 0.3938876986503601], [0.0, 0.0, 0.0, 0.3938876986503601]]]
# 1
SELECT * FROM ACCOUNTS WHERE custid = $1
* 1
{'Node Type': 'Index Scan', 'Parallel Aware': False, 'Scan Direction': 'Forward', 'Index Name': 'pk_accounts', 'Relation Name': 'accounts', 'Alias': 'accounts', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 73, 'Index Cond': "(custid = '4091455'::bigint)"}
[{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': "(custid = '4091455'::bigint)"}]
! 3 0.3938876986503601
{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': "(custid = '4091455'::bigint)"}
! 0 0.0
empty
! 1 0.0
empty
! 2 0.0
empty
* 2
{'Node Type': 'Index Scan', 'Parallel Aware': False, 'Scan Direction': 'Forward', 'Index Name': 'pk_accounts', 'Relation Name': 'accounts', 'Alias': 'accounts', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 73, 'Index Cond': "(custid = '8258886'::bigint)"}
[{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': "(custid = '8258886'::bigint)"}]
! 3 0.3938876986503601
{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': "(custid = '8258886'::bigint)"}
! 0 0.0
empty
! 1 0.0
empty
! 2 0.0
empty
* 3
[{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': "(custid = '8258886'::bigint)"}]
! 3 0.3938876986503601
{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': "(custid = '8258886'::bigint)"}
! 0 0.0
empty
! 1 0.0
empty
! 2 0.0
empty
* 4
[{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': "(custid = '8258886'::bigint)"}]
! 3 0.3938876986503601
{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': "(custid = '8258886'::bigint)"}
! 0 0.0
empty
! 1 0.0
empty
! 2 0.0
empty
* 5
[{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': "(custid = '8258886'::bigint)"}]
! 3 0.3938876986503601
{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': "(custid = '8258886'::bigint)"}
! 0 0.0
empty
! 1 0.0
empty
! 2 0.0
empty
# 2
SELECT * FROM ACCOUNTS WHERE name = $1
* 1
{'Node Type': 'Index Scan', 'Parallel Aware': False, 'Scan Direction': 'Forward', 'Index Name': 'idx_accounts_name', 'Relation Name': 'accounts', 'Alias': 'accounts', 'Startup Cost': 0.56, 'Total Cost': 8.58, 'Plan Rows': 1, 'Plan Width': 73, 'Index Cond': "((name)::text = '0000000000000000000000000000000000000000000000000000000009338014'::text)"}
[{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.56, 'Total Cost': 8.58, 'Index Cond': "((name)::text = '0000000000000000000000000000000000000000000000000000000009338014'::text)"}]
! 3 0.3938876986503601
{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.56, 'Total Cost': 8.58, 'Index Cond': "((name)::text = '0000000000000000000000000000000000000000000000000000000009338014'::text)"}
! 0 0.0
empty
! 1 0.0
empty
! 2 0.0
empty
* 2
{'Node Type': 'Index Scan', 'Parallel Aware': False, 'Scan Direction': 'Forward', 'Index Name': 'idx_accounts_name', 'Relation Name': 'accounts', 'Alias': 'accounts', 'Startup Cost': 0.56, 'Total Cost': 8.58, 'Plan Rows': 1, 'Plan Width': 73, 'Index Cond': "((name)::text = '0000000000000000000000000000000000000000000000000000000006872584'::text)"}
[{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.56, 'Total Cost': 8.58, 'Index Cond': "((name)::text = '0000000000000000000000000000000000000000000000000000000006872584'::text)"}]
! 3 0.3938876986503601
{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.56, 'Total Cost': 8.58, 'Index Cond': "((name)::text = '0000000000000000000000000000000000000000000000000000000006872584'::text)"}
! 0 0.0
empty
! 1 0.0
empty
! 2 0.0
empty
* 3
[{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.56, 'Total Cost': 8.58, 'Index Cond': "((name)::text = '0000000000000000000000000000000000000000000000000000000006872584'::text)"}]
! 3 0.3938876986503601
{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.56, 'Total Cost': 8.58, 'Index Cond': "((name)::text = '0000000000000000000000000000000000000000000000000000000006872584'::text)"}
! 0 0.0
empty
! 1 0.0
empty
! 2 0.0
empty
* 4
[{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.56, 'Total Cost': 8.58, 'Index Cond': "((name)::text = '0000000000000000000000000000000000000000000000000000000006872584'::text)"}]
! 3 0.3938876986503601
{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.56, 'Total Cost': 8.58, 'Index Cond': "((name)::text = '0000000000000000000000000000000000000000000000000000000006872584'::text)"}
! 0 0.0
empty
! 1 0.0
empty
! 2 0.0
empty
* 5
[{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.56, 'Total Cost': 8.58, 'Index Cond': "((name)::text = '0000000000000000000000000000000000000000000000000000000006872584'::text)"}]
! 3 0.3938876986503601
{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.56, 'Total Cost': 8.58, 'Index Cond': "((name)::text = '0000000000000000000000000000000000000000000000000000000006872584'::text)"}
! 0 0.0
empty
! 1 0.0
empty
! 2 0.0
empty
======Top3 metric
total cpu usage_sys: 0.3176387902781678
dsk/total_read: 0.24857746223265037
total cpu usage_usr: 0.18371618440233212
igs:
[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4.728923320770264, 4.728923320770264, 4.728578090667725, 4.728578090667725, 4.728881359100342, 4.728881359100342, 4.728881359100342, 4.728882312774658]
Transformer(
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (2): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (3): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (4): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (5): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
    )
    (norm): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
  )
  (decoder): TransformerDecoder(
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (1): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (2): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (3): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (4): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (5): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
    )
    (norm): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
  )
)
Transformer(
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (2): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (3): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (4): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (5): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
    )
    (norm): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
  )
  (decoder): TransformerDecoder(
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (1): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (2): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (3): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (4): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (5): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
    )
    (norm): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
  )
)
[[[0.0, 0.0, 0.0, 0.3938876986503601], [0.0, 0.0, 0.0, 0.3938876986503601], [0.0, 0.0, 0.0, 0.3938876986503601], [0.0, 0.0, 0.0, 0.3938876986503601], [0.0, 0.0, 0.0, 0.3938876986503601]], [[0.0, 0.0, 0.0, 0.3938876986503601], [0.0, 0.0, 0.0, 0.3938876986503601], [0.0, 0.0, 0.0, 0.3938876986503601], [0.0, 0.0, 0.0, 0.3938876986503601], [0.0, 0.0, 0.0, 0.3938876986503601]]]
# 1
SELECT * FROM ACCOUNTS WHERE custid = $1
* 1
{'Node Type': 'Index Scan', 'Parallel Aware': False, 'Scan Direction': 'Forward', 'Index Name': 'pk_accounts', 'Relation Name': 'accounts', 'Alias': 'accounts', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 73, 'Index Cond': "(custid = '4091455'::bigint)"}
[{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': "(custid = '4091455'::bigint)"}]
! 3 0.3938876986503601
{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': "(custid = '4091455'::bigint)"}
! 0 0.0
empty
! 1 0.0
empty
! 2 0.0
empty
* 2
{'Node Type': 'Index Scan', 'Parallel Aware': False, 'Scan Direction': 'Forward', 'Index Name': 'pk_accounts', 'Relation Name': 'accounts', 'Alias': 'accounts', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 73, 'Index Cond': "(custid = '8258886'::bigint)"}
[{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': "(custid = '8258886'::bigint)"}]
! 3 0.3938876986503601
{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': "(custid = '8258886'::bigint)"}
! 0 0.0
empty
! 1 0.0
empty
! 2 0.0
empty
* 3
[{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': "(custid = '8258886'::bigint)"}]
! 3 0.3938876986503601
{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': "(custid = '8258886'::bigint)"}
! 0 0.0
empty
! 1 0.0
empty
! 2 0.0
empty
* 4
[{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': "(custid = '8258886'::bigint)"}]
! 3 0.3938876986503601
{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': "(custid = '8258886'::bigint)"}
! 0 0.0
empty
! 1 0.0
empty
! 2 0.0
empty
* 5
[{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': "(custid = '8258886'::bigint)"}]
! 3 0.3938876986503601
{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': "(custid = '8258886'::bigint)"}
! 0 0.0
empty
! 1 0.0
empty
! 2 0.0
empty
# 2
SELECT * FROM ACCOUNTS WHERE name = $1
* 1
{'Node Type': 'Index Scan', 'Parallel Aware': False, 'Scan Direction': 'Forward', 'Index Name': 'idx_accounts_name', 'Relation Name': 'accounts', 'Alias': 'accounts', 'Startup Cost': 0.56, 'Total Cost': 8.58, 'Plan Rows': 1, 'Plan Width': 73, 'Index Cond': "((name)::text = '0000000000000000000000000000000000000000000000000000000009338014'::text)"}
[{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.56, 'Total Cost': 8.58, 'Index Cond': "((name)::text = '0000000000000000000000000000000000000000000000000000000009338014'::text)"}]
! 3 0.3938876986503601
{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.56, 'Total Cost': 8.58, 'Index Cond': "((name)::text = '0000000000000000000000000000000000000000000000000000000009338014'::text)"}
! 0 0.0
empty
! 1 0.0
empty
! 2 0.0
empty
* 2
{'Node Type': 'Index Scan', 'Parallel Aware': False, 'Scan Direction': 'Forward', 'Index Name': 'idx_accounts_name', 'Relation Name': 'accounts', 'Alias': 'accounts', 'Startup Cost': 0.56, 'Total Cost': 8.58, 'Plan Rows': 1, 'Plan Width': 73, 'Index Cond': "((name)::text = '0000000000000000000000000000000000000000000000000000000006872584'::text)"}
[{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.56, 'Total Cost': 8.58, 'Index Cond': "((name)::text = '0000000000000000000000000000000000000000000000000000000006872584'::text)"}]
! 3 0.3938876986503601
{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.56, 'Total Cost': 8.58, 'Index Cond': "((name)::text = '0000000000000000000000000000000000000000000000000000000006872584'::text)"}
! 0 0.0
empty
! 1 0.0
empty
! 2 0.0
empty
* 3
[{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.56, 'Total Cost': 8.58, 'Index Cond': "((name)::text = '0000000000000000000000000000000000000000000000000000000006872584'::text)"}]
! 3 0.3938876986503601
{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.56, 'Total Cost': 8.58, 'Index Cond': "((name)::text = '0000000000000000000000000000000000000000000000000000000006872584'::text)"}
! 0 0.0
empty
! 1 0.0
empty
! 2 0.0
empty
* 4
[{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.56, 'Total Cost': 8.58, 'Index Cond': "((name)::text = '0000000000000000000000000000000000000000000000000000000006872584'::text)"}]
! 3 0.3938876986503601
{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.56, 'Total Cost': 8.58, 'Index Cond': "((name)::text = '0000000000000000000000000000000000000000000000000000000006872584'::text)"}
! 0 0.0
empty
! 1 0.0
empty
! 2 0.0
empty
* 5
[{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.56, 'Total Cost': 8.58, 'Index Cond': "((name)::text = '0000000000000000000000000000000000000000000000000000000006872584'::text)"}]
! 3 0.3938876986503601
{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.56, 'Total Cost': 8.58, 'Index Cond': "((name)::text = '0000000000000000000000000000000000000000000000000000000006872584'::text)"}
! 0 0.0
empty
! 1 0.0
empty
! 2 0.0
empty
======Top3 metric
total cpu usage_sys: 0.478375332335702
dsk/total_read: 0.30450441971397824
total cpu usage_usr: 0.08516190168839749
igs:
[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 41.3556022644043, 41.3556022644043, 41.35258483886719, 41.35258483886719, 41.35523986816406, 41.35523986816406, 41.35523986816406, 41.35524368286133]
Transformer(
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (2): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (3): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (4): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (5): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
    )
    (norm): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
  )
  (decoder): TransformerDecoder(
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (1): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (2): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (3): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (4): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (5): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
    )
    (norm): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
  )
)
Transformer(
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (2): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (3): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (4): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (5): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
    )
    (norm): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
  )
  (decoder): TransformerDecoder(
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (1): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (2): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (3): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (4): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (5): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
    )
    (norm): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
  )
)
[[[0.0, 0.0, 0.0, 0.3938876986503601], [0.0, 0.0, 0.0, 0.3938876986503601], [0.0, 0.0, 0.0, 0.3938876986503601], [0.0, 0.0, 0.0, 0.3938876986503601], [0.0, 0.0, 0.0, 0.3938876986503601]], [[0.0, 0.0, 0.0, 0.3938876986503601], [0.0, 0.0, 0.0, 0.3938876986503601], [0.0, 0.0, 0.0, 0.3938876986503601], [0.0, 0.0, 0.0, 0.3938876986503601], [0.0, 0.0, 0.0, 0.3938876986503601]]]
# 1
SELECT * FROM ACCOUNTS WHERE custid = $1
* 1
{'Node Type': 'Index Scan', 'Parallel Aware': False, 'Scan Direction': 'Forward', 'Index Name': 'pk_accounts', 'Relation Name': 'accounts', 'Alias': 'accounts', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 73, 'Index Cond': "(custid = '4091455'::bigint)"}
[{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': "(custid = '4091455'::bigint)"}]
! 3 0.3938876986503601
{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': "(custid = '4091455'::bigint)"}
! 0 0.0
empty
! 1 0.0
empty
! 2 0.0
empty
* 2
{'Node Type': 'Index Scan', 'Parallel Aware': False, 'Scan Direction': 'Forward', 'Index Name': 'pk_accounts', 'Relation Name': 'accounts', 'Alias': 'accounts', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 73, 'Index Cond': "(custid = '8258886'::bigint)"}
[{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': "(custid = '8258886'::bigint)"}]
! 3 0.3938876986503601
{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': "(custid = '8258886'::bigint)"}
! 0 0.0
empty
! 1 0.0
empty
! 2 0.0
empty
* 3
[{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': "(custid = '8258886'::bigint)"}]
! 3 0.3938876986503601
{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': "(custid = '8258886'::bigint)"}
! 0 0.0
empty
! 1 0.0
empty
! 2 0.0
empty
* 4
[{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': "(custid = '8258886'::bigint)"}]
! 3 0.3938876986503601
{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': "(custid = '8258886'::bigint)"}
! 0 0.0
empty
! 1 0.0
empty
! 2 0.0
empty
* 5
[{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': "(custid = '8258886'::bigint)"}]
! 3 0.3938876986503601
{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': "(custid = '8258886'::bigint)"}
! 0 0.0
empty
! 1 0.0
empty
! 2 0.0
empty
# 2
SELECT * FROM ACCOUNTS WHERE name = $1
* 1
{'Node Type': 'Index Scan', 'Parallel Aware': False, 'Scan Direction': 'Forward', 'Index Name': 'idx_accounts_name', 'Relation Name': 'accounts', 'Alias': 'accounts', 'Startup Cost': 0.56, 'Total Cost': 8.58, 'Plan Rows': 1, 'Plan Width': 73, 'Index Cond': "((name)::text = '0000000000000000000000000000000000000000000000000000000009338014'::text)"}
[{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.56, 'Total Cost': 8.58, 'Index Cond': "((name)::text = '0000000000000000000000000000000000000000000000000000000009338014'::text)"}]
! 3 0.3938876986503601
{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.56, 'Total Cost': 8.58, 'Index Cond': "((name)::text = '0000000000000000000000000000000000000000000000000000000009338014'::text)"}
! 0 0.0
empty
! 1 0.0
empty
! 2 0.0
empty
* 2
{'Node Type': 'Index Scan', 'Parallel Aware': False, 'Scan Direction': 'Forward', 'Index Name': 'idx_accounts_name', 'Relation Name': 'accounts', 'Alias': 'accounts', 'Startup Cost': 0.56, 'Total Cost': 8.58, 'Plan Rows': 1, 'Plan Width': 73, 'Index Cond': "((name)::text = '0000000000000000000000000000000000000000000000000000000006872584'::text)"}
[{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.56, 'Total Cost': 8.58, 'Index Cond': "((name)::text = '0000000000000000000000000000000000000000000000000000000006872584'::text)"}]
! 3 0.3938876986503601
{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.56, 'Total Cost': 8.58, 'Index Cond': "((name)::text = '0000000000000000000000000000000000000000000000000000000006872584'::text)"}
! 0 0.0
empty
! 1 0.0
empty
! 2 0.0
empty
* 3
[{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.56, 'Total Cost': 8.58, 'Index Cond': "((name)::text = '0000000000000000000000000000000000000000000000000000000006872584'::text)"}]
! 3 0.3938876986503601
{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.56, 'Total Cost': 8.58, 'Index Cond': "((name)::text = '0000000000000000000000000000000000000000000000000000000006872584'::text)"}
! 0 0.0
empty
! 1 0.0
empty
! 2 0.0
empty
* 4
[{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.56, 'Total Cost': 8.58, 'Index Cond': "((name)::text = '0000000000000000000000000000000000000000000000000000000006872584'::text)"}]
! 3 0.3938876986503601
{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.56, 'Total Cost': 8.58, 'Index Cond': "((name)::text = '0000000000000000000000000000000000000000000000000000000006872584'::text)"}
! 0 0.0
empty
! 1 0.0
empty
! 2 0.0
empty
* 5
[{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.56, 'Total Cost': 8.58, 'Index Cond': "((name)::text = '0000000000000000000000000000000000000000000000000000000006872584'::text)"}]
! 3 0.3938876986503601
{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.56, 'Total Cost': 8.58, 'Index Cond': "((name)::text = '0000000000000000000000000000000000000000000000000000000006872584'::text)"}
! 0 0.0
empty
! 1 0.0
empty
! 2 0.0
empty
======Top3 metric
total cpu usage_sys: 0.4846769725065271
dsk/total_read: 0.33422089615718303
dsk/total_writ: 0.0544385812501385
igs:
[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 34.93169021606445, 34.93169021606445, 34.929141998291016, 34.929141998291016, 34.93138122558594, 34.93138122558594, 34.93138122558594, 34.93138885498047]
Transformer(
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (2): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (3): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (4): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (5): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
    )
    (norm): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
  )
  (decoder): TransformerDecoder(
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (1): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (2): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (3): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (4): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (5): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
    )
    (norm): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
  )
)
Transformer(
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (2): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (3): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (4): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (5): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
    )
    (norm): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
  )
  (decoder): TransformerDecoder(
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (1): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (2): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (3): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (4): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (5): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
    )
    (norm): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
  )
)
[[[0.0, 0.0, 0.0, 0.3938876986503601], [0.0, 0.0, 0.0, 0.3938876986503601], [0.0, 0.0, 0.0, 0.3938876986503601], [0.0, 0.0, 0.0, 0.3938876986503601], [0.0, 0.0, 0.0, 0.3938876986503601]], [[0.0, 0.0, 0.0, 0.3938876986503601], [0.0, 0.0, 0.0, 0.3938876986503601], [0.0, 0.0, 0.0, 0.3938876986503601], [0.0, 0.0, 0.0, 0.3938876986503601], [0.0, 0.0, 0.0, 0.3938876986503601]]]
# 1
SELECT * FROM ACCOUNTS WHERE custid = $1
* 1
{'Node Type': 'Index Scan', 'Parallel Aware': False, 'Scan Direction': 'Forward', 'Index Name': 'pk_accounts', 'Relation Name': 'accounts', 'Alias': 'accounts', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 73, 'Index Cond': "(custid = '4091455'::bigint)"}
[{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': "(custid = '4091455'::bigint)"}]
! 3 0.3938876986503601
{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': "(custid = '4091455'::bigint)"}
! 0 0.0
empty
! 1 0.0
empty
! 2 0.0
empty
* 2
{'Node Type': 'Index Scan', 'Parallel Aware': False, 'Scan Direction': 'Forward', 'Index Name': 'pk_accounts', 'Relation Name': 'accounts', 'Alias': 'accounts', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 73, 'Index Cond': "(custid = '8258886'::bigint)"}
[{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': "(custid = '8258886'::bigint)"}]
! 3 0.3938876986503601
{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': "(custid = '8258886'::bigint)"}
! 0 0.0
empty
! 1 0.0
empty
! 2 0.0
empty
* 3
[{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': "(custid = '8258886'::bigint)"}]
! 3 0.3938876986503601
{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': "(custid = '8258886'::bigint)"}
! 0 0.0
empty
! 1 0.0
empty
! 2 0.0
empty
* 4
[{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': "(custid = '8258886'::bigint)"}]
! 3 0.3938876986503601
{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': "(custid = '8258886'::bigint)"}
! 0 0.0
empty
! 1 0.0
empty
! 2 0.0
empty
* 5
[{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': "(custid = '8258886'::bigint)"}]
! 3 0.3938876986503601
{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': "(custid = '8258886'::bigint)"}
! 0 0.0
empty
! 1 0.0
empty
! 2 0.0
empty
# 2
SELECT * FROM ACCOUNTS WHERE name = $1
* 1
{'Node Type': 'Index Scan', 'Parallel Aware': False, 'Scan Direction': 'Forward', 'Index Name': 'idx_accounts_name', 'Relation Name': 'accounts', 'Alias': 'accounts', 'Startup Cost': 0.56, 'Total Cost': 8.58, 'Plan Rows': 1, 'Plan Width': 73, 'Index Cond': "((name)::text = '0000000000000000000000000000000000000000000000000000000009338014'::text)"}
[{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.56, 'Total Cost': 8.58, 'Index Cond': "((name)::text = '0000000000000000000000000000000000000000000000000000000009338014'::text)"}]
! 3 0.3938876986503601
{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.56, 'Total Cost': 8.58, 'Index Cond': "((name)::text = '0000000000000000000000000000000000000000000000000000000009338014'::text)"}
! 0 0.0
empty
! 1 0.0
empty
! 2 0.0
empty
* 2
{'Node Type': 'Index Scan', 'Parallel Aware': False, 'Scan Direction': 'Forward', 'Index Name': 'idx_accounts_name', 'Relation Name': 'accounts', 'Alias': 'accounts', 'Startup Cost': 0.56, 'Total Cost': 8.58, 'Plan Rows': 1, 'Plan Width': 73, 'Index Cond': "((name)::text = '0000000000000000000000000000000000000000000000000000000006872584'::text)"}
[{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.56, 'Total Cost': 8.58, 'Index Cond': "((name)::text = '0000000000000000000000000000000000000000000000000000000006872584'::text)"}]
! 3 0.3938876986503601
{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.56, 'Total Cost': 8.58, 'Index Cond': "((name)::text = '0000000000000000000000000000000000000000000000000000000006872584'::text)"}
! 0 0.0
empty
! 1 0.0
empty
! 2 0.0
empty
* 3
[{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.56, 'Total Cost': 8.58, 'Index Cond': "((name)::text = '0000000000000000000000000000000000000000000000000000000006872584'::text)"}]
! 3 0.3938876986503601
{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.56, 'Total Cost': 8.58, 'Index Cond': "((name)::text = '0000000000000000000000000000000000000000000000000000000006872584'::text)"}
! 0 0.0
empty
! 1 0.0
empty
! 2 0.0
empty
* 4
[{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.56, 'Total Cost': 8.58, 'Index Cond': "((name)::text = '0000000000000000000000000000000000000000000000000000000006872584'::text)"}]
! 3 0.3938876986503601
{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.56, 'Total Cost': 8.58, 'Index Cond': "((name)::text = '0000000000000000000000000000000000000000000000000000000006872584'::text)"}
! 0 0.0
empty
! 1 0.0
empty
! 2 0.0
empty
* 5
[{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.56, 'Total Cost': 8.58, 'Index Cond': "((name)::text = '0000000000000000000000000000000000000000000000000000000006872584'::text)"}]
! 3 0.3938876986503601
{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.56, 'Total Cost': 8.58, 'Index Cond': "((name)::text = '0000000000000000000000000000000000000000000000000000000006872584'::text)"}
! 0 0.0
empty
! 1 0.0
empty
! 2 0.0
empty
======Top3 metric
total cpu usage_sys: 0.441900796733051
dsk/total_read: 0.31206578029422327
total cpu usage_usr: 0.11469952809822202
igs:
[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 74.9770736694336, 74.9770736694336, 74.97160339355469, 74.97160339355469, 74.97640991210938, 74.97640991210938, 74.97640991210938, 74.97642517089844]
Transformer(
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (2): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (3): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (4): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (5): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
    )
    (norm): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
  )
  (decoder): TransformerDecoder(
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (1): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (2): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (3): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (4): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (5): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
    )
    (norm): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
  )
)
Transformer(
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (2): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (3): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (4): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (5): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
    )
    (norm): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
  )
  (decoder): TransformerDecoder(
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (1): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (2): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (3): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (4): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (5): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
    )
    (norm): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
  )
)
[[[0.0, 0.0, 0.0, 0.3938876986503601], [0.0, 0.0, 0.0, 0.3938876986503601], [0.0, 0.0, 0.0, 0.3938876986503601], [0.0, 0.0, 0.0, 0.3938876986503601], [0.0, 0.0, 0.0, 0.3938876986503601]], [[0.0, 0.0, 0.0, 0.3938876986503601], [0.0, 0.0, 0.0, 0.3938876986503601], [0.0, 0.0, 0.0, 0.3938876986503601], [0.0, 0.0, 0.0, 0.3938876986503601], [0.0, 0.0, 0.0, 0.3938876986503601]]]
# 1
SELECT * FROM ACCOUNTS WHERE custid = $1
* 1
{'Node Type': 'Index Scan', 'Parallel Aware': False, 'Scan Direction': 'Forward', 'Index Name': 'pk_accounts', 'Relation Name': 'accounts', 'Alias': 'accounts', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 73, 'Index Cond': "(custid = '4091455'::bigint)"}
[{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': "(custid = '4091455'::bigint)"}]
! 3 0.3938876986503601
{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': "(custid = '4091455'::bigint)"}
! 0 0.0
empty
! 1 0.0
empty
! 2 0.0
empty
* 2
{'Node Type': 'Index Scan', 'Parallel Aware': False, 'Scan Direction': 'Forward', 'Index Name': 'pk_accounts', 'Relation Name': 'accounts', 'Alias': 'accounts', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 73, 'Index Cond': "(custid = '8258886'::bigint)"}
[{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': "(custid = '8258886'::bigint)"}]
! 3 0.3938876986503601
{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': "(custid = '8258886'::bigint)"}
! 0 0.0
empty
! 1 0.0
empty
! 2 0.0
empty
* 3
[{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': "(custid = '8258886'::bigint)"}]
! 3 0.3938876986503601
{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': "(custid = '8258886'::bigint)"}
! 0 0.0
empty
! 1 0.0
empty
! 2 0.0
empty
* 4
[{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': "(custid = '8258886'::bigint)"}]
! 3 0.3938876986503601
{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': "(custid = '8258886'::bigint)"}
! 0 0.0
empty
! 1 0.0
empty
! 2 0.0
empty
* 5
[{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': "(custid = '8258886'::bigint)"}]
! 3 0.3938876986503601
{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': "(custid = '8258886'::bigint)"}
! 0 0.0
empty
! 1 0.0
empty
! 2 0.0
empty
# 2
SELECT * FROM ACCOUNTS WHERE name = $1
* 1
{'Node Type': 'Index Scan', 'Parallel Aware': False, 'Scan Direction': 'Forward', 'Index Name': 'idx_accounts_name', 'Relation Name': 'accounts', 'Alias': 'accounts', 'Startup Cost': 0.56, 'Total Cost': 8.58, 'Plan Rows': 1, 'Plan Width': 73, 'Index Cond': "((name)::text = '0000000000000000000000000000000000000000000000000000000009338014'::text)"}
[{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.56, 'Total Cost': 8.58, 'Index Cond': "((name)::text = '0000000000000000000000000000000000000000000000000000000009338014'::text)"}]
! 3 0.3938876986503601
{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.56, 'Total Cost': 8.58, 'Index Cond': "((name)::text = '0000000000000000000000000000000000000000000000000000000009338014'::text)"}
! 0 0.0
empty
! 1 0.0
empty
! 2 0.0
empty
* 2
{'Node Type': 'Index Scan', 'Parallel Aware': False, 'Scan Direction': 'Forward', 'Index Name': 'idx_accounts_name', 'Relation Name': 'accounts', 'Alias': 'accounts', 'Startup Cost': 0.56, 'Total Cost': 8.58, 'Plan Rows': 1, 'Plan Width': 73, 'Index Cond': "((name)::text = '0000000000000000000000000000000000000000000000000000000006872584'::text)"}
[{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.56, 'Total Cost': 8.58, 'Index Cond': "((name)::text = '0000000000000000000000000000000000000000000000000000000006872584'::text)"}]
! 3 0.3938876986503601
{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.56, 'Total Cost': 8.58, 'Index Cond': "((name)::text = '0000000000000000000000000000000000000000000000000000000006872584'::text)"}
! 0 0.0
empty
! 1 0.0
empty
! 2 0.0
empty
* 3
[{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.56, 'Total Cost': 8.58, 'Index Cond': "((name)::text = '0000000000000000000000000000000000000000000000000000000006872584'::text)"}]
! 3 0.3938876986503601
{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.56, 'Total Cost': 8.58, 'Index Cond': "((name)::text = '0000000000000000000000000000000000000000000000000000000006872584'::text)"}
! 0 0.0
empty
! 1 0.0
empty
! 2 0.0
empty
* 4
[{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.56, 'Total Cost': 8.58, 'Index Cond': "((name)::text = '0000000000000000000000000000000000000000000000000000000006872584'::text)"}]
! 3 0.3938876986503601
{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.56, 'Total Cost': 8.58, 'Index Cond': "((name)::text = '0000000000000000000000000000000000000000000000000000000006872584'::text)"}
! 0 0.0
empty
! 1 0.0
empty
! 2 0.0
empty
* 5
[{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.56, 'Total Cost': 8.58, 'Index Cond': "((name)::text = '0000000000000000000000000000000000000000000000000000000006872584'::text)"}]
! 3 0.3938876986503601
{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.56, 'Total Cost': 8.58, 'Index Cond': "((name)::text = '0000000000000000000000000000000000000000000000000000000006872584'::text)"}
! 0 0.0
empty
! 1 0.0
empty
! 2 0.0
empty
======Top3 metric
total cpu usage_sys: 0.3545463608308572
dsk/total_read: 0.2616552911628216
total cpu usage_usr: 0.1270825737038578
igs:
[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5.785618782043457, 5.785618782043457, 5.785196781158447, 5.785196781158447, 5.785567760467529, 5.785567760467529, 5.785567760467529, 5.785568714141846]
Transformer(
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (2): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (3): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (4): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (5): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
    )
    (norm): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
  )
  (decoder): TransformerDecoder(
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (1): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (2): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (3): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (4): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (5): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
    )
    (norm): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
  )
)
Transformer(
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (2): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (3): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (4): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (5): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
    )
    (norm): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
  )
  (decoder): TransformerDecoder(
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (1): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (2): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (3): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (4): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (5): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
    )
    (norm): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
  )
)
[[[0.0, 0.0, 0.0, 0.3938876986503601], [0.0, 0.0, 0.0, 0.3938876986503601], [0.0, 0.0, 0.0, 0.3938876986503601], [0.0, 0.0, 0.0, 0.3938876986503601], [0.0, 0.0, 0.0, 0.3938876986503601]], [[0.0, 0.0, 0.0, 0.3938876986503601], [0.0, 0.0, 0.0, 0.3938876986503601], [0.0, 0.0, 0.0, 0.3938876986503601], [0.0, 0.0, 0.0, 0.3938876986503601], [0.0, 0.0, 0.0, 0.3938876986503601]]]
# 1
SELECT * FROM ACCOUNTS WHERE custid = $1
* 1
{'Node Type': 'Index Scan', 'Parallel Aware': False, 'Scan Direction': 'Forward', 'Index Name': 'pk_accounts', 'Relation Name': 'accounts', 'Alias': 'accounts', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 73, 'Index Cond': "(custid = '4091455'::bigint)"}
[{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': "(custid = '4091455'::bigint)"}]
! 3 0.3938876986503601
{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': "(custid = '4091455'::bigint)"}
! 0 0.0
empty
! 1 0.0
empty
! 2 0.0
empty
* 2
{'Node Type': 'Index Scan', 'Parallel Aware': False, 'Scan Direction': 'Forward', 'Index Name': 'pk_accounts', 'Relation Name': 'accounts', 'Alias': 'accounts', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 73, 'Index Cond': "(custid = '8258886'::bigint)"}
[{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': "(custid = '8258886'::bigint)"}]
! 3 0.3938876986503601
{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': "(custid = '8258886'::bigint)"}
! 0 0.0
empty
! 1 0.0
empty
! 2 0.0
empty
* 3
[{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': "(custid = '8258886'::bigint)"}]
! 3 0.3938876986503601
{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': "(custid = '8258886'::bigint)"}
! 0 0.0
empty
! 1 0.0
empty
! 2 0.0
empty
* 4
[{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': "(custid = '8258886'::bigint)"}]
! 3 0.3938876986503601
{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': "(custid = '8258886'::bigint)"}
! 0 0.0
empty
! 1 0.0
empty
! 2 0.0
empty
* 5
[{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': "(custid = '8258886'::bigint)"}]
! 3 0.3938876986503601
{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': "(custid = '8258886'::bigint)"}
! 0 0.0
empty
! 1 0.0
empty
! 2 0.0
empty
# 2
SELECT * FROM ACCOUNTS WHERE name = $1
* 1
{'Node Type': 'Index Scan', 'Parallel Aware': False, 'Scan Direction': 'Forward', 'Index Name': 'idx_accounts_name', 'Relation Name': 'accounts', 'Alias': 'accounts', 'Startup Cost': 0.56, 'Total Cost': 8.58, 'Plan Rows': 1, 'Plan Width': 73, 'Index Cond': "((name)::text = '0000000000000000000000000000000000000000000000000000000009338014'::text)"}
[{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.56, 'Total Cost': 8.58, 'Index Cond': "((name)::text = '0000000000000000000000000000000000000000000000000000000009338014'::text)"}]
! 3 0.3938876986503601
{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.56, 'Total Cost': 8.58, 'Index Cond': "((name)::text = '0000000000000000000000000000000000000000000000000000000009338014'::text)"}
! 0 0.0
empty
! 1 0.0
empty
! 2 0.0
empty
* 2
{'Node Type': 'Index Scan', 'Parallel Aware': False, 'Scan Direction': 'Forward', 'Index Name': 'idx_accounts_name', 'Relation Name': 'accounts', 'Alias': 'accounts', 'Startup Cost': 0.56, 'Total Cost': 8.58, 'Plan Rows': 1, 'Plan Width': 73, 'Index Cond': "((name)::text = '0000000000000000000000000000000000000000000000000000000006872584'::text)"}
[{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.56, 'Total Cost': 8.58, 'Index Cond': "((name)::text = '0000000000000000000000000000000000000000000000000000000006872584'::text)"}]
! 3 0.3938876986503601
{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.56, 'Total Cost': 8.58, 'Index Cond': "((name)::text = '0000000000000000000000000000000000000000000000000000000006872584'::text)"}
! 0 0.0
empty
! 1 0.0
empty
! 2 0.0
empty
* 3
[{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.56, 'Total Cost': 8.58, 'Index Cond': "((name)::text = '0000000000000000000000000000000000000000000000000000000006872584'::text)"}]
! 3 0.3938876986503601
{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.56, 'Total Cost': 8.58, 'Index Cond': "((name)::text = '0000000000000000000000000000000000000000000000000000000006872584'::text)"}
! 0 0.0
empty
! 1 0.0
empty
! 2 0.0
empty
* 4
[{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.56, 'Total Cost': 8.58, 'Index Cond': "((name)::text = '0000000000000000000000000000000000000000000000000000000006872584'::text)"}]
! 3 0.3938876986503601
{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.56, 'Total Cost': 8.58, 'Index Cond': "((name)::text = '0000000000000000000000000000000000000000000000000000000006872584'::text)"}
! 0 0.0
empty
! 1 0.0
empty
! 2 0.0
empty
* 5
[{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.56, 'Total Cost': 8.58, 'Index Cond': "((name)::text = '0000000000000000000000000000000000000000000000000000000006872584'::text)"}]
! 3 0.3938876986503601
{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.56, 'Total Cost': 8.58, 'Index Cond': "((name)::text = '0000000000000000000000000000000000000000000000000000000006872584'::text)"}
! 0 0.0
empty
! 1 0.0
empty
! 2 0.0
empty
======Top3 metric
dsk/total_read: 0.45570812748261874
total cpu usage_sys: 0.1900093998674915
total cpu usage_idl: 0.0856261405596596
igs:
[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 31.8848876953125, 31.8848876953125, 31.8825626373291, 31.8825626373291, 31.884607315063477, 31.884607315063477, 31.884607315063477, 31.884613037109375]
Transformer(
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (2): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (3): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (4): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (5): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
    )
    (norm): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
  )
  (decoder): TransformerDecoder(
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (1): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (2): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (3): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (4): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (5): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
    )
    (norm): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
  )
)
Transformer(
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (2): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (3): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (4): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (5): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
    )
    (norm): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
  )
  (decoder): TransformerDecoder(
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (1): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (2): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (3): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (4): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (5): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
    )
    (norm): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
  )
)
[[[0.0, 0.0, 0.0, 0.3938876986503601], [0.0, 0.0, 0.0, 0.3938876986503601], [0.0, 0.0, 0.0, 0.3938876986503601], [0.0, 0.0, 0.0, 0.3938876986503601], [0.0, 0.0, 0.0, 0.3938876986503601]], [[0.0, 0.0, 0.0, 0.3938876986503601], [0.0, 0.0, 0.0, 0.3938876986503601], [0.0, 0.0, 0.0, 0.3938876986503601], [0.0, 0.0, 0.0, 0.3938876986503601], [0.0, 0.0, 0.0, 0.3938876986503601]]]
# 1
SELECT * FROM ACCOUNTS WHERE custid = $1
* 1
{'Node Type': 'Index Scan', 'Parallel Aware': False, 'Scan Direction': 'Forward', 'Index Name': 'pk_accounts', 'Relation Name': 'accounts', 'Alias': 'accounts', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 73, 'Index Cond': "(custid = '4091455'::bigint)"}
[{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': "(custid = '4091455'::bigint)"}]
! 3 0.3938876986503601
{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': "(custid = '4091455'::bigint)"}
! 0 0.0
empty
! 1 0.0
empty
! 2 0.0
empty
* 2
{'Node Type': 'Index Scan', 'Parallel Aware': False, 'Scan Direction': 'Forward', 'Index Name': 'pk_accounts', 'Relation Name': 'accounts', 'Alias': 'accounts', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 73, 'Index Cond': "(custid = '8258886'::bigint)"}
[{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': "(custid = '8258886'::bigint)"}]
! 3 0.3938876986503601
{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': "(custid = '8258886'::bigint)"}
! 0 0.0
empty
! 1 0.0
empty
! 2 0.0
empty
* 3
[{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': "(custid = '8258886'::bigint)"}]
! 3 0.3938876986503601
{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': "(custid = '8258886'::bigint)"}
! 0 0.0
empty
! 1 0.0
empty
! 2 0.0
empty
* 4
[{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': "(custid = '8258886'::bigint)"}]
! 3 0.3938876986503601
{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': "(custid = '8258886'::bigint)"}
! 0 0.0
empty
! 1 0.0
empty
! 2 0.0
empty
* 5
[{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': "(custid = '8258886'::bigint)"}]
! 3 0.3938876986503601
{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': "(custid = '8258886'::bigint)"}
! 0 0.0
empty
! 1 0.0
empty
! 2 0.0
empty
# 2
SELECT * FROM ACCOUNTS WHERE name = $1
* 1
{'Node Type': 'Index Scan', 'Parallel Aware': False, 'Scan Direction': 'Forward', 'Index Name': 'idx_accounts_name', 'Relation Name': 'accounts', 'Alias': 'accounts', 'Startup Cost': 0.56, 'Total Cost': 8.58, 'Plan Rows': 1, 'Plan Width': 73, 'Index Cond': "((name)::text = '0000000000000000000000000000000000000000000000000000000009338014'::text)"}
[{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.56, 'Total Cost': 8.58, 'Index Cond': "((name)::text = '0000000000000000000000000000000000000000000000000000000009338014'::text)"}]
! 3 0.3938876986503601
{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.56, 'Total Cost': 8.58, 'Index Cond': "((name)::text = '0000000000000000000000000000000000000000000000000000000009338014'::text)"}
! 0 0.0
empty
! 1 0.0
empty
! 2 0.0
empty
* 2
{'Node Type': 'Index Scan', 'Parallel Aware': False, 'Scan Direction': 'Forward', 'Index Name': 'idx_accounts_name', 'Relation Name': 'accounts', 'Alias': 'accounts', 'Startup Cost': 0.56, 'Total Cost': 8.58, 'Plan Rows': 1, 'Plan Width': 73, 'Index Cond': "((name)::text = '0000000000000000000000000000000000000000000000000000000006872584'::text)"}
[{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.56, 'Total Cost': 8.58, 'Index Cond': "((name)::text = '0000000000000000000000000000000000000000000000000000000006872584'::text)"}]
! 3 0.3938876986503601
{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.56, 'Total Cost': 8.58, 'Index Cond': "((name)::text = '0000000000000000000000000000000000000000000000000000000006872584'::text)"}
! 0 0.0
empty
! 1 0.0
empty
! 2 0.0
empty
* 3
[{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.56, 'Total Cost': 8.58, 'Index Cond': "((name)::text = '0000000000000000000000000000000000000000000000000000000006872584'::text)"}]
! 3 0.3938876986503601
{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.56, 'Total Cost': 8.58, 'Index Cond': "((name)::text = '0000000000000000000000000000000000000000000000000000000006872584'::text)"}
! 0 0.0
empty
! 1 0.0
empty
! 2 0.0
empty
* 4
[{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.56, 'Total Cost': 8.58, 'Index Cond': "((name)::text = '0000000000000000000000000000000000000000000000000000000006872584'::text)"}]
! 3 0.3938876986503601
{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.56, 'Total Cost': 8.58, 'Index Cond': "((name)::text = '0000000000000000000000000000000000000000000000000000000006872584'::text)"}
! 0 0.0
empty
! 1 0.0
empty
! 2 0.0
empty
* 5
[{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.56, 'Total Cost': 8.58, 'Index Cond': "((name)::text = '0000000000000000000000000000000000000000000000000000000006872584'::text)"}]
! 3 0.3938876986503601
{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.56, 'Total Cost': 8.58, 'Index Cond': "((name)::text = '0000000000000000000000000000000000000000000000000000000006872584'::text)"}
! 0 0.0
empty
! 1 0.0
empty
! 2 0.0
empty
======Top3 metric
dsk/total_read: 0.3358505732894828
total cpu usage_sys: 0.32471490497591604
total cpu usage_usr: 0.18001777711865513
igs:
[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 58.4179801940918, 58.4179801940918, 58.413719177246094, 58.413719177246094, 58.41746520996094, 58.41746520996094, 58.41746520996094, 58.41747283935547]
Transformer(
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (2): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (3): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (4): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (5): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
    )
    (norm): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
  )
  (decoder): TransformerDecoder(
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (1): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (2): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (3): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (4): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (5): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
    )
    (norm): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
  )
)
Transformer(
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (2): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (3): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (4): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (5): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
    )
    (norm): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
  )
  (decoder): TransformerDecoder(
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (1): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (2): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (3): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (4): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (5): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
    )
    (norm): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
  )
)
[[[0.0, 0.0, 0.0, 0.3938876986503601], [0.0, 0.0, 0.0, 0.3938876986503601], [0.0, 0.0, 0.0, 0.3938876986503601], [0.0, 0.0, 0.0, 0.3938876986503601], [0.0, 0.0, 0.0, 0.3938876986503601]], [[0.0, 0.0, 0.0, 0.3938876986503601], [0.0, 0.0, 0.0, 0.3938876986503601], [0.0, 0.0, 0.0, 0.3938876986503601], [0.0, 0.0, 0.0, 0.3938876986503601], [0.0, 0.0, 0.0, 0.3938876986503601]]]
# 1
SELECT * FROM ACCOUNTS WHERE custid = $1
* 1
{'Node Type': 'Index Scan', 'Parallel Aware': False, 'Scan Direction': 'Forward', 'Index Name': 'pk_accounts', 'Relation Name': 'accounts', 'Alias': 'accounts', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 73, 'Index Cond': "(custid = '4091455'::bigint)"}
[{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': "(custid = '4091455'::bigint)"}]
! 3 0.3938876986503601
{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': "(custid = '4091455'::bigint)"}
! 0 0.0
empty
! 1 0.0
empty
! 2 0.0
empty
* 2
{'Node Type': 'Index Scan', 'Parallel Aware': False, 'Scan Direction': 'Forward', 'Index Name': 'pk_accounts', 'Relation Name': 'accounts', 'Alias': 'accounts', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 73, 'Index Cond': "(custid = '8258886'::bigint)"}
[{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': "(custid = '8258886'::bigint)"}]
! 3 0.3938876986503601
{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': "(custid = '8258886'::bigint)"}
! 0 0.0
empty
! 1 0.0
empty
! 2 0.0
empty
* 3
[{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': "(custid = '8258886'::bigint)"}]
! 3 0.3938876986503601
{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': "(custid = '8258886'::bigint)"}
! 0 0.0
empty
! 1 0.0
empty
! 2 0.0
empty
* 4
[{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': "(custid = '8258886'::bigint)"}]
! 3 0.3938876986503601
{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': "(custid = '8258886'::bigint)"}
! 0 0.0
empty
! 1 0.0
empty
! 2 0.0
empty
* 5
[{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': "(custid = '8258886'::bigint)"}]
! 3 0.3938876986503601
{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': "(custid = '8258886'::bigint)"}
! 0 0.0
empty
! 1 0.0
empty
! 2 0.0
empty
# 2
SELECT * FROM ACCOUNTS WHERE name = $1
* 1
{'Node Type': 'Index Scan', 'Parallel Aware': False, 'Scan Direction': 'Forward', 'Index Name': 'idx_accounts_name', 'Relation Name': 'accounts', 'Alias': 'accounts', 'Startup Cost': 0.56, 'Total Cost': 8.58, 'Plan Rows': 1, 'Plan Width': 73, 'Index Cond': "((name)::text = '0000000000000000000000000000000000000000000000000000000009338014'::text)"}
[{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.56, 'Total Cost': 8.58, 'Index Cond': "((name)::text = '0000000000000000000000000000000000000000000000000000000009338014'::text)"}]
! 3 0.3938876986503601
{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.56, 'Total Cost': 8.58, 'Index Cond': "((name)::text = '0000000000000000000000000000000000000000000000000000000009338014'::text)"}
! 0 0.0
empty
! 1 0.0
empty
! 2 0.0
empty
* 2
{'Node Type': 'Index Scan', 'Parallel Aware': False, 'Scan Direction': 'Forward', 'Index Name': 'idx_accounts_name', 'Relation Name': 'accounts', 'Alias': 'accounts', 'Startup Cost': 0.56, 'Total Cost': 8.58, 'Plan Rows': 1, 'Plan Width': 73, 'Index Cond': "((name)::text = '0000000000000000000000000000000000000000000000000000000006872584'::text)"}
[{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.56, 'Total Cost': 8.58, 'Index Cond': "((name)::text = '0000000000000000000000000000000000000000000000000000000006872584'::text)"}]
! 3 0.3938876986503601
{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.56, 'Total Cost': 8.58, 'Index Cond': "((name)::text = '0000000000000000000000000000000000000000000000000000000006872584'::text)"}
! 0 0.0
empty
! 1 0.0
empty
! 2 0.0
empty
* 3
[{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.56, 'Total Cost': 8.58, 'Index Cond': "((name)::text = '0000000000000000000000000000000000000000000000000000000006872584'::text)"}]
! 3 0.3938876986503601
{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.56, 'Total Cost': 8.58, 'Index Cond': "((name)::text = '0000000000000000000000000000000000000000000000000000000006872584'::text)"}
! 0 0.0
empty
! 1 0.0
empty
! 2 0.0
empty
* 4
[{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.56, 'Total Cost': 8.58, 'Index Cond': "((name)::text = '0000000000000000000000000000000000000000000000000000000006872584'::text)"}]
! 3 0.3938876986503601
{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.56, 'Total Cost': 8.58, 'Index Cond': "((name)::text = '0000000000000000000000000000000000000000000000000000000006872584'::text)"}
! 0 0.0
empty
! 1 0.0
empty
! 2 0.0
empty
* 5
[{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.56, 'Total Cost': 8.58, 'Index Cond': "((name)::text = '0000000000000000000000000000000000000000000000000000000006872584'::text)"}]
! 3 0.3938876986503601
{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.56, 'Total Cost': 8.58, 'Index Cond': "((name)::text = '0000000000000000000000000000000000000000000000000000000006872584'::text)"}
! 0 0.0
empty
! 1 0.0
empty
! 2 0.0
empty
======Top3 metric
total cpu usage_sys: 0.42908867294321784
dsk/total_read: 0.28428397964875807
total cpu usage_usr: 0.1086212190200483
igs:
[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 62.67939376831055, 62.67939376831055, 62.67482376098633, 62.67482376098633, 62.6788444519043, 62.6788444519043, 62.6788444519043, 62.67885208129883]
Transformer(
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (2): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (3): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (4): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (5): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
    )
    (norm): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
  )
  (decoder): TransformerDecoder(
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (1): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (2): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (3): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (4): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (5): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
    )
    (norm): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
  )
)
Transformer(
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (2): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (3): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (4): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (5): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
    )
    (norm): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
  )
  (decoder): TransformerDecoder(
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (1): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (2): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (3): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (4): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (5): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
    )
    (norm): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
  )
)
[[[0.0, 0.0, 0.0, 0.3938876986503601], [0.0, 0.0, 0.0, 0.3938876986503601], [0.0, 0.0, 0.0, 0.3938876986503601], [0.0, 0.0, 0.0, 0.3938876986503601], [0.0, 0.0, 0.0, 0.3938876986503601]], [[0.0, 0.0, 0.0, 0.3938876986503601], [0.0, 0.0, 0.0, 0.3938876986503601], [0.0, 0.0, 0.0, 0.3938876986503601], [0.0, 0.0, 0.0, 0.3938876986503601], [0.0, 0.0, 0.0, 0.3938876986503601]]]
# 1
SELECT * FROM ACCOUNTS WHERE custid = $1
* 1
{'Node Type': 'Index Scan', 'Parallel Aware': False, 'Scan Direction': 'Forward', 'Index Name': 'pk_accounts', 'Relation Name': 'accounts', 'Alias': 'accounts', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 73, 'Index Cond': "(custid = '4091455'::bigint)"}
[{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': "(custid = '4091455'::bigint)"}]
! 3 0.3938876986503601
{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': "(custid = '4091455'::bigint)"}
! 0 0.0
empty
! 1 0.0
empty
! 2 0.0
empty
* 2
{'Node Type': 'Index Scan', 'Parallel Aware': False, 'Scan Direction': 'Forward', 'Index Name': 'pk_accounts', 'Relation Name': 'accounts', 'Alias': 'accounts', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 73, 'Index Cond': "(custid = '8258886'::bigint)"}
[{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': "(custid = '8258886'::bigint)"}]
! 3 0.3938876986503601
{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': "(custid = '8258886'::bigint)"}
! 0 0.0
empty
! 1 0.0
empty
! 2 0.0
empty
* 3
[{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': "(custid = '8258886'::bigint)"}]
! 3 0.3938876986503601
{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': "(custid = '8258886'::bigint)"}
! 0 0.0
empty
! 1 0.0
empty
! 2 0.0
empty
* 4
[{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': "(custid = '8258886'::bigint)"}]
! 3 0.3938876986503601
{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': "(custid = '8258886'::bigint)"}
! 0 0.0
empty
! 1 0.0
empty
! 2 0.0
empty
* 5
[{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': "(custid = '8258886'::bigint)"}]
! 3 0.3938876986503601
{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': "(custid = '8258886'::bigint)"}
! 0 0.0
empty
! 1 0.0
empty
! 2 0.0
empty
# 2
SELECT * FROM ACCOUNTS WHERE name = $1
* 1
{'Node Type': 'Index Scan', 'Parallel Aware': False, 'Scan Direction': 'Forward', 'Index Name': 'idx_accounts_name', 'Relation Name': 'accounts', 'Alias': 'accounts', 'Startup Cost': 0.56, 'Total Cost': 8.58, 'Plan Rows': 1, 'Plan Width': 73, 'Index Cond': "((name)::text = '0000000000000000000000000000000000000000000000000000000009338014'::text)"}
[{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.56, 'Total Cost': 8.58, 'Index Cond': "((name)::text = '0000000000000000000000000000000000000000000000000000000009338014'::text)"}]
! 3 0.3938876986503601
{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.56, 'Total Cost': 8.58, 'Index Cond': "((name)::text = '0000000000000000000000000000000000000000000000000000000009338014'::text)"}
! 0 0.0
empty
! 1 0.0
empty
! 2 0.0
empty
* 2
{'Node Type': 'Index Scan', 'Parallel Aware': False, 'Scan Direction': 'Forward', 'Index Name': 'idx_accounts_name', 'Relation Name': 'accounts', 'Alias': 'accounts', 'Startup Cost': 0.56, 'Total Cost': 8.58, 'Plan Rows': 1, 'Plan Width': 73, 'Index Cond': "((name)::text = '0000000000000000000000000000000000000000000000000000000006872584'::text)"}
[{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.56, 'Total Cost': 8.58, 'Index Cond': "((name)::text = '0000000000000000000000000000000000000000000000000000000006872584'::text)"}]
! 3 0.3938876986503601
{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.56, 'Total Cost': 8.58, 'Index Cond': "((name)::text = '0000000000000000000000000000000000000000000000000000000006872584'::text)"}
! 0 0.0
empty
! 1 0.0
empty
! 2 0.0
empty
* 3
[{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.56, 'Total Cost': 8.58, 'Index Cond': "((name)::text = '0000000000000000000000000000000000000000000000000000000006872584'::text)"}]
! 3 0.3938876986503601
{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.56, 'Total Cost': 8.58, 'Index Cond': "((name)::text = '0000000000000000000000000000000000000000000000000000000006872584'::text)"}
! 0 0.0
empty
! 1 0.0
empty
! 2 0.0
empty
* 4
[{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.56, 'Total Cost': 8.58, 'Index Cond': "((name)::text = '0000000000000000000000000000000000000000000000000000000006872584'::text)"}]
! 3 0.3938876986503601
{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.56, 'Total Cost': 8.58, 'Index Cond': "((name)::text = '0000000000000000000000000000000000000000000000000000000006872584'::text)"}
! 0 0.0
empty
! 1 0.0
empty
! 2 0.0
empty
* 5
[{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.56, 'Total Cost': 8.58, 'Index Cond': "((name)::text = '0000000000000000000000000000000000000000000000000000000006872584'::text)"}]
! 3 0.3938876986503601
{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.56, 'Total Cost': 8.58, 'Index Cond': "((name)::text = '0000000000000000000000000000000000000000000000000000000006872584'::text)"}
! 0 0.0
empty
! 1 0.0
empty
! 2 0.0
empty
======Top3 metric
total cpu usage_sys: 0.44625023774798955
dsk/total_read: 0.2938019318648025
memory usage_used: 0.0711843789016843
igs:
[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 43.36930847167969, 43.36930847167969, 43.36614227294922, 43.36614227294922, 43.36892318725586, 43.36892318725586, 43.36892318725586, 43.36893081665039]
Transformer(
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (2): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (3): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (4): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (5): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
    )
    (norm): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
  )
  (decoder): TransformerDecoder(
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (1): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (2): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (3): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (4): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (5): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
    )
    (norm): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
  )
)
Transformer(
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (2): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (3): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (4): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (5): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
    )
    (norm): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
  )
  (decoder): TransformerDecoder(
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (1): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (2): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (3): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (4): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (5): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
    )
    (norm): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
  )
)
[[[0.0, 0.0, 0.0, 0.3938876986503601], [0.0, 0.0, 0.0, 0.3938876986503601], [0.0, 0.0, 0.0, 0.3938876986503601], [0.0, 0.0, 0.0, 0.3938876986503601], [0.0, 0.0, 0.0, 0.3938876986503601]], [[0.0, 0.0, 0.0, 0.3938876986503601], [0.0, 0.0, 0.0, 0.3938876986503601], [0.0, 0.0, 0.0, 0.3938876986503601], [0.0, 0.0, 0.0, 0.3938876986503601], [0.0, 0.0, 0.0, 0.3938876986503601]]]
# 1
SELECT * FROM ACCOUNTS WHERE custid = $1
* 1
{'Node Type': 'Index Scan', 'Parallel Aware': False, 'Scan Direction': 'Forward', 'Index Name': 'pk_accounts', 'Relation Name': 'accounts', 'Alias': 'accounts', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 73, 'Index Cond': "(custid = '4091455'::bigint)"}
[{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': "(custid = '4091455'::bigint)"}]
! 3 0.3938876986503601
{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': "(custid = '4091455'::bigint)"}
! 0 0.0
empty
! 1 0.0
empty
! 2 0.0
empty
* 2
{'Node Type': 'Index Scan', 'Parallel Aware': False, 'Scan Direction': 'Forward', 'Index Name': 'pk_accounts', 'Relation Name': 'accounts', 'Alias': 'accounts', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 73, 'Index Cond': "(custid = '8258886'::bigint)"}
[{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': "(custid = '8258886'::bigint)"}]
! 3 0.3938876986503601
{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': "(custid = '8258886'::bigint)"}
! 0 0.0
empty
! 1 0.0
empty
! 2 0.0
empty
* 3
[{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': "(custid = '8258886'::bigint)"}]
! 3 0.3938876986503601
{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': "(custid = '8258886'::bigint)"}
! 0 0.0
empty
! 1 0.0
empty
! 2 0.0
empty
* 4
[{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': "(custid = '8258886'::bigint)"}]
! 3 0.3938876986503601
{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': "(custid = '8258886'::bigint)"}
! 0 0.0
empty
! 1 0.0
empty
! 2 0.0
empty
* 5
[{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': "(custid = '8258886'::bigint)"}]
! 3 0.3938876986503601
{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': "(custid = '8258886'::bigint)"}
! 0 0.0
empty
! 1 0.0
empty
! 2 0.0
empty
# 2
SELECT * FROM ACCOUNTS WHERE name = $1
* 1
{'Node Type': 'Index Scan', 'Parallel Aware': False, 'Scan Direction': 'Forward', 'Index Name': 'idx_accounts_name', 'Relation Name': 'accounts', 'Alias': 'accounts', 'Startup Cost': 0.56, 'Total Cost': 8.58, 'Plan Rows': 1, 'Plan Width': 73, 'Index Cond': "((name)::text = '0000000000000000000000000000000000000000000000000000000009338014'::text)"}
[{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.56, 'Total Cost': 8.58, 'Index Cond': "((name)::text = '0000000000000000000000000000000000000000000000000000000009338014'::text)"}]
! 3 0.3938876986503601
{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.56, 'Total Cost': 8.58, 'Index Cond': "((name)::text = '0000000000000000000000000000000000000000000000000000000009338014'::text)"}
! 0 0.0
empty
! 1 0.0
empty
! 2 0.0
empty
* 2
{'Node Type': 'Index Scan', 'Parallel Aware': False, 'Scan Direction': 'Forward', 'Index Name': 'idx_accounts_name', 'Relation Name': 'accounts', 'Alias': 'accounts', 'Startup Cost': 0.56, 'Total Cost': 8.58, 'Plan Rows': 1, 'Plan Width': 73, 'Index Cond': "((name)::text = '0000000000000000000000000000000000000000000000000000000006872584'::text)"}
[{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.56, 'Total Cost': 8.58, 'Index Cond': "((name)::text = '0000000000000000000000000000000000000000000000000000000006872584'::text)"}]
! 3 0.3938876986503601
{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.56, 'Total Cost': 8.58, 'Index Cond': "((name)::text = '0000000000000000000000000000000000000000000000000000000006872584'::text)"}
! 0 0.0
empty
! 1 0.0
empty
! 2 0.0
empty
* 3
[{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.56, 'Total Cost': 8.58, 'Index Cond': "((name)::text = '0000000000000000000000000000000000000000000000000000000006872584'::text)"}]
! 3 0.3938876986503601
{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.56, 'Total Cost': 8.58, 'Index Cond': "((name)::text = '0000000000000000000000000000000000000000000000000000000006872584'::text)"}
! 0 0.0
empty
! 1 0.0
empty
! 2 0.0
empty
* 4
[{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.56, 'Total Cost': 8.58, 'Index Cond': "((name)::text = '0000000000000000000000000000000000000000000000000000000006872584'::text)"}]
! 3 0.3938876986503601
{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.56, 'Total Cost': 8.58, 'Index Cond': "((name)::text = '0000000000000000000000000000000000000000000000000000000006872584'::text)"}
! 0 0.0
empty
! 1 0.0
empty
! 2 0.0
empty
* 5
[{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.56, 'Total Cost': 8.58, 'Index Cond': "((name)::text = '0000000000000000000000000000000000000000000000000000000006872584'::text)"}]
! 3 0.3938876986503601
{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.56, 'Total Cost': 8.58, 'Index Cond': "((name)::text = '0000000000000000000000000000000000000000000000000000000006872584'::text)"}
! 0 0.0
empty
! 1 0.0
empty
! 2 0.0
empty
======Top3 metric
total cpu usage_sys: 0.420210792415042
dsk/total_read: 0.28761302106461634
total cpu usage_usr: 0.11329681034708756
igs:
[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 70.33621978759766, 70.33621978759766, 70.33109283447266, 70.33109283447266, 70.33560180664062, 70.33560180664062, 70.33560180664062, 70.33560943603516]
Transformer(
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (2): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (3): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (4): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (5): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
    )
    (norm): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
  )
  (decoder): TransformerDecoder(
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (1): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (2): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (3): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (4): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (5): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
    )
    (norm): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
  )
)
Transformer(
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (2): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (3): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (4): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (5): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
    )
    (norm): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
  )
  (decoder): TransformerDecoder(
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (1): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (2): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (3): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (4): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (5): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
    )
    (norm): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
  )
)
[[[0.0, 0.0, 0.0, 0.3938876986503601], [0.0, 0.0, 0.0, 0.3938876986503601], [0.0, 0.0, 0.0, 0.3938876986503601], [0.0, 0.0, 0.0, 0.3938876986503601], [0.0, 0.0, 0.0, 0.3938876986503601]], [[0.0, 0.0, 0.0, 0.3938876986503601], [0.0, 0.0, 0.0, 0.3938876986503601], [0.0, 0.0, 0.0, 0.3938876986503601], [0.0, 0.0, 0.0, 0.3938876986503601], [0.0, 0.0, 0.0, 0.3938876986503601]]]
# 1
SELECT * FROM ACCOUNTS WHERE custid = $1
* 1
{'Node Type': 'Index Scan', 'Parallel Aware': False, 'Scan Direction': 'Forward', 'Index Name': 'pk_accounts', 'Relation Name': 'accounts', 'Alias': 'accounts', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 73, 'Index Cond': "(custid = '4091455'::bigint)"}
[{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': "(custid = '4091455'::bigint)"}]
! 3 0.3938876986503601
{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': "(custid = '4091455'::bigint)"}
! 0 0.0
empty
! 1 0.0
empty
! 2 0.0
empty
* 2
{'Node Type': 'Index Scan', 'Parallel Aware': False, 'Scan Direction': 'Forward', 'Index Name': 'pk_accounts', 'Relation Name': 'accounts', 'Alias': 'accounts', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 73, 'Index Cond': "(custid = '8258886'::bigint)"}
[{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': "(custid = '8258886'::bigint)"}]
! 3 0.3938876986503601
{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': "(custid = '8258886'::bigint)"}
! 0 0.0
empty
! 1 0.0
empty
! 2 0.0
empty
* 3
[{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': "(custid = '8258886'::bigint)"}]
! 3 0.3938876986503601
{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': "(custid = '8258886'::bigint)"}
! 0 0.0
empty
! 1 0.0
empty
! 2 0.0
empty
* 4
[{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': "(custid = '8258886'::bigint)"}]
! 3 0.3938876986503601
{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': "(custid = '8258886'::bigint)"}
! 0 0.0
empty
! 1 0.0
empty
! 2 0.0
empty
* 5
[{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': "(custid = '8258886'::bigint)"}]
! 3 0.3938876986503601
{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': "(custid = '8258886'::bigint)"}
! 0 0.0
empty
! 1 0.0
empty
! 2 0.0
empty
# 2
SELECT * FROM ACCOUNTS WHERE name = $1
* 1
{'Node Type': 'Index Scan', 'Parallel Aware': False, 'Scan Direction': 'Forward', 'Index Name': 'idx_accounts_name', 'Relation Name': 'accounts', 'Alias': 'accounts', 'Startup Cost': 0.56, 'Total Cost': 8.58, 'Plan Rows': 1, 'Plan Width': 73, 'Index Cond': "((name)::text = '0000000000000000000000000000000000000000000000000000000009338014'::text)"}
[{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.56, 'Total Cost': 8.58, 'Index Cond': "((name)::text = '0000000000000000000000000000000000000000000000000000000009338014'::text)"}]
! 3 0.3938876986503601
{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.56, 'Total Cost': 8.58, 'Index Cond': "((name)::text = '0000000000000000000000000000000000000000000000000000000009338014'::text)"}
! 0 0.0
empty
! 1 0.0
empty
! 2 0.0
empty
* 2
{'Node Type': 'Index Scan', 'Parallel Aware': False, 'Scan Direction': 'Forward', 'Index Name': 'idx_accounts_name', 'Relation Name': 'accounts', 'Alias': 'accounts', 'Startup Cost': 0.56, 'Total Cost': 8.58, 'Plan Rows': 1, 'Plan Width': 73, 'Index Cond': "((name)::text = '0000000000000000000000000000000000000000000000000000000006872584'::text)"}
[{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.56, 'Total Cost': 8.58, 'Index Cond': "((name)::text = '0000000000000000000000000000000000000000000000000000000006872584'::text)"}]
! 3 0.3938876986503601
{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.56, 'Total Cost': 8.58, 'Index Cond': "((name)::text = '0000000000000000000000000000000000000000000000000000000006872584'::text)"}
! 0 0.0
empty
! 1 0.0
empty
! 2 0.0
empty
* 3
[{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.56, 'Total Cost': 8.58, 'Index Cond': "((name)::text = '0000000000000000000000000000000000000000000000000000000006872584'::text)"}]
! 3 0.3938876986503601
{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.56, 'Total Cost': 8.58, 'Index Cond': "((name)::text = '0000000000000000000000000000000000000000000000000000000006872584'::text)"}
! 0 0.0
empty
! 1 0.0
empty
! 2 0.0
empty
* 4
[{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.56, 'Total Cost': 8.58, 'Index Cond': "((name)::text = '0000000000000000000000000000000000000000000000000000000006872584'::text)"}]
! 3 0.3938876986503601
{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.56, 'Total Cost': 8.58, 'Index Cond': "((name)::text = '0000000000000000000000000000000000000000000000000000000006872584'::text)"}
! 0 0.0
empty
! 1 0.0
empty
! 2 0.0
empty
* 5
[{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.56, 'Total Cost': 8.58, 'Index Cond': "((name)::text = '0000000000000000000000000000000000000000000000000000000006872584'::text)"}]
! 3 0.3938876986503601
{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.56, 'Total Cost': 8.58, 'Index Cond': "((name)::text = '0000000000000000000000000000000000000000000000000000000006872584'::text)"}
! 0 0.0
empty
! 1 0.0
empty
! 2 0.0
empty
======Top3 metric
dsk/total_read: 0.44359427145385044
total cpu usage_sys: 0.2089378910554507
total cpu usage_usr: 0.1456737334909821
igs:
[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 86.63456726074219, 86.63456726074219, 86.62825012207031, 86.62825012207031, 86.63380432128906, 86.63380432128906, 86.63380432128906, 86.63381958007812]
Transformer(
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (2): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (3): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (4): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (5): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
    )
    (norm): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
  )
  (decoder): TransformerDecoder(
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (1): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (2): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (3): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (4): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (5): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
    )
    (norm): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
  )
)
Transformer(
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (2): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (3): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (4): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (5): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
    )
    (norm): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
  )
  (decoder): TransformerDecoder(
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (1): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (2): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (3): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (4): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (5): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
    )
    (norm): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
  )
)
[[[0.0, 0.0, 0.0, 0.3938876986503601], [0.0, 0.0, 0.0, 0.3938876986503601], [0.0, 0.0, 0.0, 0.3938876986503601], [0.0, 0.0, 0.0, 0.3938876986503601], [0.0, 0.0, 0.0, 0.3938876986503601]], [[0.0, 0.0, 0.0, 0.3938876986503601], [0.0, 0.0, 0.0, 0.3938876986503601], [0.0, 0.0, 0.0, 0.3938876986503601], [0.0, 0.0, 0.0, 0.3938876986503601], [0.0, 0.0, 0.0, 0.3938876986503601]]]
# 1
SELECT * FROM ACCOUNTS WHERE custid = $1
* 1
{'Node Type': 'Index Scan', 'Parallel Aware': False, 'Scan Direction': 'Forward', 'Index Name': 'pk_accounts', 'Relation Name': 'accounts', 'Alias': 'accounts', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 73, 'Index Cond': "(custid = '4091455'::bigint)"}
[{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': "(custid = '4091455'::bigint)"}]
! 3 0.3938876986503601
{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': "(custid = '4091455'::bigint)"}
! 0 0.0
empty
! 1 0.0
empty
! 2 0.0
empty
* 2
{'Node Type': 'Index Scan', 'Parallel Aware': False, 'Scan Direction': 'Forward', 'Index Name': 'pk_accounts', 'Relation Name': 'accounts', 'Alias': 'accounts', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 73, 'Index Cond': "(custid = '8258886'::bigint)"}
[{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': "(custid = '8258886'::bigint)"}]
! 3 0.3938876986503601
{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': "(custid = '8258886'::bigint)"}
! 0 0.0
empty
! 1 0.0
empty
! 2 0.0
empty
* 3
[{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': "(custid = '8258886'::bigint)"}]
! 3 0.3938876986503601
{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': "(custid = '8258886'::bigint)"}
! 0 0.0
empty
! 1 0.0
empty
! 2 0.0
empty
* 4
[{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': "(custid = '8258886'::bigint)"}]
! 3 0.3938876986503601
{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': "(custid = '8258886'::bigint)"}
! 0 0.0
empty
! 1 0.0
empty
! 2 0.0
empty
* 5
[{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': "(custid = '8258886'::bigint)"}]
! 3 0.3938876986503601
{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': "(custid = '8258886'::bigint)"}
! 0 0.0
empty
! 1 0.0
empty
! 2 0.0
empty
# 2
SELECT * FROM ACCOUNTS WHERE name = $1
* 1
{'Node Type': 'Index Scan', 'Parallel Aware': False, 'Scan Direction': 'Forward', 'Index Name': 'idx_accounts_name', 'Relation Name': 'accounts', 'Alias': 'accounts', 'Startup Cost': 0.56, 'Total Cost': 8.58, 'Plan Rows': 1, 'Plan Width': 73, 'Index Cond': "((name)::text = '0000000000000000000000000000000000000000000000000000000009338014'::text)"}
[{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.56, 'Total Cost': 8.58, 'Index Cond': "((name)::text = '0000000000000000000000000000000000000000000000000000000009338014'::text)"}]
! 3 0.3938876986503601
{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.56, 'Total Cost': 8.58, 'Index Cond': "((name)::text = '0000000000000000000000000000000000000000000000000000000009338014'::text)"}
! 0 0.0
empty
! 1 0.0
empty
! 2 0.0
empty
* 2
{'Node Type': 'Index Scan', 'Parallel Aware': False, 'Scan Direction': 'Forward', 'Index Name': 'idx_accounts_name', 'Relation Name': 'accounts', 'Alias': 'accounts', 'Startup Cost': 0.56, 'Total Cost': 8.58, 'Plan Rows': 1, 'Plan Width': 73, 'Index Cond': "((name)::text = '0000000000000000000000000000000000000000000000000000000006872584'::text)"}
[{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.56, 'Total Cost': 8.58, 'Index Cond': "((name)::text = '0000000000000000000000000000000000000000000000000000000006872584'::text)"}]
! 3 0.3938876986503601
{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.56, 'Total Cost': 8.58, 'Index Cond': "((name)::text = '0000000000000000000000000000000000000000000000000000000006872584'::text)"}
! 0 0.0
empty
! 1 0.0
empty
! 2 0.0
empty
* 3
[{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.56, 'Total Cost': 8.58, 'Index Cond': "((name)::text = '0000000000000000000000000000000000000000000000000000000006872584'::text)"}]
! 3 0.3938876986503601
{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.56, 'Total Cost': 8.58, 'Index Cond': "((name)::text = '0000000000000000000000000000000000000000000000000000000006872584'::text)"}
! 0 0.0
empty
! 1 0.0
empty
! 2 0.0
empty
* 4
[{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.56, 'Total Cost': 8.58, 'Index Cond': "((name)::text = '0000000000000000000000000000000000000000000000000000000006872584'::text)"}]
! 3 0.3938876986503601
{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.56, 'Total Cost': 8.58, 'Index Cond': "((name)::text = '0000000000000000000000000000000000000000000000000000000006872584'::text)"}
! 0 0.0
empty
! 1 0.0
empty
! 2 0.0
empty
* 5
[{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.56, 'Total Cost': 8.58, 'Index Cond': "((name)::text = '0000000000000000000000000000000000000000000000000000000006872584'::text)"}]
! 3 0.3938876986503601
{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.56, 'Total Cost': 8.58, 'Index Cond': "((name)::text = '0000000000000000000000000000000000000000000000000000000006872584'::text)"}
! 0 0.0
empty
! 1 0.0
empty
! 2 0.0
empty
======Top3 metric
total cpu usage_sys: 0.44867899379271464
dsk/total_read: 0.29380543607943244
total cpu usage_usr: 0.08085888290268234
igs:
[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 57.63913345336914, 57.63913345336914, 57.63492965698242, 57.63492965698242, 57.63862609863281, 57.63862609863281, 57.63862609863281, 57.63863754272461]
Transformer(
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (2): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (3): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (4): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (5): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
    )
    (norm): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
  )
  (decoder): TransformerDecoder(
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (1): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (2): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (3): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (4): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (5): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
    )
    (norm): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
  )
)
Transformer(
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (2): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (3): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (4): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (5): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
    )
    (norm): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
  )
  (decoder): TransformerDecoder(
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (1): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (2): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (3): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (4): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (5): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
    )
    (norm): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
  )
)
[[[0.0, 0.0, 0.0, 0.3938876986503601], [0.0, 0.0, 0.0, 0.3938876986503601], [0.0, 0.0, 0.0, 0.3938876986503601], [0.0, 0.0, 0.0, 0.3938876986503601], [0.0, 0.0, 0.0, 0.3938876986503601]], [[0.0, 0.0, 0.0, 0.3938876986503601], [0.0, 0.0, 0.0, 0.3938876986503601], [0.0, 0.0, 0.0, 0.3938876986503601], [0.0, 0.0, 0.0, 0.3938876986503601], [0.0, 0.0, 0.0, 0.3938876986503601]]]
# 1
SELECT * FROM ACCOUNTS WHERE custid = $1
* 1
{'Node Type': 'Index Scan', 'Parallel Aware': False, 'Scan Direction': 'Forward', 'Index Name': 'pk_accounts', 'Relation Name': 'accounts', 'Alias': 'accounts', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 73, 'Index Cond': "(custid = '4091455'::bigint)"}
[{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': "(custid = '4091455'::bigint)"}]
! 3 0.3938876986503601
{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': "(custid = '4091455'::bigint)"}
! 0 0.0
empty
! 1 0.0
empty
! 2 0.0
empty
* 2
{'Node Type': 'Index Scan', 'Parallel Aware': False, 'Scan Direction': 'Forward', 'Index Name': 'pk_accounts', 'Relation Name': 'accounts', 'Alias': 'accounts', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 73, 'Index Cond': "(custid = '8258886'::bigint)"}
[{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': "(custid = '8258886'::bigint)"}]
! 3 0.3938876986503601
{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': "(custid = '8258886'::bigint)"}
! 0 0.0
empty
! 1 0.0
empty
! 2 0.0
empty
* 3
[{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': "(custid = '8258886'::bigint)"}]
! 3 0.3938876986503601
{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': "(custid = '8258886'::bigint)"}
! 0 0.0
empty
! 1 0.0
empty
! 2 0.0
empty
* 4
[{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': "(custid = '8258886'::bigint)"}]
! 3 0.3938876986503601
{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': "(custid = '8258886'::bigint)"}
! 0 0.0
empty
! 1 0.0
empty
! 2 0.0
empty
* 5
[{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': "(custid = '8258886'::bigint)"}]
! 3 0.3938876986503601
{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': "(custid = '8258886'::bigint)"}
! 0 0.0
empty
! 1 0.0
empty
! 2 0.0
empty
# 2
SELECT * FROM ACCOUNTS WHERE name = $1
* 1
{'Node Type': 'Index Scan', 'Parallel Aware': False, 'Scan Direction': 'Forward', 'Index Name': 'idx_accounts_name', 'Relation Name': 'accounts', 'Alias': 'accounts', 'Startup Cost': 0.56, 'Total Cost': 8.58, 'Plan Rows': 1, 'Plan Width': 73, 'Index Cond': "((name)::text = '0000000000000000000000000000000000000000000000000000000009338014'::text)"}
[{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.56, 'Total Cost': 8.58, 'Index Cond': "((name)::text = '0000000000000000000000000000000000000000000000000000000009338014'::text)"}]
! 3 0.3938876986503601
{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.56, 'Total Cost': 8.58, 'Index Cond': "((name)::text = '0000000000000000000000000000000000000000000000000000000009338014'::text)"}
! 0 0.0
empty
! 1 0.0
empty
! 2 0.0
empty
* 2
{'Node Type': 'Index Scan', 'Parallel Aware': False, 'Scan Direction': 'Forward', 'Index Name': 'idx_accounts_name', 'Relation Name': 'accounts', 'Alias': 'accounts', 'Startup Cost': 0.56, 'Total Cost': 8.58, 'Plan Rows': 1, 'Plan Width': 73, 'Index Cond': "((name)::text = '0000000000000000000000000000000000000000000000000000000006872584'::text)"}
[{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.56, 'Total Cost': 8.58, 'Index Cond': "((name)::text = '0000000000000000000000000000000000000000000000000000000006872584'::text)"}]
! 3 0.3938876986503601
{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.56, 'Total Cost': 8.58, 'Index Cond': "((name)::text = '0000000000000000000000000000000000000000000000000000000006872584'::text)"}
! 0 0.0
empty
! 1 0.0
empty
! 2 0.0
empty
* 3
[{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.56, 'Total Cost': 8.58, 'Index Cond': "((name)::text = '0000000000000000000000000000000000000000000000000000000006872584'::text)"}]
! 3 0.3938876986503601
{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.56, 'Total Cost': 8.58, 'Index Cond': "((name)::text = '0000000000000000000000000000000000000000000000000000000006872584'::text)"}
! 0 0.0
empty
! 1 0.0
empty
! 2 0.0
empty
* 4
[{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.56, 'Total Cost': 8.58, 'Index Cond': "((name)::text = '0000000000000000000000000000000000000000000000000000000006872584'::text)"}]
! 3 0.3938876986503601
{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.56, 'Total Cost': 8.58, 'Index Cond': "((name)::text = '0000000000000000000000000000000000000000000000000000000006872584'::text)"}
! 0 0.0
empty
! 1 0.0
empty
! 2 0.0
empty
* 5
[{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.56, 'Total Cost': 8.58, 'Index Cond': "((name)::text = '0000000000000000000000000000000000000000000000000000000006872584'::text)"}]
! 3 0.3938876986503601
{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.56, 'Total Cost': 8.58, 'Index Cond': "((name)::text = '0000000000000000000000000000000000000000000000000000000006872584'::text)"}
! 0 0.0
empty
! 1 0.0
empty
! 2 0.0
empty
======Top3 metric
total cpu usage_sys: 0.43440286650232307
dsk/total_read: 0.263319521814563
total cpu usage_usr: 0.0987579833269445
igs:
[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 72.4789047241211, 72.4789047241211, 72.47361755371094, 72.47361755371094, 72.47826385498047, 72.47826385498047, 72.47826385498047, 72.478271484375]
Transformer(
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (2): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (3): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (4): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (5): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
    )
    (norm): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
  )
  (decoder): TransformerDecoder(
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (1): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (2): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (3): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (4): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (5): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
    )
    (norm): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
  )
)
Transformer(
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (2): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (3): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (4): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (5): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
    )
    (norm): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
  )
  (decoder): TransformerDecoder(
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (1): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (2): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (3): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (4): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (5): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
    )
    (norm): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
  )
)
[[[0.0, 0.0, 0.0, 0.3938876986503601], [0.0, 0.0, 0.0, 0.3938876986503601], [0.0, 0.0, 0.0, 0.3938876986503601], [0.0, 0.0, 0.0, 0.3938876986503601], [0.0, 0.0, 0.0, 0.3938876986503601]], [[0.0, 0.0, 0.0, 0.3938876986503601], [0.0, 0.0, 0.0, 0.3938876986503601], [0.0, 0.0, 0.0, 0.3938876986503601], [0.0, 0.0, 0.0, 0.3938876986503601], [0.0, 0.0, 0.0, 0.3938876986503601]]]
# 1
SELECT * FROM ACCOUNTS WHERE custid = $1
* 1
{'Node Type': 'Index Scan', 'Parallel Aware': False, 'Scan Direction': 'Forward', 'Index Name': 'pk_accounts', 'Relation Name': 'accounts', 'Alias': 'accounts', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 73, 'Index Cond': "(custid = '4091455'::bigint)"}
[{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': "(custid = '4091455'::bigint)"}]
! 3 0.3938876986503601
{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': "(custid = '4091455'::bigint)"}
! 0 0.0
empty
! 1 0.0
empty
! 2 0.0
empty
* 2
{'Node Type': 'Index Scan', 'Parallel Aware': False, 'Scan Direction': 'Forward', 'Index Name': 'pk_accounts', 'Relation Name': 'accounts', 'Alias': 'accounts', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 73, 'Index Cond': "(custid = '8258886'::bigint)"}
[{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': "(custid = '8258886'::bigint)"}]
! 3 0.3938876986503601
{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': "(custid = '8258886'::bigint)"}
! 0 0.0
empty
! 1 0.0
empty
! 2 0.0
empty
* 3
[{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': "(custid = '8258886'::bigint)"}]
! 3 0.3938876986503601
{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': "(custid = '8258886'::bigint)"}
! 0 0.0
empty
! 1 0.0
empty
! 2 0.0
empty
* 4
[{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': "(custid = '8258886'::bigint)"}]
! 3 0.3938876986503601
{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': "(custid = '8258886'::bigint)"}
! 0 0.0
empty
! 1 0.0
empty
! 2 0.0
empty
* 5
[{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': "(custid = '8258886'::bigint)"}]
! 3 0.3938876986503601
{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': "(custid = '8258886'::bigint)"}
! 0 0.0
empty
! 1 0.0
empty
! 2 0.0
empty
# 2
SELECT * FROM ACCOUNTS WHERE name = $1
* 1
{'Node Type': 'Index Scan', 'Parallel Aware': False, 'Scan Direction': 'Forward', 'Index Name': 'idx_accounts_name', 'Relation Name': 'accounts', 'Alias': 'accounts', 'Startup Cost': 0.56, 'Total Cost': 8.58, 'Plan Rows': 1, 'Plan Width': 73, 'Index Cond': "((name)::text = '0000000000000000000000000000000000000000000000000000000009338014'::text)"}
[{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.56, 'Total Cost': 8.58, 'Index Cond': "((name)::text = '0000000000000000000000000000000000000000000000000000000009338014'::text)"}]
! 3 0.3938876986503601
{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.56, 'Total Cost': 8.58, 'Index Cond': "((name)::text = '0000000000000000000000000000000000000000000000000000000009338014'::text)"}
! 0 0.0
empty
! 1 0.0
empty
! 2 0.0
empty
* 2
{'Node Type': 'Index Scan', 'Parallel Aware': False, 'Scan Direction': 'Forward', 'Index Name': 'idx_accounts_name', 'Relation Name': 'accounts', 'Alias': 'accounts', 'Startup Cost': 0.56, 'Total Cost': 8.58, 'Plan Rows': 1, 'Plan Width': 73, 'Index Cond': "((name)::text = '0000000000000000000000000000000000000000000000000000000006872584'::text)"}
[{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.56, 'Total Cost': 8.58, 'Index Cond': "((name)::text = '0000000000000000000000000000000000000000000000000000000006872584'::text)"}]
! 3 0.3938876986503601
{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.56, 'Total Cost': 8.58, 'Index Cond': "((name)::text = '0000000000000000000000000000000000000000000000000000000006872584'::text)"}
! 0 0.0
empty
! 1 0.0
empty
! 2 0.0
empty
* 3
[{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.56, 'Total Cost': 8.58, 'Index Cond': "((name)::text = '0000000000000000000000000000000000000000000000000000000006872584'::text)"}]
! 3 0.3938876986503601
{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.56, 'Total Cost': 8.58, 'Index Cond': "((name)::text = '0000000000000000000000000000000000000000000000000000000006872584'::text)"}
! 0 0.0
empty
! 1 0.0
empty
! 2 0.0
empty
* 4
[{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.56, 'Total Cost': 8.58, 'Index Cond': "((name)::text = '0000000000000000000000000000000000000000000000000000000006872584'::text)"}]
! 3 0.3938876986503601
{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.56, 'Total Cost': 8.58, 'Index Cond': "((name)::text = '0000000000000000000000000000000000000000000000000000000006872584'::text)"}
! 0 0.0
empty
! 1 0.0
empty
! 2 0.0
empty
* 5
[{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.56, 'Total Cost': 8.58, 'Index Cond': "((name)::text = '0000000000000000000000000000000000000000000000000000000006872584'::text)"}]
! 3 0.3938876986503601
{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.56, 'Total Cost': 8.58, 'Index Cond': "((name)::text = '0000000000000000000000000000000000000000000000000000000006872584'::text)"}
! 0 0.0
empty
! 1 0.0
empty
! 2 0.0
empty
======Top3 metric
dsk/total_read: 0.38176928023791235
total cpu usage_usr: 0.23194993441837247
total cpu usage_sys: 0.20655883264734945
igs:
[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 64.6769027709961, 64.6769027709961, 64.67218017578125, 64.67218017578125, 64.67633056640625, 64.67633056640625, 64.67633056640625, 64.67633819580078]
Transformer(
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (2): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (3): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (4): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (5): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
    )
    (norm): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
  )
  (decoder): TransformerDecoder(
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (1): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (2): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (3): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (4): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (5): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
    )
    (norm): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
  )
)
Transformer(
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (2): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (3): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (4): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (5): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
    )
    (norm): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
  )
  (decoder): TransformerDecoder(
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (1): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (2): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (3): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (4): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (5): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
    )
    (norm): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
  )
)
[[[0.0, 0.0, 0.0, 0.3938876986503601], [0.0, 0.0, 0.0, 0.3938876986503601], [0.0, 0.0, 0.0, 0.3938876986503601], [0.0, 0.0, 0.0, 0.3938876986503601], [0.0, 0.0, 0.0, 0.3938876986503601]], [[0.0, 0.0, 0.0, 0.3938876986503601], [0.0, 0.0, 0.0, 0.3938876986503601], [0.0, 0.0, 0.0, 0.3938876986503601], [0.0, 0.0, 0.0, 0.3938876986503601], [0.0, 0.0, 0.0, 0.3938876986503601]]]
# 1
SELECT * FROM ACCOUNTS WHERE custid = $1
* 1
{'Node Type': 'Index Scan', 'Parallel Aware': False, 'Scan Direction': 'Forward', 'Index Name': 'pk_accounts', 'Relation Name': 'accounts', 'Alias': 'accounts', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 73, 'Index Cond': "(custid = '4091455'::bigint)"}
[{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': "(custid = '4091455'::bigint)"}]
! 3 0.3938876986503601
{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': "(custid = '4091455'::bigint)"}
! 0 0.0
empty
! 1 0.0
empty
! 2 0.0
empty
* 2
{'Node Type': 'Index Scan', 'Parallel Aware': False, 'Scan Direction': 'Forward', 'Index Name': 'pk_accounts', 'Relation Name': 'accounts', 'Alias': 'accounts', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 73, 'Index Cond': "(custid = '8258886'::bigint)"}
[{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': "(custid = '8258886'::bigint)"}]
! 3 0.3938876986503601
{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': "(custid = '8258886'::bigint)"}
! 0 0.0
empty
! 1 0.0
empty
! 2 0.0
empty
* 3
[{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': "(custid = '8258886'::bigint)"}]
! 3 0.3938876986503601
{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': "(custid = '8258886'::bigint)"}
! 0 0.0
empty
! 1 0.0
empty
! 2 0.0
empty
* 4
[{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': "(custid = '8258886'::bigint)"}]
! 3 0.3938876986503601
{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': "(custid = '8258886'::bigint)"}
! 0 0.0
empty
! 1 0.0
empty
! 2 0.0
empty
* 5
[{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': "(custid = '8258886'::bigint)"}]
! 3 0.3938876986503601
{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': "(custid = '8258886'::bigint)"}
! 0 0.0
empty
! 1 0.0
empty
! 2 0.0
empty
# 2
SELECT * FROM ACCOUNTS WHERE name = $1
* 1
{'Node Type': 'Index Scan', 'Parallel Aware': False, 'Scan Direction': 'Forward', 'Index Name': 'idx_accounts_name', 'Relation Name': 'accounts', 'Alias': 'accounts', 'Startup Cost': 0.56, 'Total Cost': 8.58, 'Plan Rows': 1, 'Plan Width': 73, 'Index Cond': "((name)::text = '0000000000000000000000000000000000000000000000000000000009338014'::text)"}
[{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.56, 'Total Cost': 8.58, 'Index Cond': "((name)::text = '0000000000000000000000000000000000000000000000000000000009338014'::text)"}]
! 3 0.3938876986503601
{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.56, 'Total Cost': 8.58, 'Index Cond': "((name)::text = '0000000000000000000000000000000000000000000000000000000009338014'::text)"}
! 0 0.0
empty
! 1 0.0
empty
! 2 0.0
empty
* 2
{'Node Type': 'Index Scan', 'Parallel Aware': False, 'Scan Direction': 'Forward', 'Index Name': 'idx_accounts_name', 'Relation Name': 'accounts', 'Alias': 'accounts', 'Startup Cost': 0.56, 'Total Cost': 8.58, 'Plan Rows': 1, 'Plan Width': 73, 'Index Cond': "((name)::text = '0000000000000000000000000000000000000000000000000000000006872584'::text)"}
[{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.56, 'Total Cost': 8.58, 'Index Cond': "((name)::text = '0000000000000000000000000000000000000000000000000000000006872584'::text)"}]
! 3 0.3938876986503601
{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.56, 'Total Cost': 8.58, 'Index Cond': "((name)::text = '0000000000000000000000000000000000000000000000000000000006872584'::text)"}
! 0 0.0
empty
! 1 0.0
empty
! 2 0.0
empty
* 3
[{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.56, 'Total Cost': 8.58, 'Index Cond': "((name)::text = '0000000000000000000000000000000000000000000000000000000006872584'::text)"}]
! 3 0.3938876986503601
{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.56, 'Total Cost': 8.58, 'Index Cond': "((name)::text = '0000000000000000000000000000000000000000000000000000000006872584'::text)"}
! 0 0.0
empty
! 1 0.0
empty
! 2 0.0
empty
* 4
[{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.56, 'Total Cost': 8.58, 'Index Cond': "((name)::text = '0000000000000000000000000000000000000000000000000000000006872584'::text)"}]
! 3 0.3938876986503601
{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.56, 'Total Cost': 8.58, 'Index Cond': "((name)::text = '0000000000000000000000000000000000000000000000000000000006872584'::text)"}
! 0 0.0
empty
! 1 0.0
empty
! 2 0.0
empty
* 5
[{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.56, 'Total Cost': 8.58, 'Index Cond': "((name)::text = '0000000000000000000000000000000000000000000000000000000006872584'::text)"}]
! 3 0.3938876986503601
{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.56, 'Total Cost': 8.58, 'Index Cond': "((name)::text = '0000000000000000000000000000000000000000000000000000000006872584'::text)"}
! 0 0.0
empty
! 1 0.0
empty
! 2 0.0
empty
======Top3 metric
total cpu usage_sys: 0.45182061502257825
dsk/total_read: 0.2864647316485426
total cpu usage_usr: 0.087377999742125
igs:
[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 47.560970306396484, 47.560970306396484, 47.55750274658203, 47.55750274658203, 47.560550689697266, 47.560550689697266, 47.560550689697266, 47.5605583190918]
Transformer(
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (2): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (3): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (4): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (5): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
    )
    (norm): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
  )
  (decoder): TransformerDecoder(
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (1): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (2): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (3): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (4): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (5): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
    )
    (norm): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
  )
)
Transformer(
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (2): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (3): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (4): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (5): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
    )
    (norm): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
  )
  (decoder): TransformerDecoder(
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (1): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (2): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (3): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (4): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (5): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
    )
    (norm): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
  )
)
[[[0.0, 0.0, 0.0, 0.3938876986503601], [0.0, 0.0, 0.0, 0.3938876986503601], [0.0, 0.0, 0.0, 0.3938876986503601], [0.0, 0.0, 0.0, 0.3938876986503601], [0.0, 0.0, 0.0, 0.3938876986503601]], [[0.0, 0.0, 0.0, 0.3938876986503601], [0.0, 0.0, 0.0, 0.3938876986503601], [0.0, 0.0, 0.0, 0.3938876986503601], [0.0, 0.0, 0.0, 0.3938876986503601], [0.0, 0.0, 0.0, 0.3938876986503601]]]
# 1
SELECT * FROM ACCOUNTS WHERE custid = $1
* 1
{'Node Type': 'Index Scan', 'Parallel Aware': False, 'Scan Direction': 'Forward', 'Index Name': 'pk_accounts', 'Relation Name': 'accounts', 'Alias': 'accounts', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 73, 'Index Cond': "(custid = '4091455'::bigint)"}
[{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': "(custid = '4091455'::bigint)"}]
! 3 0.3938876986503601
{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': "(custid = '4091455'::bigint)"}
! 0 0.0
empty
! 1 0.0
empty
! 2 0.0
empty
* 2
{'Node Type': 'Index Scan', 'Parallel Aware': False, 'Scan Direction': 'Forward', 'Index Name': 'pk_accounts', 'Relation Name': 'accounts', 'Alias': 'accounts', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 73, 'Index Cond': "(custid = '8258886'::bigint)"}
[{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': "(custid = '8258886'::bigint)"}]
! 3 0.3938876986503601
{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': "(custid = '8258886'::bigint)"}
! 0 0.0
empty
! 1 0.0
empty
! 2 0.0
empty
* 3
[{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': "(custid = '8258886'::bigint)"}]
! 3 0.3938876986503601
{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': "(custid = '8258886'::bigint)"}
! 0 0.0
empty
! 1 0.0
empty
! 2 0.0
empty
* 4
[{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': "(custid = '8258886'::bigint)"}]
! 3 0.3938876986503601
{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': "(custid = '8258886'::bigint)"}
! 0 0.0
empty
! 1 0.0
empty
! 2 0.0
empty
* 5
[{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': "(custid = '8258886'::bigint)"}]
! 3 0.3938876986503601
{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': "(custid = '8258886'::bigint)"}
! 0 0.0
empty
! 1 0.0
empty
! 2 0.0
empty
# 2
SELECT * FROM ACCOUNTS WHERE name = $1
* 1
{'Node Type': 'Index Scan', 'Parallel Aware': False, 'Scan Direction': 'Forward', 'Index Name': 'idx_accounts_name', 'Relation Name': 'accounts', 'Alias': 'accounts', 'Startup Cost': 0.56, 'Total Cost': 8.58, 'Plan Rows': 1, 'Plan Width': 73, 'Index Cond': "((name)::text = '0000000000000000000000000000000000000000000000000000000009338014'::text)"}
[{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.56, 'Total Cost': 8.58, 'Index Cond': "((name)::text = '0000000000000000000000000000000000000000000000000000000009338014'::text)"}]
! 3 0.3938876986503601
{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.56, 'Total Cost': 8.58, 'Index Cond': "((name)::text = '0000000000000000000000000000000000000000000000000000000009338014'::text)"}
! 0 0.0
empty
! 1 0.0
empty
! 2 0.0
empty
* 2
{'Node Type': 'Index Scan', 'Parallel Aware': False, 'Scan Direction': 'Forward', 'Index Name': 'idx_accounts_name', 'Relation Name': 'accounts', 'Alias': 'accounts', 'Startup Cost': 0.56, 'Total Cost': 8.58, 'Plan Rows': 1, 'Plan Width': 73, 'Index Cond': "((name)::text = '0000000000000000000000000000000000000000000000000000000006872584'::text)"}
[{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.56, 'Total Cost': 8.58, 'Index Cond': "((name)::text = '0000000000000000000000000000000000000000000000000000000006872584'::text)"}]
! 3 0.3938876986503601
{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.56, 'Total Cost': 8.58, 'Index Cond': "((name)::text = '0000000000000000000000000000000000000000000000000000000006872584'::text)"}
! 0 0.0
empty
! 1 0.0
empty
! 2 0.0
empty
* 3
[{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.56, 'Total Cost': 8.58, 'Index Cond': "((name)::text = '0000000000000000000000000000000000000000000000000000000006872584'::text)"}]
! 3 0.3938876986503601
{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.56, 'Total Cost': 8.58, 'Index Cond': "((name)::text = '0000000000000000000000000000000000000000000000000000000006872584'::text)"}
! 0 0.0
empty
! 1 0.0
empty
! 2 0.0
empty
* 4
[{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.56, 'Total Cost': 8.58, 'Index Cond': "((name)::text = '0000000000000000000000000000000000000000000000000000000006872584'::text)"}]
! 3 0.3938876986503601
{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.56, 'Total Cost': 8.58, 'Index Cond': "((name)::text = '0000000000000000000000000000000000000000000000000000000006872584'::text)"}
! 0 0.0
empty
! 1 0.0
empty
! 2 0.0
empty
* 5
[{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.56, 'Total Cost': 8.58, 'Index Cond': "((name)::text = '0000000000000000000000000000000000000000000000000000000006872584'::text)"}]
! 3 0.3938876986503601
{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.56, 'Total Cost': 8.58, 'Index Cond': "((name)::text = '0000000000000000000000000000000000000000000000000000000006872584'::text)"}
! 0 0.0
empty
! 1 0.0
empty
! 2 0.0
empty
======Top3 metric
total cpu usage_sys: 0.41728351683341436
dsk/total_read: 0.28408022079322376
total cpu usage_usr: 0.11741786669491372
igs:
[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 76.79173278808594, 76.79173278808594, 76.7861328125, 76.7861328125, 76.79105377197266, 76.79105377197266, 76.79105377197266, 76.79106903076172]
Transformer(
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (2): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (3): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (4): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (5): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
    )
    (norm): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
  )
  (decoder): TransformerDecoder(
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (1): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (2): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (3): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (4): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (5): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
    )
    (norm): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
  )
)
Transformer(
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (2): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (3): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (4): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (5): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
    )
    (norm): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
  )
  (decoder): TransformerDecoder(
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (1): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (2): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (3): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (4): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (5): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
    )
    (norm): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
  )
)
[[[0.0, 0.0, 0.0, 0.3938876986503601], [0.0, 0.0, 0.0, 0.3938876986503601], [0.0, 0.0, 0.0, 0.3938876986503601], [0.0, 0.0, 0.0, 0.3938876986503601], [0.0, 0.0, 0.0, 0.3938876986503601]], [[0.0, 0.0, 0.0, 0.3938876986503601], [0.0, 0.0, 0.0, 0.3938876986503601], [0.0, 0.0, 0.0, 0.3938876986503601], [0.0, 0.0, 0.0, 0.3938876986503601], [0.0, 0.0, 0.0, 0.3938876986503601]]]
# 1
SELECT * FROM ACCOUNTS WHERE custid = $1
* 1
{'Node Type': 'Index Scan', 'Parallel Aware': False, 'Scan Direction': 'Forward', 'Index Name': 'pk_accounts', 'Relation Name': 'accounts', 'Alias': 'accounts', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 73, 'Index Cond': "(custid = '4091455'::bigint)"}
[{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': "(custid = '4091455'::bigint)"}]
! 3 0.3938876986503601
{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': "(custid = '4091455'::bigint)"}
! 0 0.0
empty
! 1 0.0
empty
! 2 0.0
empty
* 2
{'Node Type': 'Index Scan', 'Parallel Aware': False, 'Scan Direction': 'Forward', 'Index Name': 'pk_accounts', 'Relation Name': 'accounts', 'Alias': 'accounts', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 73, 'Index Cond': "(custid = '8258886'::bigint)"}
[{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': "(custid = '8258886'::bigint)"}]
! 3 0.3938876986503601
{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': "(custid = '8258886'::bigint)"}
! 0 0.0
empty
! 1 0.0
empty
! 2 0.0
empty
* 3
[{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': "(custid = '8258886'::bigint)"}]
! 3 0.3938876986503601
{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': "(custid = '8258886'::bigint)"}
! 0 0.0
empty
! 1 0.0
empty
! 2 0.0
empty
* 4
[{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': "(custid = '8258886'::bigint)"}]
! 3 0.3938876986503601
{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': "(custid = '8258886'::bigint)"}
! 0 0.0
empty
! 1 0.0
empty
! 2 0.0
empty
* 5
[{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': "(custid = '8258886'::bigint)"}]
! 3 0.3938876986503601
{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': "(custid = '8258886'::bigint)"}
! 0 0.0
empty
! 1 0.0
empty
! 2 0.0
empty
# 2
SELECT * FROM ACCOUNTS WHERE name = $1
* 1
{'Node Type': 'Index Scan', 'Parallel Aware': False, 'Scan Direction': 'Forward', 'Index Name': 'idx_accounts_name', 'Relation Name': 'accounts', 'Alias': 'accounts', 'Startup Cost': 0.56, 'Total Cost': 8.58, 'Plan Rows': 1, 'Plan Width': 73, 'Index Cond': "((name)::text = '0000000000000000000000000000000000000000000000000000000009338014'::text)"}
[{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.56, 'Total Cost': 8.58, 'Index Cond': "((name)::text = '0000000000000000000000000000000000000000000000000000000009338014'::text)"}]
! 3 0.3938876986503601
{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.56, 'Total Cost': 8.58, 'Index Cond': "((name)::text = '0000000000000000000000000000000000000000000000000000000009338014'::text)"}
! 0 0.0
empty
! 1 0.0
empty
! 2 0.0
empty
* 2
{'Node Type': 'Index Scan', 'Parallel Aware': False, 'Scan Direction': 'Forward', 'Index Name': 'idx_accounts_name', 'Relation Name': 'accounts', 'Alias': 'accounts', 'Startup Cost': 0.56, 'Total Cost': 8.58, 'Plan Rows': 1, 'Plan Width': 73, 'Index Cond': "((name)::text = '0000000000000000000000000000000000000000000000000000000006872584'::text)"}
[{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.56, 'Total Cost': 8.58, 'Index Cond': "((name)::text = '0000000000000000000000000000000000000000000000000000000006872584'::text)"}]
! 3 0.3938876986503601
{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.56, 'Total Cost': 8.58, 'Index Cond': "((name)::text = '0000000000000000000000000000000000000000000000000000000006872584'::text)"}
! 0 0.0
empty
! 1 0.0
empty
! 2 0.0
empty
* 3
[{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.56, 'Total Cost': 8.58, 'Index Cond': "((name)::text = '0000000000000000000000000000000000000000000000000000000006872584'::text)"}]
! 3 0.3938876986503601
{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.56, 'Total Cost': 8.58, 'Index Cond': "((name)::text = '0000000000000000000000000000000000000000000000000000000006872584'::text)"}
! 0 0.0
empty
! 1 0.0
empty
! 2 0.0
empty
* 4
[{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.56, 'Total Cost': 8.58, 'Index Cond': "((name)::text = '0000000000000000000000000000000000000000000000000000000006872584'::text)"}]
! 3 0.3938876986503601
{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.56, 'Total Cost': 8.58, 'Index Cond': "((name)::text = '0000000000000000000000000000000000000000000000000000000006872584'::text)"}
! 0 0.0
empty
! 1 0.0
empty
! 2 0.0
empty
* 5
[{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.56, 'Total Cost': 8.58, 'Index Cond': "((name)::text = '0000000000000000000000000000000000000000000000000000000006872584'::text)"}]
! 3 0.3938876986503601
{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.56, 'Total Cost': 8.58, 'Index Cond': "((name)::text = '0000000000000000000000000000000000000000000000000000000006872584'::text)"}
! 0 0.0
empty
! 1 0.0
empty
! 2 0.0
empty
======Top3 metric
total cpu usage_sys: 0.41919212706181713
dsk/total_read: 0.28127360513021915
total cpu usage_usr: 0.11857490696647308
igs:
[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 71.93724822998047, 71.93724822998047, 71.93199920654297, 71.93199920654297, 71.93660736083984, 71.93660736083984, 71.93660736083984, 71.9366226196289]
Transformer(
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (2): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (3): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (4): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (5): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
    )
    (norm): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
  )
  (decoder): TransformerDecoder(
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (1): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (2): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (3): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (4): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (5): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
    )
    (norm): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
  )
)
Transformer(
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (2): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (3): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (4): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (5): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
    )
    (norm): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
  )
  (decoder): TransformerDecoder(
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (1): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (2): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (3): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (4): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (5): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
    )
    (norm): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
  )
)
[[[0.0, 0.0, 0.0, 0.3938876986503601], [0.0, 0.0, 0.0, 0.3938876986503601], [0.0, 0.0, 0.0, 0.3938876986503601], [0.0, 0.0, 0.0, 0.3938876986503601], [0.0, 0.0, 0.0, 0.3938876986503601]], [[0.0, 0.0, 0.0, 0.3938876986503601], [0.0, 0.0, 0.0, 0.3938876986503601], [0.0, 0.0, 0.0, 0.3938876986503601], [0.0, 0.0, 0.0, 0.3938876986503601], [0.0, 0.0, 0.0, 0.3938876986503601]]]
# 1
SELECT * FROM ACCOUNTS WHERE custid = $1
* 1
{'Node Type': 'Index Scan', 'Parallel Aware': False, 'Scan Direction': 'Forward', 'Index Name': 'pk_accounts', 'Relation Name': 'accounts', 'Alias': 'accounts', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 73, 'Index Cond': "(custid = '4091455'::bigint)"}
[{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': "(custid = '4091455'::bigint)"}]
! 3 0.3938876986503601
{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': "(custid = '4091455'::bigint)"}
! 0 0.0
empty
! 1 0.0
empty
! 2 0.0
empty
* 2
{'Node Type': 'Index Scan', 'Parallel Aware': False, 'Scan Direction': 'Forward', 'Index Name': 'pk_accounts', 'Relation Name': 'accounts', 'Alias': 'accounts', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 73, 'Index Cond': "(custid = '8258886'::bigint)"}
[{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': "(custid = '8258886'::bigint)"}]
! 3 0.3938876986503601
{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': "(custid = '8258886'::bigint)"}
! 0 0.0
empty
! 1 0.0
empty
! 2 0.0
empty
* 3
[{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': "(custid = '8258886'::bigint)"}]
! 3 0.3938876986503601
{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': "(custid = '8258886'::bigint)"}
! 0 0.0
empty
! 1 0.0
empty
! 2 0.0
empty
* 4
[{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': "(custid = '8258886'::bigint)"}]
! 3 0.3938876986503601
{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': "(custid = '8258886'::bigint)"}
! 0 0.0
empty
! 1 0.0
empty
! 2 0.0
empty
* 5
[{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': "(custid = '8258886'::bigint)"}]
! 3 0.3938876986503601
{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': "(custid = '8258886'::bigint)"}
! 0 0.0
empty
! 1 0.0
empty
! 2 0.0
empty
# 2
SELECT * FROM ACCOUNTS WHERE name = $1
* 1
{'Node Type': 'Index Scan', 'Parallel Aware': False, 'Scan Direction': 'Forward', 'Index Name': 'idx_accounts_name', 'Relation Name': 'accounts', 'Alias': 'accounts', 'Startup Cost': 0.56, 'Total Cost': 8.58, 'Plan Rows': 1, 'Plan Width': 73, 'Index Cond': "((name)::text = '0000000000000000000000000000000000000000000000000000000009338014'::text)"}
[{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.56, 'Total Cost': 8.58, 'Index Cond': "((name)::text = '0000000000000000000000000000000000000000000000000000000009338014'::text)"}]
! 3 0.3938876986503601
{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.56, 'Total Cost': 8.58, 'Index Cond': "((name)::text = '0000000000000000000000000000000000000000000000000000000009338014'::text)"}
! 0 0.0
empty
! 1 0.0
empty
! 2 0.0
empty
* 2
{'Node Type': 'Index Scan', 'Parallel Aware': False, 'Scan Direction': 'Forward', 'Index Name': 'idx_accounts_name', 'Relation Name': 'accounts', 'Alias': 'accounts', 'Startup Cost': 0.56, 'Total Cost': 8.58, 'Plan Rows': 1, 'Plan Width': 73, 'Index Cond': "((name)::text = '0000000000000000000000000000000000000000000000000000000006872584'::text)"}
[{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.56, 'Total Cost': 8.58, 'Index Cond': "((name)::text = '0000000000000000000000000000000000000000000000000000000006872584'::text)"}]
! 3 0.3938876986503601
{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.56, 'Total Cost': 8.58, 'Index Cond': "((name)::text = '0000000000000000000000000000000000000000000000000000000006872584'::text)"}
! 0 0.0
empty
! 1 0.0
empty
! 2 0.0
empty
* 3
[{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.56, 'Total Cost': 8.58, 'Index Cond': "((name)::text = '0000000000000000000000000000000000000000000000000000000006872584'::text)"}]
! 3 0.3938876986503601
{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.56, 'Total Cost': 8.58, 'Index Cond': "((name)::text = '0000000000000000000000000000000000000000000000000000000006872584'::text)"}
! 0 0.0
empty
! 1 0.0
empty
! 2 0.0
empty
* 4
[{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.56, 'Total Cost': 8.58, 'Index Cond': "((name)::text = '0000000000000000000000000000000000000000000000000000000006872584'::text)"}]
! 3 0.3938876986503601
{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.56, 'Total Cost': 8.58, 'Index Cond': "((name)::text = '0000000000000000000000000000000000000000000000000000000006872584'::text)"}
! 0 0.0
empty
! 1 0.0
empty
! 2 0.0
empty
* 5
[{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.56, 'Total Cost': 8.58, 'Index Cond': "((name)::text = '0000000000000000000000000000000000000000000000000000000006872584'::text)"}]
! 3 0.3938876986503601
{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.56, 'Total Cost': 8.58, 'Index Cond': "((name)::text = '0000000000000000000000000000000000000000000000000000000006872584'::text)"}
! 0 0.0
empty
! 1 0.0
empty
! 2 0.0
empty
======Top3 metric
total cpu usage_sys: 0.4053945085651413
dsk/total_read: 0.2905779334788845
total cpu usage_usr: 0.10268631132858057
igs:
[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 67.16859436035156, 67.16859436035156, 67.1636962890625, 67.1636962890625, 67.16799926757812, 67.16799926757812, 67.16799926757812, 67.16801452636719]
Transformer(
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (2): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (3): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (4): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (5): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
    )
    (norm): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
  )
  (decoder): TransformerDecoder(
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (1): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (2): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (3): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (4): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (5): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
    )
    (norm): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
  )
)
Transformer(
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (2): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (3): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (4): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (5): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
    )
    (norm): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
  )
  (decoder): TransformerDecoder(
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (1): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (2): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (3): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (4): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (5): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
    )
    (norm): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
  )
)
[[[0.0, 0.0, 0.0, 0.3938876986503601], [0.0, 0.0, 0.0, 0.3938876986503601], [0.0, 0.0, 0.0, 0.3938876986503601], [0.0, 0.0, 0.0, 0.3938876986503601], [0.0, 0.0, 0.0, 0.3938876986503601]], [[0.0, 0.0, 0.0, 0.3938876986503601], [0.0, 0.0, 0.0, 0.3938876986503601], [0.0, 0.0, 0.0, 0.3938876986503601], [0.0, 0.0, 0.0, 0.3938876986503601], [0.0, 0.0, 0.0, 0.3938876986503601]]]
# 1
SELECT * FROM ACCOUNTS WHERE custid = $1
* 1
{'Node Type': 'Index Scan', 'Parallel Aware': False, 'Scan Direction': 'Forward', 'Index Name': 'pk_accounts', 'Relation Name': 'accounts', 'Alias': 'accounts', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 73, 'Index Cond': "(custid = '4091455'::bigint)"}
[{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': "(custid = '4091455'::bigint)"}]
! 3 0.3938876986503601
{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': "(custid = '4091455'::bigint)"}
! 0 0.0
empty
! 1 0.0
empty
! 2 0.0
empty
* 2
{'Node Type': 'Index Scan', 'Parallel Aware': False, 'Scan Direction': 'Forward', 'Index Name': 'pk_accounts', 'Relation Name': 'accounts', 'Alias': 'accounts', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 73, 'Index Cond': "(custid = '8258886'::bigint)"}
[{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': "(custid = '8258886'::bigint)"}]
! 3 0.3938876986503601
{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': "(custid = '8258886'::bigint)"}
! 0 0.0
empty
! 1 0.0
empty
! 2 0.0
empty
* 3
[{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': "(custid = '8258886'::bigint)"}]
! 3 0.3938876986503601
{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': "(custid = '8258886'::bigint)"}
! 0 0.0
empty
! 1 0.0
empty
! 2 0.0
empty
* 4
[{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': "(custid = '8258886'::bigint)"}]
! 3 0.3938876986503601
{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': "(custid = '8258886'::bigint)"}
! 0 0.0
empty
! 1 0.0
empty
! 2 0.0
empty
* 5
[{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': "(custid = '8258886'::bigint)"}]
! 3 0.3938876986503601
{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': "(custid = '8258886'::bigint)"}
! 0 0.0
empty
! 1 0.0
empty
! 2 0.0
empty
# 2
SELECT * FROM ACCOUNTS WHERE name = $1
* 1
{'Node Type': 'Index Scan', 'Parallel Aware': False, 'Scan Direction': 'Forward', 'Index Name': 'idx_accounts_name', 'Relation Name': 'accounts', 'Alias': 'accounts', 'Startup Cost': 0.56, 'Total Cost': 8.58, 'Plan Rows': 1, 'Plan Width': 73, 'Index Cond': "((name)::text = '0000000000000000000000000000000000000000000000000000000009338014'::text)"}
[{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.56, 'Total Cost': 8.58, 'Index Cond': "((name)::text = '0000000000000000000000000000000000000000000000000000000009338014'::text)"}]
! 3 0.3938876986503601
{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.56, 'Total Cost': 8.58, 'Index Cond': "((name)::text = '0000000000000000000000000000000000000000000000000000000009338014'::text)"}
! 0 0.0
empty
! 1 0.0
empty
! 2 0.0
empty
* 2
{'Node Type': 'Index Scan', 'Parallel Aware': False, 'Scan Direction': 'Forward', 'Index Name': 'idx_accounts_name', 'Relation Name': 'accounts', 'Alias': 'accounts', 'Startup Cost': 0.56, 'Total Cost': 8.58, 'Plan Rows': 1, 'Plan Width': 73, 'Index Cond': "((name)::text = '0000000000000000000000000000000000000000000000000000000006872584'::text)"}
[{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.56, 'Total Cost': 8.58, 'Index Cond': "((name)::text = '0000000000000000000000000000000000000000000000000000000006872584'::text)"}]
! 3 0.3938876986503601
{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.56, 'Total Cost': 8.58, 'Index Cond': "((name)::text = '0000000000000000000000000000000000000000000000000000000006872584'::text)"}
! 0 0.0
empty
! 1 0.0
empty
! 2 0.0
empty
* 3
[{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.56, 'Total Cost': 8.58, 'Index Cond': "((name)::text = '0000000000000000000000000000000000000000000000000000000006872584'::text)"}]
! 3 0.3938876986503601
{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.56, 'Total Cost': 8.58, 'Index Cond': "((name)::text = '0000000000000000000000000000000000000000000000000000000006872584'::text)"}
! 0 0.0
empty
! 1 0.0
empty
! 2 0.0
empty
* 4
[{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.56, 'Total Cost': 8.58, 'Index Cond': "((name)::text = '0000000000000000000000000000000000000000000000000000000006872584'::text)"}]
! 3 0.3938876986503601
{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.56, 'Total Cost': 8.58, 'Index Cond': "((name)::text = '0000000000000000000000000000000000000000000000000000000006872584'::text)"}
! 0 0.0
empty
! 1 0.0
empty
! 2 0.0
empty
* 5
[{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.56, 'Total Cost': 8.58, 'Index Cond': "((name)::text = '0000000000000000000000000000000000000000000000000000000006872584'::text)"}]
! 3 0.3938876986503601
{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.56, 'Total Cost': 8.58, 'Index Cond': "((name)::text = '0000000000000000000000000000000000000000000000000000000006872584'::text)"}
! 0 0.0
empty
! 1 0.0
empty
! 2 0.0
empty
======Top3 metric
total cpu usage_sys: 0.4118235782018383
dsk/total_read: 0.26821944898005395
total cpu usage_usr: 0.10792693437593402
igs:
[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 72.32218933105469, 72.32218933105469, 72.3169174194336, 72.3169174194336, 72.3215560913086, 72.3215560913086, 72.3215560913086, 72.32156372070312]
Transformer(
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (2): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (3): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (4): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (5): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
    )
    (norm): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
  )
  (decoder): TransformerDecoder(
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (1): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (2): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (3): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (4): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (5): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
    )
    (norm): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
  )
)
Transformer(
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (2): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (3): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (4): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (5): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
    )
    (norm): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
  )
  (decoder): TransformerDecoder(
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (1): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (2): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (3): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (4): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (5): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
    )
    (norm): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
  )
)
[[[0.0, 0.0, 0.0, 0.3938876986503601], [0.0, 0.0, 0.0, 0.3938876986503601], [0.0, 0.0, 0.0, 0.3938876986503601], [0.0, 0.0, 0.0, 0.3938876986503601], [0.0, 0.0, 0.0, 0.3938876986503601]], [[0.0, 0.0, 0.0, 0.3938876986503601], [0.0, 0.0, 0.0, 0.3938876986503601], [0.0, 0.0, 0.0, 0.3938876986503601], [0.0, 0.0, 0.0, 0.3938876986503601], [0.0, 0.0, 0.0, 0.3938876986503601]]]
# 1
SELECT * FROM ACCOUNTS WHERE custid = $1
* 1
{'Node Type': 'Index Scan', 'Parallel Aware': False, 'Scan Direction': 'Forward', 'Index Name': 'pk_accounts', 'Relation Name': 'accounts', 'Alias': 'accounts', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 73, 'Index Cond': "(custid = '4091455'::bigint)"}
[{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': "(custid = '4091455'::bigint)"}]
! 3 0.3938876986503601
{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': "(custid = '4091455'::bigint)"}
! 0 0.0
empty
! 1 0.0
empty
! 2 0.0
empty
* 2
{'Node Type': 'Index Scan', 'Parallel Aware': False, 'Scan Direction': 'Forward', 'Index Name': 'pk_accounts', 'Relation Name': 'accounts', 'Alias': 'accounts', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 73, 'Index Cond': "(custid = '8258886'::bigint)"}
[{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': "(custid = '8258886'::bigint)"}]
! 3 0.3938876986503601
{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': "(custid = '8258886'::bigint)"}
! 0 0.0
empty
! 1 0.0
empty
! 2 0.0
empty
* 3
[{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': "(custid = '8258886'::bigint)"}]
! 3 0.3938876986503601
{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': "(custid = '8258886'::bigint)"}
! 0 0.0
empty
! 1 0.0
empty
! 2 0.0
empty
* 4
[{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': "(custid = '8258886'::bigint)"}]
! 3 0.3938876986503601
{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': "(custid = '8258886'::bigint)"}
! 0 0.0
empty
! 1 0.0
empty
! 2 0.0
empty
* 5
[{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': "(custid = '8258886'::bigint)"}]
! 3 0.3938876986503601
{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': "(custid = '8258886'::bigint)"}
! 0 0.0
empty
! 1 0.0
empty
! 2 0.0
empty
# 2
SELECT * FROM ACCOUNTS WHERE name = $1
* 1
{'Node Type': 'Index Scan', 'Parallel Aware': False, 'Scan Direction': 'Forward', 'Index Name': 'idx_accounts_name', 'Relation Name': 'accounts', 'Alias': 'accounts', 'Startup Cost': 0.56, 'Total Cost': 8.58, 'Plan Rows': 1, 'Plan Width': 73, 'Index Cond': "((name)::text = '0000000000000000000000000000000000000000000000000000000009338014'::text)"}
[{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.56, 'Total Cost': 8.58, 'Index Cond': "((name)::text = '0000000000000000000000000000000000000000000000000000000009338014'::text)"}]
! 3 0.3938876986503601
{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.56, 'Total Cost': 8.58, 'Index Cond': "((name)::text = '0000000000000000000000000000000000000000000000000000000009338014'::text)"}
! 0 0.0
empty
! 1 0.0
empty
! 2 0.0
empty
* 2
{'Node Type': 'Index Scan', 'Parallel Aware': False, 'Scan Direction': 'Forward', 'Index Name': 'idx_accounts_name', 'Relation Name': 'accounts', 'Alias': 'accounts', 'Startup Cost': 0.56, 'Total Cost': 8.58, 'Plan Rows': 1, 'Plan Width': 73, 'Index Cond': "((name)::text = '0000000000000000000000000000000000000000000000000000000006872584'::text)"}
[{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.56, 'Total Cost': 8.58, 'Index Cond': "((name)::text = '0000000000000000000000000000000000000000000000000000000006872584'::text)"}]
! 3 0.3938876986503601
{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.56, 'Total Cost': 8.58, 'Index Cond': "((name)::text = '0000000000000000000000000000000000000000000000000000000006872584'::text)"}
! 0 0.0
empty
! 1 0.0
empty
! 2 0.0
empty
* 3
[{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.56, 'Total Cost': 8.58, 'Index Cond': "((name)::text = '0000000000000000000000000000000000000000000000000000000006872584'::text)"}]
! 3 0.3938876986503601
{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.56, 'Total Cost': 8.58, 'Index Cond': "((name)::text = '0000000000000000000000000000000000000000000000000000000006872584'::text)"}
! 0 0.0
empty
! 1 0.0
empty
! 2 0.0
empty
* 4
[{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.56, 'Total Cost': 8.58, 'Index Cond': "((name)::text = '0000000000000000000000000000000000000000000000000000000006872584'::text)"}]
! 3 0.3938876986503601
{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.56, 'Total Cost': 8.58, 'Index Cond': "((name)::text = '0000000000000000000000000000000000000000000000000000000006872584'::text)"}
! 0 0.0
empty
! 1 0.0
empty
! 2 0.0
empty
* 5
[{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.56, 'Total Cost': 8.58, 'Index Cond': "((name)::text = '0000000000000000000000000000000000000000000000000000000006872584'::text)"}]
! 3 0.3938876986503601
{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.56, 'Total Cost': 8.58, 'Index Cond': "((name)::text = '0000000000000000000000000000000000000000000000000000000006872584'::text)"}
! 0 0.0
empty
! 1 0.0
empty
! 2 0.0
empty
======Top3 metric
total cpu usage_sys: 0.3464162661291728
dsk/total_read: 0.3335350923127715
total cpu usage_usr: 0.1361548441923673
igs:
[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 90.70166015625, 90.70166015625, 90.69503784179688, 90.69503784179688, 90.70085906982422, 90.70085906982422, 90.70085906982422, 90.70087432861328]
Transformer(
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (2): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (3): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (4): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (5): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
    )
    (norm): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
  )
  (decoder): TransformerDecoder(
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (1): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (2): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (3): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (4): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (5): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
    )
    (norm): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
  )
)
Transformer(
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (2): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (3): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (4): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (5): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
    )
    (norm): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
  )
  (decoder): TransformerDecoder(
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (1): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (2): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (3): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (4): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (5): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
    )
    (norm): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
  )
)
[[[0.0, 0.0, 0.0, 0.3938876986503601], [0.0, 0.0, 0.0, 0.3938876986503601], [0.0, 0.0, 0.0, 0.3938876986503601], [0.0, 0.0, 0.0, 0.3938876986503601], [0.0, 0.0, 0.0, 0.3938876986503601]], [[0.0, 0.0, 0.0, 0.3938876986503601], [0.0, 0.0, 0.0, 0.3938876986503601], [0.0, 0.0, 0.0, 0.3938876986503601], [0.0, 0.0, 0.0, 0.3938876986503601], [0.0, 0.0, 0.0, 0.3938876986503601]]]
# 1
SELECT * FROM ACCOUNTS WHERE custid = $1
* 1
{'Node Type': 'Index Scan', 'Parallel Aware': False, 'Scan Direction': 'Forward', 'Index Name': 'pk_accounts', 'Relation Name': 'accounts', 'Alias': 'accounts', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 73, 'Index Cond': "(custid = '4091455'::bigint)"}
[{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': "(custid = '4091455'::bigint)"}]
! 3 0.3938876986503601
{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': "(custid = '4091455'::bigint)"}
! 0 0.0
empty
! 1 0.0
empty
! 2 0.0
empty
* 2
{'Node Type': 'Index Scan', 'Parallel Aware': False, 'Scan Direction': 'Forward', 'Index Name': 'pk_accounts', 'Relation Name': 'accounts', 'Alias': 'accounts', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 73, 'Index Cond': "(custid = '8258886'::bigint)"}
[{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': "(custid = '8258886'::bigint)"}]
! 3 0.3938876986503601
{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': "(custid = '8258886'::bigint)"}
! 0 0.0
empty
! 1 0.0
empty
! 2 0.0
empty
* 3
[{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': "(custid = '8258886'::bigint)"}]
! 3 0.3938876986503601
{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': "(custid = '8258886'::bigint)"}
! 0 0.0
empty
! 1 0.0
empty
! 2 0.0
empty
* 4
[{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': "(custid = '8258886'::bigint)"}]
! 3 0.3938876986503601
{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': "(custid = '8258886'::bigint)"}
! 0 0.0
empty
! 1 0.0
empty
! 2 0.0
empty
* 5
[{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': "(custid = '8258886'::bigint)"}]
! 3 0.3938876986503601
{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': "(custid = '8258886'::bigint)"}
! 0 0.0
empty
! 1 0.0
empty
! 2 0.0
empty
# 2
SELECT * FROM ACCOUNTS WHERE name = $1
* 1
{'Node Type': 'Index Scan', 'Parallel Aware': False, 'Scan Direction': 'Forward', 'Index Name': 'idx_accounts_name', 'Relation Name': 'accounts', 'Alias': 'accounts', 'Startup Cost': 0.56, 'Total Cost': 8.58, 'Plan Rows': 1, 'Plan Width': 73, 'Index Cond': "((name)::text = '0000000000000000000000000000000000000000000000000000000009338014'::text)"}
[{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.56, 'Total Cost': 8.58, 'Index Cond': "((name)::text = '0000000000000000000000000000000000000000000000000000000009338014'::text)"}]
! 3 0.3938876986503601
{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.56, 'Total Cost': 8.58, 'Index Cond': "((name)::text = '0000000000000000000000000000000000000000000000000000000009338014'::text)"}
! 0 0.0
empty
! 1 0.0
empty
! 2 0.0
empty
* 2
{'Node Type': 'Index Scan', 'Parallel Aware': False, 'Scan Direction': 'Forward', 'Index Name': 'idx_accounts_name', 'Relation Name': 'accounts', 'Alias': 'accounts', 'Startup Cost': 0.56, 'Total Cost': 8.58, 'Plan Rows': 1, 'Plan Width': 73, 'Index Cond': "((name)::text = '0000000000000000000000000000000000000000000000000000000006872584'::text)"}
[{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.56, 'Total Cost': 8.58, 'Index Cond': "((name)::text = '0000000000000000000000000000000000000000000000000000000006872584'::text)"}]
! 3 0.3938876986503601
{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.56, 'Total Cost': 8.58, 'Index Cond': "((name)::text = '0000000000000000000000000000000000000000000000000000000006872584'::text)"}
! 0 0.0
empty
! 1 0.0
empty
! 2 0.0
empty
* 3
[{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.56, 'Total Cost': 8.58, 'Index Cond': "((name)::text = '0000000000000000000000000000000000000000000000000000000006872584'::text)"}]
! 3 0.3938876986503601
{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.56, 'Total Cost': 8.58, 'Index Cond': "((name)::text = '0000000000000000000000000000000000000000000000000000000006872584'::text)"}
! 0 0.0
empty
! 1 0.0
empty
! 2 0.0
empty
* 4
[{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.56, 'Total Cost': 8.58, 'Index Cond': "((name)::text = '0000000000000000000000000000000000000000000000000000000006872584'::text)"}]
! 3 0.3938876986503601
{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.56, 'Total Cost': 8.58, 'Index Cond': "((name)::text = '0000000000000000000000000000000000000000000000000000000006872584'::text)"}
! 0 0.0
empty
! 1 0.0
empty
! 2 0.0
empty
* 5
[{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.56, 'Total Cost': 8.58, 'Index Cond': "((name)::text = '0000000000000000000000000000000000000000000000000000000006872584'::text)"}]
! 3 0.3938876986503601
{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.56, 'Total Cost': 8.58, 'Index Cond': "((name)::text = '0000000000000000000000000000000000000000000000000000000006872584'::text)"}
! 0 0.0
empty
! 1 0.0
empty
! 2 0.0
empty
======Top3 metric
total cpu usage_sys: 0.4203950373046498
dsk/total_read: 0.27972839145211814
total cpu usage_usr: 0.12254641779260068
igs:
[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 72.27513885498047, 72.27513885498047, 72.26986694335938, 72.26986694335938, 72.27449798583984, 72.27449798583984, 72.27449798583984, 72.2745132446289]
Transformer(
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (2): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (3): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (4): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (5): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
    )
    (norm): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
  )
  (decoder): TransformerDecoder(
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (1): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (2): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (3): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (4): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (5): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
    )
    (norm): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
  )
)
Transformer(
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (2): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (3): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (4): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (5): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
    )
    (norm): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
  )
  (decoder): TransformerDecoder(
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (1): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (2): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (3): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (4): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (5): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
    )
    (norm): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
  )
)
[[[0.0, 0.0, 0.0, 0.3938876986503601], [0.0, 0.0, 0.0, 0.3938876986503601], [0.0, 0.0, 0.0, 0.3938876986503601], [0.0, 0.0, 0.0, 0.3938876986503601], [0.0, 0.0, 0.0, 0.3938876986503601]], [[0.0, 0.0, 0.0, 0.3938876986503601], [0.0, 0.0, 0.0, 0.3938876986503601], [0.0, 0.0, 0.0, 0.3938876986503601], [0.0, 0.0, 0.0, 0.3938876986503601], [0.0, 0.0, 0.0, 0.3938876986503601]]]
# 1
SELECT * FROM ACCOUNTS WHERE custid = $1
* 1
{'Node Type': 'Index Scan', 'Parallel Aware': False, 'Scan Direction': 'Forward', 'Index Name': 'pk_accounts', 'Relation Name': 'accounts', 'Alias': 'accounts', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 73, 'Index Cond': "(custid = '4091455'::bigint)"}
[{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': "(custid = '4091455'::bigint)"}]
! 3 0.3938876986503601
{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': "(custid = '4091455'::bigint)"}
! 0 0.0
empty
! 1 0.0
empty
! 2 0.0
empty
* 2
{'Node Type': 'Index Scan', 'Parallel Aware': False, 'Scan Direction': 'Forward', 'Index Name': 'pk_accounts', 'Relation Name': 'accounts', 'Alias': 'accounts', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 73, 'Index Cond': "(custid = '8258886'::bigint)"}
[{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': "(custid = '8258886'::bigint)"}]
! 3 0.3938876986503601
{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': "(custid = '8258886'::bigint)"}
! 0 0.0
empty
! 1 0.0
empty
! 2 0.0
empty
* 3
[{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': "(custid = '8258886'::bigint)"}]
! 3 0.3938876986503601
{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': "(custid = '8258886'::bigint)"}
! 0 0.0
empty
! 1 0.0
empty
! 2 0.0
empty
* 4
[{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': "(custid = '8258886'::bigint)"}]
! 3 0.3938876986503601
{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': "(custid = '8258886'::bigint)"}
! 0 0.0
empty
! 1 0.0
empty
! 2 0.0
empty
* 5
[{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': "(custid = '8258886'::bigint)"}]
! 3 0.3938876986503601
{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': "(custid = '8258886'::bigint)"}
! 0 0.0
empty
! 1 0.0
empty
! 2 0.0
empty
# 2
SELECT * FROM ACCOUNTS WHERE name = $1
* 1
{'Node Type': 'Index Scan', 'Parallel Aware': False, 'Scan Direction': 'Forward', 'Index Name': 'idx_accounts_name', 'Relation Name': 'accounts', 'Alias': 'accounts', 'Startup Cost': 0.56, 'Total Cost': 8.58, 'Plan Rows': 1, 'Plan Width': 73, 'Index Cond': "((name)::text = '0000000000000000000000000000000000000000000000000000000009338014'::text)"}
[{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.56, 'Total Cost': 8.58, 'Index Cond': "((name)::text = '0000000000000000000000000000000000000000000000000000000009338014'::text)"}]
! 3 0.3938876986503601
{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.56, 'Total Cost': 8.58, 'Index Cond': "((name)::text = '0000000000000000000000000000000000000000000000000000000009338014'::text)"}
! 0 0.0
empty
! 1 0.0
empty
! 2 0.0
empty
* 2
{'Node Type': 'Index Scan', 'Parallel Aware': False, 'Scan Direction': 'Forward', 'Index Name': 'idx_accounts_name', 'Relation Name': 'accounts', 'Alias': 'accounts', 'Startup Cost': 0.56, 'Total Cost': 8.58, 'Plan Rows': 1, 'Plan Width': 73, 'Index Cond': "((name)::text = '0000000000000000000000000000000000000000000000000000000006872584'::text)"}
[{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.56, 'Total Cost': 8.58, 'Index Cond': "((name)::text = '0000000000000000000000000000000000000000000000000000000006872584'::text)"}]
! 3 0.3938876986503601
{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.56, 'Total Cost': 8.58, 'Index Cond': "((name)::text = '0000000000000000000000000000000000000000000000000000000006872584'::text)"}
! 0 0.0
empty
! 1 0.0
empty
! 2 0.0
empty
* 3
[{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.56, 'Total Cost': 8.58, 'Index Cond': "((name)::text = '0000000000000000000000000000000000000000000000000000000006872584'::text)"}]
! 3 0.3938876986503601
{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.56, 'Total Cost': 8.58, 'Index Cond': "((name)::text = '0000000000000000000000000000000000000000000000000000000006872584'::text)"}
! 0 0.0
empty
! 1 0.0
empty
! 2 0.0
empty
* 4
[{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.56, 'Total Cost': 8.58, 'Index Cond': "((name)::text = '0000000000000000000000000000000000000000000000000000000006872584'::text)"}]
! 3 0.3938876986503601
{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.56, 'Total Cost': 8.58, 'Index Cond': "((name)::text = '0000000000000000000000000000000000000000000000000000000006872584'::text)"}
! 0 0.0
empty
! 1 0.0
empty
! 2 0.0
empty
* 5
[{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.56, 'Total Cost': 8.58, 'Index Cond': "((name)::text = '0000000000000000000000000000000000000000000000000000000006872584'::text)"}]
! 3 0.3938876986503601
{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.56, 'Total Cost': 8.58, 'Index Cond': "((name)::text = '0000000000000000000000000000000000000000000000000000000006872584'::text)"}
! 0 0.0
empty
! 1 0.0
empty
! 2 0.0
empty
======Top3 metric
total cpu usage_sys: 0.37196499948783596
dsk/total_read: 0.2940823965535793
total cpu usage_usr: 0.16851276480878033
igs:
[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 67.6048355102539, 67.6048355102539, 67.59990692138672, 67.59990692138672, 67.60424041748047, 67.60424041748047, 67.60424041748047, 67.604248046875]
Transformer(
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (2): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (3): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (4): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (5): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
    )
    (norm): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
  )
  (decoder): TransformerDecoder(
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (1): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (2): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (3): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (4): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (5): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
    )
    (norm): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
  )
)
Transformer(
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (2): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (3): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (4): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (5): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
    )
    (norm): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
  )
  (decoder): TransformerDecoder(
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (1): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (2): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (3): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (4): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (5): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
    )
    (norm): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
  )
)
[[[0.0, 0.0, 0.0, 0.3938876986503601], [0.0, 0.0, 0.0, 0.3938876986503601], [0.0, 0.0, 0.0, 0.3938876986503601], [0.0, 0.0, 0.0, 0.3938876986503601], [0.0, 0.0, 0.0, 0.3938876986503601]], [[0.0, 0.0, 0.0, 0.3938876986503601], [0.0, 0.0, 0.0, 0.3938876986503601], [0.0, 0.0, 0.0, 0.3938876986503601], [0.0, 0.0, 0.0, 0.3938876986503601], [0.0, 0.0, 0.0, 0.3938876986503601]]]
# 1
SELECT * FROM ACCOUNTS WHERE custid = $1
* 1
{'Node Type': 'Index Scan', 'Parallel Aware': False, 'Scan Direction': 'Forward', 'Index Name': 'pk_accounts', 'Relation Name': 'accounts', 'Alias': 'accounts', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 73, 'Index Cond': "(custid = '4091455'::bigint)"}
[{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': "(custid = '4091455'::bigint)"}]
! 3 0.3938876986503601
{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': "(custid = '4091455'::bigint)"}
! 0 0.0
empty
! 1 0.0
empty
! 2 0.0
empty
* 2
{'Node Type': 'Index Scan', 'Parallel Aware': False, 'Scan Direction': 'Forward', 'Index Name': 'pk_accounts', 'Relation Name': 'accounts', 'Alias': 'accounts', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 73, 'Index Cond': "(custid = '8258886'::bigint)"}
[{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': "(custid = '8258886'::bigint)"}]
! 3 0.3938876986503601
{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': "(custid = '8258886'::bigint)"}
! 0 0.0
empty
! 1 0.0
empty
! 2 0.0
empty
* 3
[{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': "(custid = '8258886'::bigint)"}]
! 3 0.3938876986503601
{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': "(custid = '8258886'::bigint)"}
! 0 0.0
empty
! 1 0.0
empty
! 2 0.0
empty
* 4
[{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': "(custid = '8258886'::bigint)"}]
! 3 0.3938876986503601
{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': "(custid = '8258886'::bigint)"}
! 0 0.0
empty
! 1 0.0
empty
! 2 0.0
empty
* 5
[{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': "(custid = '8258886'::bigint)"}]
! 3 0.3938876986503601
{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': "(custid = '8258886'::bigint)"}
! 0 0.0
empty
! 1 0.0
empty
! 2 0.0
empty
# 2
SELECT * FROM ACCOUNTS WHERE name = $1
* 1
{'Node Type': 'Index Scan', 'Parallel Aware': False, 'Scan Direction': 'Forward', 'Index Name': 'idx_accounts_name', 'Relation Name': 'accounts', 'Alias': 'accounts', 'Startup Cost': 0.56, 'Total Cost': 8.58, 'Plan Rows': 1, 'Plan Width': 73, 'Index Cond': "((name)::text = '0000000000000000000000000000000000000000000000000000000009338014'::text)"}
[{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.56, 'Total Cost': 8.58, 'Index Cond': "((name)::text = '0000000000000000000000000000000000000000000000000000000009338014'::text)"}]
! 3 0.3938876986503601
{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.56, 'Total Cost': 8.58, 'Index Cond': "((name)::text = '0000000000000000000000000000000000000000000000000000000009338014'::text)"}
! 0 0.0
empty
! 1 0.0
empty
! 2 0.0
empty
* 2
{'Node Type': 'Index Scan', 'Parallel Aware': False, 'Scan Direction': 'Forward', 'Index Name': 'idx_accounts_name', 'Relation Name': 'accounts', 'Alias': 'accounts', 'Startup Cost': 0.56, 'Total Cost': 8.58, 'Plan Rows': 1, 'Plan Width': 73, 'Index Cond': "((name)::text = '0000000000000000000000000000000000000000000000000000000006872584'::text)"}
[{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.56, 'Total Cost': 8.58, 'Index Cond': "((name)::text = '0000000000000000000000000000000000000000000000000000000006872584'::text)"}]
! 3 0.3938876986503601
{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.56, 'Total Cost': 8.58, 'Index Cond': "((name)::text = '0000000000000000000000000000000000000000000000000000000006872584'::text)"}
! 0 0.0
empty
! 1 0.0
empty
! 2 0.0
empty
* 3
[{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.56, 'Total Cost': 8.58, 'Index Cond': "((name)::text = '0000000000000000000000000000000000000000000000000000000006872584'::text)"}]
! 3 0.3938876986503601
{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.56, 'Total Cost': 8.58, 'Index Cond': "((name)::text = '0000000000000000000000000000000000000000000000000000000006872584'::text)"}
! 0 0.0
empty
! 1 0.0
empty
! 2 0.0
empty
* 4
[{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.56, 'Total Cost': 8.58, 'Index Cond': "((name)::text = '0000000000000000000000000000000000000000000000000000000006872584'::text)"}]
! 3 0.3938876986503601
{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.56, 'Total Cost': 8.58, 'Index Cond': "((name)::text = '0000000000000000000000000000000000000000000000000000000006872584'::text)"}
! 0 0.0
empty
! 1 0.0
empty
! 2 0.0
empty
* 5
[{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.56, 'Total Cost': 8.58, 'Index Cond': "((name)::text = '0000000000000000000000000000000000000000000000000000000006872584'::text)"}]
! 3 0.3938876986503601
{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.56, 'Total Cost': 8.58, 'Index Cond': "((name)::text = '0000000000000000000000000000000000000000000000000000000006872584'::text)"}
! 0 0.0
empty
! 1 0.0
empty
! 2 0.0
empty
======Top3 metric
total cpu usage_sys: 0.4248620651198079
dsk/total_read: 0.2802202955104295
total cpu usage_usr: 0.10559683126706951
igs:
[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 68.7076416015625, 68.7076416015625, 68.70262908935547, 68.70262908935547, 68.70703125, 68.70703125, 68.70703125, 68.70704650878906]
Transformer(
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (2): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (3): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (4): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (5): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
    )
    (norm): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
  )
  (decoder): TransformerDecoder(
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (1): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (2): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (3): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (4): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (5): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
    )
    (norm): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
  )
)
Transformer(
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (2): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (3): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (4): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (5): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
    )
    (norm): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
  )
  (decoder): TransformerDecoder(
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (1): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (2): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (3): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (4): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (5): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
    )
    (norm): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
  )
)
[[[0.0, 0.0, 0.0, 0.3938876986503601], [0.0, 0.0, 0.0, 0.3938876986503601], [0.0, 0.0, 0.0, 0.3938876986503601], [0.0, 0.0, 0.0, 0.3938876986503601], [0.0, 0.0, 0.0, 0.3938876986503601]], [[0.0, 0.0, 0.0, 0.3938876986503601], [0.0, 0.0, 0.0, 0.3938876986503601], [0.0, 0.0, 0.0, 0.3938876986503601], [0.0, 0.0, 0.0, 0.3938876986503601], [0.0, 0.0, 0.0, 0.3938876986503601]]]
# 1
SELECT * FROM ACCOUNTS WHERE custid = $1
* 1
{'Node Type': 'Index Scan', 'Parallel Aware': False, 'Scan Direction': 'Forward', 'Index Name': 'pk_accounts', 'Relation Name': 'accounts', 'Alias': 'accounts', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 73, 'Index Cond': "(custid = '4091455'::bigint)"}
[{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': "(custid = '4091455'::bigint)"}]
! 3 0.3938876986503601
{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': "(custid = '4091455'::bigint)"}
! 0 0.0
empty
! 1 0.0
empty
! 2 0.0
empty
* 2
{'Node Type': 'Index Scan', 'Parallel Aware': False, 'Scan Direction': 'Forward', 'Index Name': 'pk_accounts', 'Relation Name': 'accounts', 'Alias': 'accounts', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 73, 'Index Cond': "(custid = '8258886'::bigint)"}
[{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': "(custid = '8258886'::bigint)"}]
! 3 0.3938876986503601
{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': "(custid = '8258886'::bigint)"}
! 0 0.0
empty
! 1 0.0
empty
! 2 0.0
empty
* 3
[{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': "(custid = '8258886'::bigint)"}]
! 3 0.3938876986503601
{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': "(custid = '8258886'::bigint)"}
! 0 0.0
empty
! 1 0.0
empty
! 2 0.0
empty
* 4
[{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': "(custid = '8258886'::bigint)"}]
! 3 0.3938876986503601
{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': "(custid = '8258886'::bigint)"}
! 0 0.0
empty
! 1 0.0
empty
! 2 0.0
empty
* 5
[{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': "(custid = '8258886'::bigint)"}]
! 3 0.3938876986503601
{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': "(custid = '8258886'::bigint)"}
! 0 0.0
empty
! 1 0.0
empty
! 2 0.0
empty
# 2
SELECT * FROM ACCOUNTS WHERE name = $1
* 1
{'Node Type': 'Index Scan', 'Parallel Aware': False, 'Scan Direction': 'Forward', 'Index Name': 'idx_accounts_name', 'Relation Name': 'accounts', 'Alias': 'accounts', 'Startup Cost': 0.56, 'Total Cost': 8.58, 'Plan Rows': 1, 'Plan Width': 73, 'Index Cond': "((name)::text = '0000000000000000000000000000000000000000000000000000000009338014'::text)"}
[{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.56, 'Total Cost': 8.58, 'Index Cond': "((name)::text = '0000000000000000000000000000000000000000000000000000000009338014'::text)"}]
! 3 0.3938876986503601
{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.56, 'Total Cost': 8.58, 'Index Cond': "((name)::text = '0000000000000000000000000000000000000000000000000000000009338014'::text)"}
! 0 0.0
empty
! 1 0.0
empty
! 2 0.0
empty
* 2
{'Node Type': 'Index Scan', 'Parallel Aware': False, 'Scan Direction': 'Forward', 'Index Name': 'idx_accounts_name', 'Relation Name': 'accounts', 'Alias': 'accounts', 'Startup Cost': 0.56, 'Total Cost': 8.58, 'Plan Rows': 1, 'Plan Width': 73, 'Index Cond': "((name)::text = '0000000000000000000000000000000000000000000000000000000006872584'::text)"}
[{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.56, 'Total Cost': 8.58, 'Index Cond': "((name)::text = '0000000000000000000000000000000000000000000000000000000006872584'::text)"}]
! 3 0.3938876986503601
{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.56, 'Total Cost': 8.58, 'Index Cond': "((name)::text = '0000000000000000000000000000000000000000000000000000000006872584'::text)"}
! 0 0.0
empty
! 1 0.0
empty
! 2 0.0
empty
* 3
[{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.56, 'Total Cost': 8.58, 'Index Cond': "((name)::text = '0000000000000000000000000000000000000000000000000000000006872584'::text)"}]
! 3 0.3938876986503601
{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.56, 'Total Cost': 8.58, 'Index Cond': "((name)::text = '0000000000000000000000000000000000000000000000000000000006872584'::text)"}
! 0 0.0
empty
! 1 0.0
empty
! 2 0.0
empty
* 4
[{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.56, 'Total Cost': 8.58, 'Index Cond': "((name)::text = '0000000000000000000000000000000000000000000000000000000006872584'::text)"}]
! 3 0.3938876986503601
{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.56, 'Total Cost': 8.58, 'Index Cond': "((name)::text = '0000000000000000000000000000000000000000000000000000000006872584'::text)"}
! 0 0.0
empty
! 1 0.0
empty
! 2 0.0
empty
* 5
[{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.56, 'Total Cost': 8.58, 'Index Cond': "((name)::text = '0000000000000000000000000000000000000000000000000000000006872584'::text)"}]
! 3 0.3938876986503601
{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.56, 'Total Cost': 8.58, 'Index Cond': "((name)::text = '0000000000000000000000000000000000000000000000000000000006872584'::text)"}
! 0 0.0
empty
! 1 0.0
empty
! 2 0.0
empty
======Top3 metric
dsk/total_read: 0.3337868540846877
total cpu usage_usr: 0.25475999291357176
total cpu usage_sys: 0.2003445587282166
igs:
[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 119.18708801269531, 119.18708801269531, 119.17839813232422, 119.17839813232422, 119.18604278564453, 119.18604278564453, 119.18604278564453, 119.1860580444336]
Transformer(
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (2): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (3): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (4): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (5): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
    )
    (norm): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
  )
  (decoder): TransformerDecoder(
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (1): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (2): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (3): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (4): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (5): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
    )
    (norm): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
  )
)
Transformer(
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (2): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (3): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (4): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (5): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
    )
    (norm): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
  )
  (decoder): TransformerDecoder(
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (1): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (2): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (3): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (4): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (5): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
    )
    (norm): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
  )
)
[[[0.0, 0.0, 0.0, 0.3938876986503601], [0.0, 0.0, 0.0, 0.3938876986503601], [0.0, 0.0, 0.0, 0.3938876986503601], [0.0, 0.0, 0.0, 0.3938876986503601], [0.0, 0.0, 0.0, 0.3938876986503601]], [[0.0, 0.0, 0.0, 0.3938876986503601], [0.0, 0.0, 0.0, 0.3938876986503601], [0.0, 0.0, 0.0, 0.3938876986503601], [0.0, 0.0, 0.0, 0.3938876986503601], [0.0, 0.0, 0.0, 0.3938876986503601]]]
# 1
SELECT * FROM ACCOUNTS WHERE custid = $1
* 1
{'Node Type': 'Index Scan', 'Parallel Aware': False, 'Scan Direction': 'Forward', 'Index Name': 'pk_accounts', 'Relation Name': 'accounts', 'Alias': 'accounts', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 73, 'Index Cond': "(custid = '4091455'::bigint)"}
[{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': "(custid = '4091455'::bigint)"}]
! 3 0.3938876986503601
{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': "(custid = '4091455'::bigint)"}
! 0 0.0
empty
! 1 0.0
empty
! 2 0.0
empty
* 2
{'Node Type': 'Index Scan', 'Parallel Aware': False, 'Scan Direction': 'Forward', 'Index Name': 'pk_accounts', 'Relation Name': 'accounts', 'Alias': 'accounts', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 73, 'Index Cond': "(custid = '8258886'::bigint)"}
[{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': "(custid = '8258886'::bigint)"}]
! 3 0.3938876986503601
{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': "(custid = '8258886'::bigint)"}
! 0 0.0
empty
! 1 0.0
empty
! 2 0.0
empty
* 3
[{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': "(custid = '8258886'::bigint)"}]
! 3 0.3938876986503601
{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': "(custid = '8258886'::bigint)"}
! 0 0.0
empty
! 1 0.0
empty
! 2 0.0
empty
* 4
[{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': "(custid = '8258886'::bigint)"}]
! 3 0.3938876986503601
{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': "(custid = '8258886'::bigint)"}
! 0 0.0
empty
! 1 0.0
empty
! 2 0.0
empty
* 5
[{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': "(custid = '8258886'::bigint)"}]
! 3 0.3938876986503601
{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': "(custid = '8258886'::bigint)"}
! 0 0.0
empty
! 1 0.0
empty
! 2 0.0
empty
# 2
SELECT * FROM ACCOUNTS WHERE name = $1
* 1
{'Node Type': 'Index Scan', 'Parallel Aware': False, 'Scan Direction': 'Forward', 'Index Name': 'idx_accounts_name', 'Relation Name': 'accounts', 'Alias': 'accounts', 'Startup Cost': 0.56, 'Total Cost': 8.58, 'Plan Rows': 1, 'Plan Width': 73, 'Index Cond': "((name)::text = '0000000000000000000000000000000000000000000000000000000009338014'::text)"}
[{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.56, 'Total Cost': 8.58, 'Index Cond': "((name)::text = '0000000000000000000000000000000000000000000000000000000009338014'::text)"}]
! 3 0.3938876986503601
{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.56, 'Total Cost': 8.58, 'Index Cond': "((name)::text = '0000000000000000000000000000000000000000000000000000000009338014'::text)"}
! 0 0.0
empty
! 1 0.0
empty
! 2 0.0
empty
* 2
{'Node Type': 'Index Scan', 'Parallel Aware': False, 'Scan Direction': 'Forward', 'Index Name': 'idx_accounts_name', 'Relation Name': 'accounts', 'Alias': 'accounts', 'Startup Cost': 0.56, 'Total Cost': 8.58, 'Plan Rows': 1, 'Plan Width': 73, 'Index Cond': "((name)::text = '0000000000000000000000000000000000000000000000000000000006872584'::text)"}
[{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.56, 'Total Cost': 8.58, 'Index Cond': "((name)::text = '0000000000000000000000000000000000000000000000000000000006872584'::text)"}]
! 3 0.3938876986503601
{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.56, 'Total Cost': 8.58, 'Index Cond': "((name)::text = '0000000000000000000000000000000000000000000000000000000006872584'::text)"}
! 0 0.0
empty
! 1 0.0
empty
! 2 0.0
empty
* 3
[{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.56, 'Total Cost': 8.58, 'Index Cond': "((name)::text = '0000000000000000000000000000000000000000000000000000000006872584'::text)"}]
! 3 0.3938876986503601
{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.56, 'Total Cost': 8.58, 'Index Cond': "((name)::text = '0000000000000000000000000000000000000000000000000000000006872584'::text)"}
! 0 0.0
empty
! 1 0.0
empty
! 2 0.0
empty
* 4
[{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.56, 'Total Cost': 8.58, 'Index Cond': "((name)::text = '0000000000000000000000000000000000000000000000000000000006872584'::text)"}]
! 3 0.3938876986503601
{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.56, 'Total Cost': 8.58, 'Index Cond': "((name)::text = '0000000000000000000000000000000000000000000000000000000006872584'::text)"}
! 0 0.0
empty
! 1 0.0
empty
! 2 0.0
empty
* 5
[{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.56, 'Total Cost': 8.58, 'Index Cond': "((name)::text = '0000000000000000000000000000000000000000000000000000000006872584'::text)"}]
! 3 0.3938876986503601
{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.56, 'Total Cost': 8.58, 'Index Cond': "((name)::text = '0000000000000000000000000000000000000000000000000000000006872584'::text)"}
! 0 0.0
empty
! 1 0.0
empty
! 2 0.0
empty
======Top3 metric
total cpu usage_sys: 0.4017747123121237
dsk/total_read: 0.2661858047953197
total cpu usage_usr: 0.10815026343721003
igs:
[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 69.569580078125, 69.569580078125, 69.56450653076172, 69.56450653076172, 69.5689697265625, 69.5689697265625, 69.5689697265625, 69.56897735595703]
Transformer(
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (2): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (3): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (4): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (5): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
    )
    (norm): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
  )
  (decoder): TransformerDecoder(
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (1): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (2): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (3): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (4): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (5): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
    )
    (norm): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
  )
)
Transformer(
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (2): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (3): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (4): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (5): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
    )
    (norm): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
  )
  (decoder): TransformerDecoder(
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (1): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (2): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (3): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (4): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (5): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
    )
    (norm): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
  )
)
[[[0.0, 0.0, 0.0, 0.3938876986503601], [0.0, 0.0, 0.0, 0.3938876986503601], [0.0, 0.0, 0.0, 0.3938876986503601], [0.0, 0.0, 0.0, 0.3938876986503601], [0.0, 0.0, 0.0, 0.3938876986503601]], [[0.0, 0.0, 0.0, 0.3938876986503601], [0.0, 0.0, 0.0, 0.3938876986503601], [0.0, 0.0, 0.0, 0.3938876986503601], [0.0, 0.0, 0.0, 0.3938876986503601], [0.0, 0.0, 0.0, 0.3938876986503601]]]
# 1
SELECT * FROM ACCOUNTS WHERE custid = $1
* 1
{'Node Type': 'Index Scan', 'Parallel Aware': False, 'Scan Direction': 'Forward', 'Index Name': 'pk_accounts', 'Relation Name': 'accounts', 'Alias': 'accounts', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 73, 'Index Cond': "(custid = '4091455'::bigint)"}
[{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': "(custid = '4091455'::bigint)"}]
! 3 0.3938876986503601
{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': "(custid = '4091455'::bigint)"}
! 0 0.0
empty
! 1 0.0
empty
! 2 0.0
empty
* 2
{'Node Type': 'Index Scan', 'Parallel Aware': False, 'Scan Direction': 'Forward', 'Index Name': 'pk_accounts', 'Relation Name': 'accounts', 'Alias': 'accounts', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Plan Rows': 1, 'Plan Width': 73, 'Index Cond': "(custid = '8258886'::bigint)"}
[{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': "(custid = '8258886'::bigint)"}]
! 3 0.3938876986503601
{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': "(custid = '8258886'::bigint)"}
! 0 0.0
empty
! 1 0.0
empty
! 2 0.0
empty
* 3
[{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': "(custid = '8258886'::bigint)"}]
! 3 0.3938876986503601
{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': "(custid = '8258886'::bigint)"}
! 0 0.0
empty
! 1 0.0
empty
! 2 0.0
empty
* 4
[{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': "(custid = '8258886'::bigint)"}]
! 3 0.3938876986503601
{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': "(custid = '8258886'::bigint)"}
! 0 0.0
empty
! 1 0.0
empty
! 2 0.0
empty
* 5
[{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': "(custid = '8258886'::bigint)"}]
! 3 0.3938876986503601
{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.43, 'Total Cost': 8.45, 'Index Cond': "(custid = '8258886'::bigint)"}
! 0 0.0
empty
! 1 0.0
empty
! 2 0.0
empty
# 2
SELECT * FROM ACCOUNTS WHERE name = $1
* 1
{'Node Type': 'Index Scan', 'Parallel Aware': False, 'Scan Direction': 'Forward', 'Index Name': 'idx_accounts_name', 'Relation Name': 'accounts', 'Alias': 'accounts', 'Startup Cost': 0.56, 'Total Cost': 8.58, 'Plan Rows': 1, 'Plan Width': 73, 'Index Cond': "((name)::text = '0000000000000000000000000000000000000000000000000000000009338014'::text)"}
[{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.56, 'Total Cost': 8.58, 'Index Cond': "((name)::text = '0000000000000000000000000000000000000000000000000000000009338014'::text)"}]
! 3 0.3938876986503601
{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.56, 'Total Cost': 8.58, 'Index Cond': "((name)::text = '0000000000000000000000000000000000000000000000000000000009338014'::text)"}
! 0 0.0
empty
! 1 0.0
empty
! 2 0.0
empty
* 2
{'Node Type': 'Index Scan', 'Parallel Aware': False, 'Scan Direction': 'Forward', 'Index Name': 'idx_accounts_name', 'Relation Name': 'accounts', 'Alias': 'accounts', 'Startup Cost': 0.56, 'Total Cost': 8.58, 'Plan Rows': 1, 'Plan Width': 73, 'Index Cond': "((name)::text = '0000000000000000000000000000000000000000000000000000000006872584'::text)"}
[{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.56, 'Total Cost': 8.58, 'Index Cond': "((name)::text = '0000000000000000000000000000000000000000000000000000000006872584'::text)"}]
! 3 0.3938876986503601
{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.56, 'Total Cost': 8.58, 'Index Cond': "((name)::text = '0000000000000000000000000000000000000000000000000000000006872584'::text)"}
! 0 0.0
empty
! 1 0.0
empty
! 2 0.0
empty
* 3
[{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.56, 'Total Cost': 8.58, 'Index Cond': "((name)::text = '0000000000000000000000000000000000000000000000000000000006872584'::text)"}]
! 3 0.3938876986503601
{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.56, 'Total Cost': 8.58, 'Index Cond': "((name)::text = '0000000000000000000000000000000000000000000000000000000006872584'::text)"}
! 0 0.0
empty
! 1 0.0
empty
! 2 0.0
empty
* 4
[{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.56, 'Total Cost': 8.58, 'Index Cond': "((name)::text = '0000000000000000000000000000000000000000000000000000000006872584'::text)"}]
! 3 0.3938876986503601
{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.56, 'Total Cost': 8.58, 'Index Cond': "((name)::text = '0000000000000000000000000000000000000000000000000000000006872584'::text)"}
! 0 0.0
empty
! 1 0.0
empty
! 2 0.0
empty
* 5
[{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.56, 'Total Cost': 8.58, 'Index Cond': "((name)::text = '0000000000000000000000000000000000000000000000000000000006872584'::text)"}]
! 3 0.3938876986503601
{'Node Type': 'Index Scan', 'Relation Name': 'accounts', 'Startup Cost': 0.56, 'Total Cost': 8.58, 'Index Cond': "((name)::text = '0000000000000000000000000000000000000000000000000000000006872584'::text)"}
! 0 0.0
empty
! 1 0.0
empty
! 2 0.0
empty
all cnt: 55	find cnt: 26: acc: 0.4727272727272727
weights time: 0.008125335519964045	sql time: 0.5936005982485685
total_op_time: 10.789776763916015	bad_op_time: 5.402644823147701
eval time: 3002.498523950577
